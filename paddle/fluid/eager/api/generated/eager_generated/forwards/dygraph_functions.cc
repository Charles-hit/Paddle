
#include "paddle/phi/api/lib/dygraph_api.h"
#include "paddle/fluid/eager/api/generated/eager_generated/forwards/dygraph_functions.h"
#include "paddle/fluid/eager/api/generated/eager_generated/backwards/nodes.h"
#include "paddle/fluid/eager/eager_layout_auto_tune.h"
#include "paddle/phi/api/include/strings_api.h"
#include "paddle/phi/api/include/sparse_api.h"
#include "paddle/fluid/eager/api/utils/global_utils.h"
#include "paddle/fluid/platform/profiler/event_tracing.h"
#include "paddle/fluid/eager/amp_utils.h"
#include "paddle/fluid/eager/eager_amp_auto_cast.h"
#include "paddle/phi/backends/gpu/gpu_info.h"
#include "paddle/fluid/eager/nan_inf_utils.h"
#include "paddle/fluid/eager/api/manual/eager_manual/dygraph_forward_api.h"
DECLARE_bool(check_nan_inf);

std::unordered_map<std::string, std::vector<std::string>> core_ops_args_info = {
    { "acos", { "x" } },
{ "acosh", { "x" } },
{ "addmm", { "input","x","y","beta","alpha" } },
{ "angle", { "x" } },
{ "argsort", { "x","axis","descending" } },
{ "as_complex", { "x" } },
{ "as_real", { "x" } },
{ "asin", { "x" } },
{ "asinh", { "x" } },
{ "atan", { "x" } },
{ "atan2", { "x","y" } },
{ "atanh", { "x" } },
{ "bernoulli", { "x" } },
{ "bmm", { "x","y" } },
{ "ceil", { "x" } },
{ "ceil_", { "x" } },
{ "celu", { "x","alpha" } },
{ "cholesky", { "x","upper" } },
{ "cholesky_solve", { "x","y","upper" } },
{ "complex", { "real","imag" } },
{ "conj", { "x" } },
{ "cos", { "x" } },
{ "cosh", { "x" } },
{ "crop", { "x","shape","offsets" } },
{ "cross", { "x","y","axis" } },
{ "det", { "x" } },
{ "diag", { "x","offset","padding_value" } },
{ "diag_embed", { "input","offset","dim1","dim2" } },
{ "diagonal", { "x","offset","axis1","axis2" } },
{ "digamma", { "x" } },
{ "dist", { "x","y","p" } },
{ "dot", { "x","y" } },
{ "eig", { "x" } },
{ "eigh", { "x","UPLO" } },
{ "eigvals", { "x" } },
{ "elu", { "x","alpha" } },
{ "elu_", { "x","alpha" } },
{ "equal_all", { "x","y" } },
{ "erf", { "x" } },
{ "erfinv", { "x" } },
{ "erfinv_", { "x" } },
{ "exp", { "x" } },
{ "exp_", { "x" } },
{ "expm1", { "x" } },
{ "fft_c2c", { "x","axes","normalization","forward" } },
{ "fft_c2r", { "x","axes","normalization","forward","last_dim_size" } },
{ "fft_r2c", { "x","axes","normalization","forward","onesided" } },
{ "fill_diagonal_tensor", { "x","y","offset","dim1","dim2" } },
{ "fill_diagonal_tensor_", { "x","y","offset","dim1","dim2" } },
{ "flip", { "x","axis" } },
{ "floor", { "x" } },
{ "floor_", { "x" } },
{ "fold", { "x","output_sizes","kernel_sizes","strides","paddings","dilations" } },
{ "frame", { "x","frame_length","hop_length","axis" } },
{ "gather_nd", { "x","index" } },
{ "gather_tree", { "ids","parents" } },
{ "gelu", { "x","approximate" } },
{ "grid_sample", { "x","grid","mode","padding_mode","align_corners" } },
{ "gumbel_softmax", { "x","temperature","hard","axis" } },
{ "hardshrink", { "x","threshold" } },
{ "hardsigmoid", { "x","slope","offset" } },
{ "histogram", { "input","bins","min","max" } },
{ "index_sample", { "x","index" } },
{ "index_select", { "x","index","axis" } },
{ "inverse", { "x" } },
{ "is_empty", { "x" } },
{ "isclose", { "x","y","rtol","atol","equal_nan" } },
{ "isfinite", { "x" } },
{ "isinf", { "x" } },
{ "isnan", { "x" } },
{ "kthvalue", { "x","k","axis","keepdim" } },
{ "label_smooth", { "label","prior_dist","epsilon" } },
{ "leaky_relu", { "x","alpha" } },
{ "lerp", { "x","y","weight" } },
{ "lerp_", { "x","y","weight" } },
{ "lgamma", { "x" } },
{ "log", { "x" } },
{ "log10", { "x" } },
{ "log1p", { "x" } },
{ "log2", { "x" } },
{ "log_loss", { "input","label","epsilon" } },
{ "logit", { "x","eps" } },
{ "logsigmoid", { "x" } },
{ "lu_unpack", { "x","y","unpack_ludata","unpack_pivots" } },
{ "masked_select", { "x","mask" } },
{ "matrix_power", { "x","n" } },
{ "maxout", { "x","groups","axis" } },
{ "mode", { "x","axis","keepdim" } },
{ "multinomial", { "x","num_samples","replacement" } },
{ "mv", { "x","vec" } },
{ "nll_loss", { "input","label","weight","ignore_index","reduction" } },
{ "npu_identity", { "x","format" } },
{ "overlap_add", { "x","hop_length","axis" } },
{ "pixel_shuffle", { "x","upscale_factor","data_format" } },
{ "poisson", { "x" } },
{ "put_along_axis", { "arr","indices","value","axis","reduce" } },
{ "put_along_axis_", { "arr","indices","value","axis","reduce" } },
{ "qr", { "x","mode" } },
{ "reciprocal", { "x" } },
{ "reciprocal_", { "x" } },
{ "relu", { "x" } },
{ "relu_", { "x" } },
{ "renorm", { "x","p","axis","max_norm" } },
{ "roll", { "x","shifts","axis" } },
{ "round", { "x" } },
{ "round_", { "x" } },
{ "rsqrt", { "x" } },
{ "rsqrt_", { "x" } },
{ "scatter", { "x","index","updates","overwrite" } },
{ "scatter_", { "x","index","updates","overwrite" } },
{ "scatter_nd_add", { "x","index","updates" } },
{ "searchsorted", { "sorted_sequence","values","out_int32","right" } },
{ "selu", { "x","scale","alpha" } },
{ "send_uv", { "x","y","src_index","dst_index","message_op" } },
{ "shard_index", { "input","index_num","nshards","shard_id","ignore_value" } },
{ "sigmoid", { "x" } },
{ "silu", { "x" } },
{ "sin", { "x" } },
{ "sinh", { "x" } },
{ "softplus", { "x","beta","threshold" } },
{ "softshrink", { "x","threshold" } },
{ "softsign", { "x" } },
{ "solve", { "x","y" } },
{ "sqrt", { "x" } },
{ "sqrt_", { "x" } },
{ "square", { "x" } },
{ "squeeze", { "x","axis" } },
{ "squeeze_", { "x","axis" } },
{ "svd", { "x","full_matrices" } },
{ "take_along_axis", { "arr","indices","axis" } },
{ "tan", { "x" } },
{ "tanh", { "x" } },
{ "tanh_", { "x" } },
{ "tanh_shrink", { "x" } },
{ "thresholded_relu", { "x","threshold" } },
{ "topk", { "x","k","axis","largest","sorted" } },
{ "trace", { "x","offset","axis1","axis2" } },
{ "trunc", { "input" } },
{ "unfold", { "x","kernel_sizes","strides","paddings","dilations" } },
{ "unsqueeze", { "x","axes" } },
{ "unsqueeze_", { "x","axes" } },
{ "unstack", { "x","axis","num" } },
{ "viterbi_decode", { "potentials","transition_params","lengths","include_bos_eos_tag" } },
{ "warprnnt", { "input","label","input_lengths","label_lengths","blank","fastemit_lambda" } },
{ "where", { "condition","x","y" } },
{ "abs", { "x" } },
{ "accuracy", { "x","indices","label" } },
{ "adadelta_", { "param","grad","avg_squared_grad","avg_squared_update","rho","epsilon" } },
{ "adagrad_", { "param","grad","moment","learning_rate","epsilon" } },
{ "adam_", { "param","grad","learning_rate","moment1","moment2","beta1_pow","beta2_pow","master_param","skip_update","beta1","beta2","epsilon","lazy_mode","min_row_size_to_use_multithread","multi_precision","use_global_beta_pow" } },
{ "adamax_", { "param","grad","learning_rate","moment","inf_norm","beta1_pow","beta1","beta2","epsilon" } },
{ "adamw_", { "param","grad","learning_rate","moment1","moment2","beta1_pow","beta2_pow","master_param","skip_update","beta1","beta2","epsilon","lr_ratio","coeff","with_decay","lazy_mode","min_row_size_to_use_multithread","multi_precision","use_global_beta_pow" } },
{ "add", { "x","y" } },
{ "add_", { "x","y" } },
{ "affine_grid", { "input","outputShape","align_corners" } },
{ "all", { "x","axis","keepdim" } },
{ "allclose", { "x","y","rtol","atol","equal_nan" } },
{ "amax", { "x","axis","keepdim" } },
{ "amin", { "x","axis","keepdim" } },
{ "any", { "x","axis","keepdim" } },
{ "arange", { "start","end","step","dtype","place" } },
{ "argmax", { "x","axis","keepdims","flatten","dtype" } },
{ "argmin", { "x","axis","keepdims","flatten","dtype" } },
{ "assign", { "x" } },
{ "assign_out_", { "x","output" } },
{ "assign_value_", { "output","shape","dtype","values","place" } },
{ "auc", { "x","label","stat_pos","stat_neg","ins_tag_weight","curve","num_thresholds","slide_steps" } },
{ "average_accumulates_", { "param","in_sum_1","in_sum_2","in_sum_3","in_num_accumulates","in_old_num_accumulates","in_num_updates","average_window","max_average_window","min_average_window" } },
{ "batch_norm", { "x","mean","variance","scale","bias","is_test","momentum","epsilon","data_layout","use_global_stats","trainable_statistics" } },
{ "bce_loss", { "input","label" } },
{ "bicubic_interp", { "x","out_size","size_tensor","scale_tensor","data_layout","out_d","out_h","out_w","scale","interp_method","align_corners","align_mode" } },
{ "bilinear_interp", { "x","out_size","size_tensor","scale_tensor","data_layout","out_d","out_h","out_w","scale","interp_method","align_corners","align_mode" } },
{ "bilinear_tensor_product", { "x","y","weight","bias" } },
{ "bincount", { "x","weights","minlength" } },
{ "bitwise_and", { "x","y" } },
{ "bitwise_not", { "x" } },
{ "bitwise_or", { "x","y" } },
{ "bitwise_xor", { "x","y" } },
{ "box_coder", { "prior_box","prior_box_var","target_box","code_type","box_normalized","axis","variance" } },
{ "broadcast_tensors", { "input" } },
{ "cast", { "x","index_dtype","value_dtype" } },
{ "check_finite_and_unscale_", { "x","scale","input_found_infinite" } },
{ "class_center_sample", { "label","num_classes","num_samples","ring_id","rank","nranks","fix_seed","seed" } },
{ "clip", { "x","min","max" } },
{ "clip_", { "x","min","max" } },
{ "clip_by_norm", { "x","max_norm" } },
{ "coalesce_tensor", { "input","dtype","copy_data","set_constant","persist_output","constant","use_align","align_size","size_of_dtype","concated_shapes","concated_ranks" } },
{ "concat", { "x","axis" } },
{ "conv2d_transpose", { "x","filter","strides","paddings","output_padding","output_size","padding_algorithm","groups","dilations","data_format" } },
{ "conv3d", { "x","kernel","paddings","dilations","strides","groups","subm","key" } },
{ "conv3d_transpose", { "x","filter","strides","paddings","output_padding","output_size","padding_algorithm","groups","dilations","data_format" } },
{ "copy_to", { "x","place","blocking" } },
{ "cross_entropy_with_softmax", { "input","label","soft_label","use_softmax","numeric_stable_mode","ignore_index","axis" } },
{ "cumprod", { "x","dim" } },
{ "cumsum", { "x","axis","flatten","exclusive","reverse" } },
{ "decode_jpeg", { "x","mode","place" } },
{ "deformable_conv", { "x","offset","filter","mask","strides","paddings","dilations","deformable_groups","groups","im2col_step" } },
{ "depthwise_conv2d", { "input","filter","strides","paddings","padding_algorithm","groups","dilations","data_format" } },
{ "depthwise_conv2d_transpose", { "x","filter","strides","paddings","output_padding","output_size","padding_algorithm","groups","dilations","data_format" } },
{ "dirichlet", { "alpha" } },
{ "distribute_fpn_proposals", { "fpn_rois","rois_num","min_level","max_level","refer_level","refer_scale","pixel_offset" } },
{ "divide", { "x","y" } },
{ "dropout", { "x","seed_tensor","p","is_test","mode","seed","fix_seed" } },
{ "edit_distance", { "hyps","refs","hypslength","refslength","normalized" } },
{ "eigvalsh", { "x","uplo","is_test" } },
{ "einsum", { "x","equation" } },
{ "elementwise_pow", { "x","y" } },
{ "embedding", { "x","weight","padding_idx","sparse" } },
{ "empty", { "shape","place" } },
{ "empty_like", { "x","place" } },
{ "equal", { "x","y" } },
{ "expand", { "x","shape" } },
{ "expand_as", { "x","y","target_shape" } },
{ "exponential_", { "x","lam" } },
{ "eye", { "num_rows","num_columns","dtype","place" } },
{ "fill", { "x","value" } },
{ "fill_", { "x","value" } },
{ "fill_diagonal", { "x","value","offset","wrap" } },
{ "fill_diagonal_", { "x","value","offset","wrap" } },
{ "flatten", { "x","start_axis","stop_axis" } },
{ "flatten_", { "x","start_axis","stop_axis" } },
{ "floor_divide", { "x","y" } },
{ "fmax", { "x","y" } },
{ "fmin", { "x","y" } },
{ "frobenius_norm", { "x","axis","keep_dim","reduce_all" } },
{ "full", { "shape","value","dtype","place" } },
{ "full_", { "output","shape","value","dtype","place" } },
{ "full_batch_size_like", { "input","shape","dtype","value","input_dim_idx","output_dim_idx","place" } },
{ "full_like", { "x","value","dtype" } },
{ "gather", { "x","index","axis" } },
{ "gaussian", { "shape","mean","std","seed","dtype","place" } },
{ "generate_proposals", { "scores","bbox_deltas","im_shape","anchors","variances","pre_nms_top_n","post_nms_top_n","nms_thresh","min_size","eta","pixel_offset" } },
{ "greater_equal", { "x","y" } },
{ "greater_than", { "x","y" } },
{ "group_norm", { "x","scale","bias","epsilon","groups","data_layout" } },
{ "hardswish", { "x" } },
{ "hardtanh", { "x","t_min","t_max" } },
{ "hsigmoid_loss", { "x","label","w","bias","path","code","num_classes","remote_prefetch","is_sparse" } },
{ "huber_loss", { "input","label","delta" } },
{ "imag", { "x" } },
{ "increment", { "x","value" } },
{ "increment_", { "x","value" } },
{ "index_add", { "x","index","add_value","axis" } },
{ "index_add_", { "x","index","add_value","axis" } },
{ "instance_norm", { "x","scale","bias","epsilon" } },
{ "kldiv_loss", { "x","label","reduction" } },
{ "kron", { "x","y" } },
{ "lamb_", { "param","grad","learning_rate","moment1","moment2","beta1_pow","beta2_pow","master_param","skip_update","weight_decay","beta1","beta2","epsilon","multi_precision" } },
{ "layer_norm", { "x","scale","bias","epsilon","begin_norm_axis" } },
{ "less_equal", { "x","y" } },
{ "less_than", { "x","y" } },
{ "linear_interp", { "x","out_size","size_tensor","scale_tensor","data_layout","out_d","out_h","out_w","scale","interp_method","align_corners","align_mode" } },
{ "linspace", { "start","stop","number","dtype","place" } },
{ "log_softmax", { "x","axis" } },
{ "logcumsumexp", { "x","axis","flatten","exclusive","reverse" } },
{ "logical_and", { "x","y" } },
{ "logical_not", { "x" } },
{ "logical_or", { "x","y" } },
{ "logical_xor", { "x","y" } },
{ "logsumexp", { "x","axis","keepdim","reduce_all" } },
{ "lstsq", { "x","y","rcond","driver" } },
{ "lu", { "x","pivot" } },
{ "margin_cross_entropy", { "logits","label","return_softmax","ring_id","rank","nranks","margin1","margin2","margin3","scale" } },
{ "matmul", { "x","y" } },
{ "matrix_nms", { "bboxes","scores","score_threshold","nms_top_k","keep_top_k","post_threshold","use_gaussian","gaussian_sigma","background_label","normalized" } },
{ "matrix_rank", { "x","tol","hermitian","use_default_tol" } },
{ "matrix_rank_tol", { "x","atol_tensor","use_default_tol","hermitian" } },
{ "max", { "x","axis","keepdim" } },
{ "max_pool2d_with_index", { "x","kernel_size","strides","paddings","global_pooling","adaptive" } },
{ "max_pool3d_with_index", { "x","kernel_size","strides","paddings","global_pooling","adaptive" } },
{ "maximum", { "x","y" } },
{ "mean", { "x","axis","keepdim" } },
{ "mean_all", { "x" } },
{ "merge_selected_rows", { "x" } },
{ "merged_adam_", { "param","grad","learning_rate","moment1","moment2","beta1_pow","beta2_pow","master_param","beta1","beta2","epsilon","multi_precision","use_global_beta_pow" } },
{ "merged_momentum_", { "param","grad","velocity","learning_rate","master_param","mu","use_nesterov","regularization_method","regularization_coeff","multi_precision","rescale_grad" } },
{ "meshgrid", { "inputs" } },
{ "min", { "x","axis","keepdim" } },
{ "minimum", { "x","y" } },
{ "mish", { "x","threshold" } },
{ "momentum_", { "param","grad","velocity","learning_rate","master_param","mu","use_nesterov","regularization_method","regularization_coeff","multi_precision","rescale_grad" } },
{ "multi_dot", { "x" } },
{ "multiclass_nms3", { "bboxes","scores","rois_num","score_threshold","nms_top_k","keep_top_k","nms_threshold","normalized","nms_eta","background_label" } },
{ "multiplex", { "inputs","index" } },
{ "multiply", { "x","y" } },
{ "nearest_interp", { "x","out_size","size_tensor","scale_tensor","data_layout","out_d","out_h","out_w","scale","interp_method","align_corners","align_mode" } },
{ "nms", { "x","threshold" } },
{ "nonzero", { "condition" } },
{ "norm", { "x","axis","epsilon","is_test" } },
{ "not_equal", { "x","y" } },
{ "numel", { "x" } },
{ "one_hot", { "x","num_classes" } },
{ "ones", { "shape","dtype","place" } },
{ "ones_like", { "x","dtype","place" } },
{ "p_norm", { "x","porder","axis","epsilon","keepdim","asvector" } },
{ "pad", { "x","paddings","pad_value" } },
{ "pad3d", { "x","paddings","mode","pad_value","data_format" } },
{ "pool2d", { "x","kernel_size","strides","paddings","ceil_mode","exclusive","data_format","pooling_type","global_pooling","adaptive","padding_algorithm" } },
{ "pool3d", { "x","kernel_size","strides","paddings","ceil_mode","exclusive","data_format","pooling_type","global_pooling","adaptive","padding_algorithm" } },
{ "pow", { "x","factor" } },
{ "prelu", { "x","alpha","data_format","mode" } },
{ "prior_box", { "input","image","min_sizes","aspect_ratios","variances","max_sizes","flip","clip","step_w","step_h","offset","min_max_aspect_ratios_order" } },
{ "prod", { "x","dims","keep_dim","reduce_all" } },
{ "psroi_pool", { "x","boxes","boxes_num","pooled_height","pooled_width","output_channels","spatial_scale" } },
{ "randint", { "low","high","shape","dtype","place" } },
{ "randperm", { "n","dtype","place" } },
{ "real", { "x" } },
{ "relu6", { "x" } },
{ "remainder", { "x","y" } },
{ "remainder_", { "x","y" } },
{ "repeat_interleave", { "x","repeats","axis" } },
{ "repeat_interleave_with_tensor_index", { "x","repeats","axis" } },
{ "reshape", { "x","shape" } },
{ "reshape_", { "x","shape" } },
{ "reverse", { "x","axis" } },
{ "rmsprop_", { "param","mean_square","grad","moment","learning_rate","mean_grad","epsilon","decay","momentum","centered" } },
{ "rnn", { "x","pre_state","weight_list","sequence_length","dropout_state_in","dropout_prob","is_bidirec","input_size","hidden_size","num_layers","mode","seed","is_test" } },
{ "roi_align", { "x","boxes","boxes_num","pooled_height","pooled_width","spatial_scale","sampling_ratio","aligned" } },
{ "roi_pool", { "x","boxes","boxes_num","pooled_height","pooled_width","spatial_scale" } },
{ "scale", { "x","scale","bias","bias_after_scale" } },
{ "scale_", { "x","scale","bias","bias_after_scale" } },
{ "segment_pool", { "x","segment_ids","pooltype" } },
{ "send_u_recv", { "x","src_index","dst_index","reduce_op","out_size" } },
{ "send_ue_recv", { "x","y","src_index","dst_index","message_op","reduce_op","out_size" } },
{ "sgd_", { "param","learning_rate","grad","master_param","multi_precision" } },
{ "shape", { "input" } },
{ "sigmoid_cross_entropy_with_logits", { "x","label","normalize","ignore_index" } },
{ "sign", { "x" } },
{ "slice", { "input","axes","starts","ends","infer_flags","decrease_axis" } },
{ "slogdet", { "x" } },
{ "softmax", { "x","axis" } },
{ "softmax_", { "x","axis" } },
{ "spectral_norm", { "weight","u","v","dim","power_iters","eps" } },
{ "split", { "x","num_or_sections","axis" } },
{ "split_with_num", { "x","num","axis" } },
{ "squared_l2_norm", { "x" } },
{ "stack", { "x","axis" } },
{ "strided_slice", { "x","axes","starts","ends","strides" } },
{ "subtract", { "x","y" } },
{ "subtract_", { "x","y" } },
{ "sum", { "x","axis","dtype","keepdim" } },
{ "swish", { "x" } },
{ "sync_batch_norm_", { "x","mean","variance","scale","bias","is_test","momentum","epsilon","data_layout","use_global_stats","trainable_statistics" } },
{ "temporal_shift", { "x","seg_num","shift_ratio","data_format_str" } },
{ "tile", { "x","repeat_times" } },
{ "transpose", { "x","perm" } },
{ "triangular_solve", { "x","y","upper","tranpose","unitriangular" } },
{ "tril", { "x","diagonal" } },
{ "tril_indices", { "rows","cols","offset","dtype","place" } },
{ "trilinear_interp", { "x","out_size","size_tensor","scale_tensor","data_layout","out_d","out_h","out_w","scale","interp_method","align_corners","align_mode" } },
{ "triu", { "x","diagonal" } },
{ "triu_indices", { "row","col","offset","dtype","place" } },
{ "truncated_gaussian_random", { "shape","mean","std","seed","dtype","place" } },
{ "unbind", { "input","axis" } },
{ "uniform", { "shape","dtype","min","max","seed","place" } },
{ "uniform_inplace", { "x","min","max","seed","diag_num","diag_step","diag_val" } },
{ "uniform_inplace_", { "x","min","max","seed","diag_num","diag_step","diag_val" } },
{ "unique", { "x","return_index","return_inverse","return_counts","axis","dtype" } },
{ "unique_consecutive", { "x","return_inverse","return_counts","axis","dtype" } },
{ "unpool", { "x","indices","ksize","strides","padding","output_size","data_format" } },
{ "unpool3d", { "x","indices","ksize","strides","padding","output_size","data_format" } },
{ "update_loss_scaling_", { "x","found_infinite","prev_loss_scaling","in_good_steps","in_bad_steps","incr_every_n_steps","decr_every_n_nan_or_inf","incr_ratio","decr_ratio","stop_update" } },
{ "warpctc", { "logits","label","logits_length","labels_length","blank","norm_by_times" } },
{ "yolo_box", { "x","img_size","anchors","class_num","conf_thresh","downsample_ratio","clip_bbox","scale_x_y","iou_aware","iou_aware_factor" } },
{ "yolo_loss", { "x","gt_box","gt_label","gt_score","anchors","anchor_mask","class_num","ignore_thresh","downsample_ratio","use_label_smooth","scale_x_y" } },
{ "zeros", { "shape","dtype","place" } },
{ "zeros_like", { "x","dtype","place" } },
{ "batch_norm_", { "x","mean","variance","scale","bias","is_test","momentum","epsilon","data_layout","use_global_stats","trainable_statistics" } },
{ "divide_scalar", { "x","scalar" } },
{ "sparse_coo_tensor", { "values","indices","shape" } },
{ "to_dense", { "x" } },
{ "to_sparse_coo", { "x","sparse_dim" } },
{ "to_sparse_csr", { "x" } },
{ "values", { "x" } },
{ "coalesce", { "x" } },
{ "fused_attention", { "query","key","value","sparse_mask","key_padding_mask","attn_mask" } },
{ "masked_matmul", { "x","y","mask" } },
{ "maxpool", { "x","kernel_sizes","paddings","dilations","strides" } },
{ "lower", { "x","use_utf8_encoding" } },
{ "upper", { "x","use_utf8_encoding" } },
};
std::unordered_map<std::string, std::vector<std::string>> core_ops_args_type_info = {
    { "acos", { "tensor" } },
{ "acosh", { "tensor" } },
{ "addmm", { "tensor","tensor","tensor","","" } },
{ "angle", { "tensor" } },
{ "argsort", { "tensor","","" } },
{ "as_complex", { "tensor" } },
{ "as_real", { "tensor" } },
{ "asin", { "tensor" } },
{ "asinh", { "tensor" } },
{ "atan", { "tensor" } },
{ "atan2", { "tensor","tensor" } },
{ "atanh", { "tensor" } },
{ "bernoulli", { "tensor" } },
{ "bmm", { "tensor","tensor" } },
{ "ceil", { "tensor" } },
{ "ceil_", { "tensor" } },
{ "celu", { "tensor","" } },
{ "cholesky", { "tensor","" } },
{ "cholesky_solve", { "tensor","tensor","" } },
{ "complex", { "tensor","tensor" } },
{ "conj", { "tensor" } },
{ "cos", { "tensor" } },
{ "cosh", { "tensor" } },
{ "crop", { "tensor","","" } },
{ "cross", { "tensor","tensor","" } },
{ "det", { "tensor" } },
{ "diag", { "tensor","","" } },
{ "diag_embed", { "tensor","","","" } },
{ "diagonal", { "tensor","","","" } },
{ "digamma", { "tensor" } },
{ "dist", { "tensor","tensor","" } },
{ "dot", { "tensor","tensor" } },
{ "eig", { "tensor" } },
{ "eigh", { "tensor","" } },
{ "eigvals", { "tensor" } },
{ "elu", { "tensor","" } },
{ "elu_", { "tensor","" } },
{ "equal_all", { "tensor","tensor" } },
{ "erf", { "tensor" } },
{ "erfinv", { "tensor" } },
{ "erfinv_", { "tensor" } },
{ "exp", { "tensor" } },
{ "exp_", { "tensor" } },
{ "expm1", { "tensor" } },
{ "fft_c2c", { "tensor","","","" } },
{ "fft_c2r", { "tensor","","","","" } },
{ "fft_r2c", { "tensor","","","","" } },
{ "fill_diagonal_tensor", { "tensor","tensor","","","" } },
{ "fill_diagonal_tensor_", { "tensor","tensor","","","" } },
{ "flip", { "tensor","" } },
{ "floor", { "tensor" } },
{ "floor_", { "tensor" } },
{ "fold", { "tensor","","","","","" } },
{ "frame", { "tensor","","","" } },
{ "gather_nd", { "tensor","tensor" } },
{ "gather_tree", { "tensor","tensor" } },
{ "gelu", { "tensor","" } },
{ "grid_sample", { "tensor","tensor","","","" } },
{ "gumbel_softmax", { "tensor","","","" } },
{ "hardshrink", { "tensor","" } },
{ "hardsigmoid", { "tensor","","" } },
{ "histogram", { "tensor","","","" } },
{ "index_sample", { "tensor","tensor" } },
{ "index_select", { "tensor","tensor","" } },
{ "inverse", { "tensor" } },
{ "is_empty", { "tensor" } },
{ "isclose", { "tensor","tensor","","","" } },
{ "isfinite", { "tensor" } },
{ "isinf", { "tensor" } },
{ "isnan", { "tensor" } },
{ "kthvalue", { "tensor","","","" } },
{ "label_smooth", { "tensor","tensor","" } },
{ "leaky_relu", { "tensor","" } },
{ "lerp", { "tensor","tensor","tensor" } },
{ "lerp_", { "tensor","tensor","tensor" } },
{ "lgamma", { "tensor" } },
{ "log", { "tensor" } },
{ "log10", { "tensor" } },
{ "log1p", { "tensor" } },
{ "log2", { "tensor" } },
{ "log_loss", { "tensor","tensor","" } },
{ "logit", { "tensor","" } },
{ "logsigmoid", { "tensor" } },
{ "lu_unpack", { "tensor","tensor","","" } },
{ "masked_select", { "tensor","tensor" } },
{ "matrix_power", { "tensor","" } },
{ "maxout", { "tensor","","" } },
{ "mode", { "tensor","","" } },
{ "multinomial", { "tensor","","" } },
{ "mv", { "tensor","tensor" } },
{ "nll_loss", { "tensor","tensor","tensor","","" } },
{ "npu_identity", { "tensor","" } },
{ "overlap_add", { "tensor","","" } },
{ "pixel_shuffle", { "tensor","","" } },
{ "poisson", { "tensor" } },
{ "put_along_axis", { "tensor","tensor","tensor","","" } },
{ "put_along_axis_", { "tensor","tensor","tensor","","" } },
{ "qr", { "tensor","" } },
{ "reciprocal", { "tensor" } },
{ "reciprocal_", { "tensor" } },
{ "relu", { "tensor" } },
{ "relu_", { "tensor" } },
{ "renorm", { "tensor","","","" } },
{ "roll", { "tensor","","" } },
{ "round", { "tensor" } },
{ "round_", { "tensor" } },
{ "rsqrt", { "tensor" } },
{ "rsqrt_", { "tensor" } },
{ "scatter", { "tensor","tensor","tensor","" } },
{ "scatter_", { "tensor","tensor","tensor","" } },
{ "scatter_nd_add", { "tensor","tensor","tensor" } },
{ "searchsorted", { "tensor","tensor","","" } },
{ "selu", { "tensor","","" } },
{ "send_uv", { "tensor","tensor","tensor","tensor","" } },
{ "shard_index", { "tensor","","","","" } },
{ "sigmoid", { "tensor" } },
{ "silu", { "tensor" } },
{ "sin", { "tensor" } },
{ "sinh", { "tensor" } },
{ "softplus", { "tensor","","" } },
{ "softshrink", { "tensor","" } },
{ "softsign", { "tensor" } },
{ "solve", { "tensor","tensor" } },
{ "sqrt", { "tensor" } },
{ "sqrt_", { "tensor" } },
{ "square", { "tensor" } },
{ "squeeze", { "tensor","" } },
{ "squeeze_", { "tensor","" } },
{ "svd", { "tensor","" } },
{ "take_along_axis", { "tensor","tensor","" } },
{ "tan", { "tensor" } },
{ "tanh", { "tensor" } },
{ "tanh_", { "tensor" } },
{ "tanh_shrink", { "tensor" } },
{ "thresholded_relu", { "tensor","" } },
{ "topk", { "tensor","","","","" } },
{ "trace", { "tensor","","","" } },
{ "trunc", { "tensor" } },
{ "unfold", { "tensor","","","","" } },
{ "unsqueeze", { "tensor","" } },
{ "unsqueeze_", { "tensor","" } },
{ "unstack", { "tensor","","" } },
{ "viterbi_decode", { "tensor","tensor","tensor","" } },
{ "warprnnt", { "tensor","tensor","tensor","tensor","","" } },
{ "where", { "tensor","tensor","tensor" } },
{ "abs", { "tensor" } },
{ "accuracy", { "tensor","tensor","tensor" } },
{ "adadelta_", { "tensor","tensor","tensor","tensor","","" } },
{ "adagrad_", { "tensor","tensor","tensor","tensor","" } },
{ "adam_", { "tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","","","","","","","" } },
{ "adamax_", { "tensor","tensor","tensor","tensor","tensor","tensor","","","" } },
{ "adamw_", { "tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","","","","","","","","","","" } },
{ "add", { "tensor","tensor" } },
{ "add_", { "tensor","tensor" } },
{ "affine_grid", { "tensor","","" } },
{ "all", { "tensor","","" } },
{ "allclose", { "tensor","tensor","","","" } },
{ "amax", { "tensor","","" } },
{ "amin", { "tensor","","" } },
{ "any", { "tensor","","" } },
{ "arange", { "tensor","tensor","tensor","","" } },
{ "argmax", { "tensor","","","","" } },
{ "argmin", { "tensor","","","","" } },
{ "assign", { "tensor" } },
{ "assign_out_", { "tensor","tensor" } },
{ "assign_value_", { "tensor","","","","" } },
{ "auc", { "tensor","tensor","tensor","tensor","tensor","","","" } },
{ "average_accumulates_", { "tensor","tensor","tensor","tensor","tensor","tensor","tensor","","","" } },
{ "batch_norm", { "tensor","tensor","tensor","tensor","tensor","","","","","","" } },
{ "bce_loss", { "tensor","tensor" } },
{ "bicubic_interp", { "tensor","tensor","list","tensor","","","","","","","","" } },
{ "bilinear_interp", { "tensor","tensor","list","tensor","","","","","","","","" } },
{ "bilinear_tensor_product", { "tensor","tensor","tensor","tensor" } },
{ "bincount", { "tensor","tensor","" } },
{ "bitwise_and", { "tensor","tensor" } },
{ "bitwise_not", { "tensor" } },
{ "bitwise_or", { "tensor","tensor" } },
{ "bitwise_xor", { "tensor","tensor" } },
{ "box_coder", { "tensor","tensor","tensor","","","","" } },
{ "broadcast_tensors", { "list" } },
{ "cast", { "tensor","","" } },
{ "check_finite_and_unscale_", { "list","tensor","tensor" } },
{ "class_center_sample", { "tensor","","","","","","","" } },
{ "clip", { "tensor","","" } },
{ "clip_", { "tensor","","" } },
{ "clip_by_norm", { "tensor","" } },
{ "coalesce_tensor", { "list","","","","","","","","","","" } },
{ "concat", { "list","" } },
{ "conv2d_transpose", { "tensor","tensor","","","","","","","","" } },
{ "conv3d", { "tensor","tensor","","","","","","" } },
{ "conv3d_transpose", { "tensor","tensor","","","","","","","","" } },
{ "copy_to", { "tensor","","" } },
{ "cross_entropy_with_softmax", { "tensor","tensor","","","","","" } },
{ "cumprod", { "tensor","" } },
{ "cumsum", { "tensor","","","","" } },
{ "decode_jpeg", { "tensor","","" } },
{ "deformable_conv", { "tensor","tensor","tensor","tensor","","","","","","" } },
{ "depthwise_conv2d", { "tensor","tensor","","","","","","" } },
{ "depthwise_conv2d_transpose", { "tensor","tensor","","","","","","","","" } },
{ "dirichlet", { "tensor" } },
{ "distribute_fpn_proposals", { "tensor","tensor","","","","","" } },
{ "divide", { "tensor","tensor" } },
{ "dropout", { "tensor","tensor","","","","","" } },
{ "edit_distance", { "tensor","tensor","tensor","tensor","" } },
{ "eigvalsh", { "tensor","","" } },
{ "einsum", { "list","" } },
{ "elementwise_pow", { "tensor","tensor" } },
{ "embedding", { "tensor","tensor","","" } },
{ "empty", { "","" } },
{ "empty_like", { "tensor","" } },
{ "equal", { "tensor","tensor" } },
{ "expand", { "tensor","" } },
{ "expand_as", { "tensor","tensor","" } },
{ "exponential_", { "tensor","" } },
{ "eye", { "","","","" } },
{ "fill", { "tensor","" } },
{ "fill_", { "tensor","" } },
{ "fill_diagonal", { "tensor","","","" } },
{ "fill_diagonal_", { "tensor","","","" } },
{ "flatten", { "tensor","","" } },
{ "flatten_", { "tensor","","" } },
{ "floor_divide", { "tensor","tensor" } },
{ "fmax", { "tensor","tensor" } },
{ "fmin", { "tensor","tensor" } },
{ "frobenius_norm", { "tensor","","","" } },
{ "full", { "","","","" } },
{ "full_", { "tensor","","","","" } },
{ "full_batch_size_like", { "tensor","","","","","","" } },
{ "full_like", { "tensor","","" } },
{ "gather", { "tensor","tensor","" } },
{ "gaussian", { "","","","","","" } },
{ "generate_proposals", { "tensor","tensor","tensor","tensor","tensor","","","","","","" } },
{ "greater_equal", { "tensor","tensor" } },
{ "greater_than", { "tensor","tensor" } },
{ "group_norm", { "tensor","tensor","tensor","","","" } },
{ "hardswish", { "tensor" } },
{ "hardtanh", { "tensor","","" } },
{ "hsigmoid_loss", { "tensor","tensor","tensor","tensor","tensor","tensor","","","" } },
{ "huber_loss", { "tensor","tensor","" } },
{ "imag", { "tensor" } },
{ "increment", { "tensor","" } },
{ "increment_", { "tensor","" } },
{ "index_add", { "tensor","tensor","tensor","" } },
{ "index_add_", { "tensor","tensor","tensor","" } },
{ "instance_norm", { "tensor","tensor","tensor","" } },
{ "kldiv_loss", { "tensor","tensor","" } },
{ "kron", { "tensor","tensor" } },
{ "lamb_", { "tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","tensor","","","","","" } },
{ "layer_norm", { "tensor","tensor","tensor","","" } },
{ "less_equal", { "tensor","tensor" } },
{ "less_than", { "tensor","tensor" } },
{ "linear_interp", { "tensor","tensor","list","tensor","","","","","","","","" } },
{ "linspace", { "tensor","tensor","tensor","","" } },
{ "log_softmax", { "tensor","" } },
{ "logcumsumexp", { "tensor","","","","" } },
{ "logical_and", { "tensor","tensor" } },
{ "logical_not", { "tensor" } },
{ "logical_or", { "tensor","tensor" } },
{ "logical_xor", { "tensor","tensor" } },
{ "logsumexp", { "tensor","","","" } },
{ "lstsq", { "tensor","tensor","","" } },
{ "lu", { "tensor","" } },
{ "margin_cross_entropy", { "tensor","tensor","","","","","","","","" } },
{ "matmul", { "tensor","tensor" } },
{ "matrix_nms", { "tensor","tensor","","","","","","","","" } },
{ "matrix_rank", { "tensor","","","" } },
{ "matrix_rank_tol", { "tensor","tensor","","" } },
{ "max", { "tensor","","" } },
{ "max_pool2d_with_index", { "tensor","","","","","" } },
{ "max_pool3d_with_index", { "tensor","","","","","" } },
{ "maximum", { "tensor","tensor" } },
{ "mean", { "tensor","","" } },
{ "mean_all", { "tensor" } },
{ "merge_selected_rows", { "tensor" } },
{ "merged_adam_", { "list","list","list","list","list","list","list","list","","","","","" } },
{ "merged_momentum_", { "list","list","list","list","list","","","","","","" } },
{ "meshgrid", { "list" } },
{ "min", { "tensor","","" } },
{ "minimum", { "tensor","tensor" } },
{ "mish", { "tensor","" } },
{ "momentum_", { "tensor","tensor","tensor","tensor","tensor","","","","","","" } },
{ "multi_dot", { "list" } },
{ "multiclass_nms3", { "tensor","tensor","tensor","","","","","","","" } },
{ "multiplex", { "list","tensor" } },
{ "multiply", { "tensor","tensor" } },
{ "nearest_interp", { "tensor","tensor","list","tensor","","","","","","","","" } },
{ "nms", { "tensor","" } },
{ "nonzero", { "tensor" } },
{ "norm", { "tensor","","","" } },
{ "not_equal", { "tensor","tensor" } },
{ "numel", { "tensor" } },
{ "one_hot", { "tensor","" } },
{ "ones", { "","","" } },
{ "ones_like", { "tensor","","" } },
{ "p_norm", { "tensor","","","","","" } },
{ "pad", { "tensor","","" } },
{ "pad3d", { "tensor","","","","" } },
{ "pool2d", { "tensor","","","","","","","","","","" } },
{ "pool3d", { "tensor","","","","","","","","","","" } },
{ "pow", { "tensor","" } },
{ "prelu", { "tensor","tensor","","" } },
{ "prior_box", { "tensor","tensor","","","","","","","","","","" } },
{ "prod", { "tensor","","","" } },
{ "psroi_pool", { "tensor","tensor","tensor","","","","" } },
{ "randint", { "","","","","" } },
{ "randperm", { "","","" } },
{ "real", { "tensor" } },
{ "relu6", { "tensor" } },
{ "remainder", { "tensor","tensor" } },
{ "remainder_", { "tensor","tensor" } },
{ "repeat_interleave", { "tensor","","" } },
{ "repeat_interleave_with_tensor_index", { "tensor","tensor","" } },
{ "reshape", { "tensor","" } },
{ "reshape_", { "tensor","" } },
{ "reverse", { "tensor","" } },
{ "rmsprop_", { "tensor","tensor","tensor","tensor","tensor","tensor","","","","" } },
{ "rnn", { "tensor","list","list","tensor","tensor","","","","","","","","" } },
{ "roi_align", { "tensor","tensor","tensor","","","","","" } },
{ "roi_pool", { "tensor","tensor","tensor","","","" } },
{ "scale", { "tensor","","","" } },
{ "scale_", { "tensor","","","" } },
{ "segment_pool", { "tensor","tensor","" } },
{ "send_u_recv", { "tensor","tensor","tensor","","" } },
{ "send_ue_recv", { "tensor","tensor","tensor","tensor","","","" } },
{ "sgd_", { "tensor","tensor","tensor","tensor","" } },
{ "shape", { "tensor" } },
{ "sigmoid_cross_entropy_with_logits", { "tensor","tensor","","" } },
{ "sign", { "tensor" } },
{ "slice", { "tensor","","","","","" } },
{ "slogdet", { "tensor" } },
{ "softmax", { "tensor","" } },
{ "softmax_", { "tensor","" } },
{ "spectral_norm", { "tensor","tensor","tensor","","","" } },
{ "split", { "tensor","","" } },
{ "split_with_num", { "tensor","","" } },
{ "squared_l2_norm", { "tensor" } },
{ "stack", { "list","" } },
{ "strided_slice", { "tensor","","","","" } },
{ "subtract", { "tensor","tensor" } },
{ "subtract_", { "tensor","tensor" } },
{ "sum", { "tensor","","","" } },
{ "swish", { "tensor" } },
{ "sync_batch_norm_", { "tensor","tensor","tensor","tensor","tensor","","","","","","" } },
{ "temporal_shift", { "tensor","","","" } },
{ "tile", { "tensor","" } },
{ "transpose", { "tensor","" } },
{ "triangular_solve", { "tensor","tensor","","","" } },
{ "tril", { "tensor","" } },
{ "tril_indices", { "","","","","" } },
{ "trilinear_interp", { "tensor","tensor","list","tensor","","","","","","","","" } },
{ "triu", { "tensor","" } },
{ "triu_indices", { "","","","","" } },
{ "truncated_gaussian_random", { "","","","","","" } },
{ "unbind", { "tensor","" } },
{ "uniform", { "","","","","","" } },
{ "uniform_inplace", { "tensor","","","","","","" } },
{ "uniform_inplace_", { "tensor","","","","","","" } },
{ "unique", { "tensor","","","","","" } },
{ "unique_consecutive", { "tensor","","","","" } },
{ "unpool", { "tensor","tensor","","","","","" } },
{ "unpool3d", { "tensor","tensor","","","","","" } },
{ "update_loss_scaling_", { "list","tensor","tensor","tensor","tensor","","","","","" } },
{ "warpctc", { "tensor","tensor","tensor","tensor","","" } },
{ "yolo_box", { "tensor","tensor","","","","","","","","" } },
{ "yolo_loss", { "tensor","tensor","tensor","tensor","","","","","","","" } },
{ "zeros", { "","","" } },
{ "zeros_like", { "tensor","","" } },
{ "batch_norm_", { "tensor","tensor","tensor","tensor","tensor","","","","","","" } },
{ "divide_scalar", { "tensor","" } },
{ "sparse_coo_tensor", { "tensor","tensor","" } },
{ "to_dense", { "tensor" } },
{ "to_sparse_coo", { "tensor","" } },
{ "to_sparse_csr", { "tensor" } },
{ "values", { "tensor" } },
{ "coalesce", { "tensor" } },
{ "fused_attention", { "tensor","tensor","tensor","tensor","tensor","tensor" } },
{ "masked_matmul", { "tensor","tensor","tensor" } },
{ "maxpool", { "tensor","","","","" } },
{ "lower", { "tensor","" } },
{ "upper", { "tensor","" } },
};
std::unordered_map<std::string, std::vector<std::string>> core_ops_returns_info = {
    { "acos", { "out" } },
{ "acosh", { "out" } },
{ "addmm", { "out" } },
{ "angle", { "out" } },
{ "argsort", { "out","indices" } },
{ "as_complex", { "out" } },
{ "as_real", { "out" } },
{ "asin", { "out" } },
{ "asinh", { "out" } },
{ "atan", { "out" } },
{ "atan2", { "out" } },
{ "atanh", { "out" } },
{ "bernoulli", { "out" } },
{ "bmm", { "out" } },
{ "ceil", { "out" } },
{ "ceil_", { "out" } },
{ "celu", { "out" } },
{ "cholesky", { "out" } },
{ "cholesky_solve", { "out" } },
{ "complex", { "out" } },
{ "conj", { "out" } },
{ "cos", { "out" } },
{ "cosh", { "out" } },
{ "crop", { "out" } },
{ "cross", { "out" } },
{ "det", { "out" } },
{ "diag", { "out" } },
{ "diag_embed", { "out" } },
{ "diagonal", { "out" } },
{ "digamma", { "out" } },
{ "dist", { "out" } },
{ "dot", { "out" } },
{ "eig", { "out_w","out_v" } },
{ "eigh", { "out_w","out_v" } },
{ "eigvals", { "out" } },
{ "elu", { "out" } },
{ "elu_", { "out" } },
{ "equal_all", { "out" } },
{ "erf", { "out" } },
{ "erfinv", { "out" } },
{ "erfinv_", { "out" } },
{ "exp", { "out" } },
{ "exp_", { "out" } },
{ "expm1", { "out" } },
{ "fft_c2c", { "out" } },
{ "fft_c2r", { "out" } },
{ "fft_r2c", { "out" } },
{ "fill_diagonal_tensor", { "out" } },
{ "fill_diagonal_tensor_", { "out" } },
{ "flip", { "out" } },
{ "floor", { "out" } },
{ "floor_", { "out" } },
{ "fold", { "out" } },
{ "frame", { "out" } },
{ "gather_nd", { "out" } },
{ "gather_tree", { "out" } },
{ "gelu", { "out" } },
{ "grid_sample", { "out" } },
{ "gumbel_softmax", { "out" } },
{ "hardshrink", { "out" } },
{ "hardsigmoid", { "out" } },
{ "histogram", { "out" } },
{ "index_sample", { "out" } },
{ "index_select", { "out" } },
{ "inverse", { "out" } },
{ "is_empty", { "out" } },
{ "isclose", { "out" } },
{ "isfinite", { "out" } },
{ "isinf", { "out" } },
{ "isnan", { "out" } },
{ "kthvalue", { "out","indices" } },
{ "label_smooth", { "out" } },
{ "leaky_relu", { "out" } },
{ "lerp", { "out" } },
{ "lerp_", { "out" } },
{ "lgamma", { "out" } },
{ "log", { "out" } },
{ "log10", { "out" } },
{ "log1p", { "out" } },
{ "log2", { "out" } },
{ "log_loss", { "out" } },
{ "logit", { "out" } },
{ "logsigmoid", { "out" } },
{ "lu_unpack", { "pmat","l","u" } },
{ "masked_select", { "out" } },
{ "matrix_power", { "out" } },
{ "maxout", { "out" } },
{ "mode", { "out","indices" } },
{ "multinomial", { "out" } },
{ "mv", { "out" } },
{ "nll_loss", { "out","total_weight" } },
{ "npu_identity", { "out" } },
{ "overlap_add", { "out" } },
{ "pixel_shuffle", { "out" } },
{ "poisson", { "out" } },
{ "put_along_axis", { "out" } },
{ "put_along_axis_", { "out" } },
{ "qr", { "q","r" } },
{ "reciprocal", { "out" } },
{ "reciprocal_", { "out" } },
{ "relu", { "out" } },
{ "relu_", { "out" } },
{ "renorm", { "out" } },
{ "roll", { "out" } },
{ "round", { "out" } },
{ "round_", { "out" } },
{ "rsqrt", { "out" } },
{ "rsqrt_", { "out" } },
{ "scatter", { "out" } },
{ "scatter_", { "out" } },
{ "scatter_nd_add", { "out" } },
{ "searchsorted", { "out" } },
{ "selu", { "out" } },
{ "send_uv", { "out" } },
{ "shard_index", { "out" } },
{ "sigmoid", { "out" } },
{ "silu", { "out" } },
{ "sin", { "out" } },
{ "sinh", { "out" } },
{ "softplus", { "out" } },
{ "softshrink", { "out" } },
{ "softsign", { "out" } },
{ "solve", { "out" } },
{ "sqrt", { "out" } },
{ "sqrt_", { "out" } },
{ "square", { "out" } },
{ "squeeze", { "out","xshape" } },
{ "squeeze_", { "out","xshape" } },
{ "svd", { "u","s","vh" } },
{ "take_along_axis", { "out" } },
{ "tan", { "out" } },
{ "tanh", { "out" } },
{ "tanh_", { "out" } },
{ "tanh_shrink", { "out" } },
{ "thresholded_relu", { "out" } },
{ "topk", { "out","indices" } },
{ "trace", { "out" } },
{ "trunc", { "out" } },
{ "unfold", { "out" } },
{ "unsqueeze", { "out","xshape" } },
{ "unsqueeze_", { "out","xshape" } },
{ "unstack", { "out" } },
{ "viterbi_decode", { "scores","path" } },
{ "warprnnt", { "loss","warprnntgrad" } },
{ "where", { "out" } },
{ "abs", { "out" } },
{ "accuracy", { "accuracy","correct","total" } },
{ "adadelta_", { "param_out","moment_out","inf_norm_out" } },
{ "adagrad_", { "param_out","moment_out" } },
{ "adam_", { "param_out","moment1_out","moment2_out","beta1_pow_out","beta2_pow_out","master_param_outs" } },
{ "adamax_", { "param_out","avg_squared_grad_out","avg_squared_update_out" } },
{ "adamw_", { "param_out","moment1_out","moment2_out","beta1_pow_out","beta2_pow_out","master_param_outs" } },
{ "add", { "out" } },
{ "add_", { "out" } },
{ "affine_grid", { "output" } },
{ "all", { "out" } },
{ "allclose", { "out" } },
{ "amax", { "out" } },
{ "amin", { "out" } },
{ "any", { "out" } },
{ "arange", { "out" } },
{ "argmax", { "out" } },
{ "argmin", { "out" } },
{ "assign", { "out" } },
{ "assign_out_", { "out" } },
{ "assign_value_", { "out" } },
{ "auc", { "auc","stat_pos_out","stat_neg_out" } },
{ "average_accumulates_", { "out_sum_1","out_sum_2","out_sum_3","out_num_accumulates","out_old_num_accumulates","out_num_updates" } },
{ "batch_norm", { "out","mean_out","variance_out","saved_mean","saved_variance","reserve_space" } },
{ "bce_loss", { "out" } },
{ "bicubic_interp", { "output" } },
{ "bilinear_interp", { "output" } },
{ "bilinear_tensor_product", { "out" } },
{ "bincount", { "out" } },
{ "bitwise_and", { "out" } },
{ "bitwise_not", { "out" } },
{ "bitwise_or", { "out" } },
{ "bitwise_xor", { "out" } },
{ "box_coder", { "output_box" } },
{ "broadcast_tensors", { "out" } },
{ "cast", { "out" } },
{ "check_finite_and_unscale_", { "out","output_found_infinite" } },
{ "class_center_sample", { "remapped_label","sampled_local_class_center" } },
{ "clip", { "out" } },
{ "clip_", { "out" } },
{ "clip_by_norm", { "out" } },
{ "coalesce_tensor", { "output","fused_output" } },
{ "concat", { "out" } },
{ "conv2d_transpose", { "out" } },
{ "conv3d", { "out","rulebook","counter" } },
{ "conv3d_transpose", { "out" } },
{ "copy_to", { "out" } },
{ "cross_entropy_with_softmax", { "softmax","loss" } },
{ "cumprod", { "out" } },
{ "cumsum", { "out" } },
{ "decode_jpeg", { "out" } },
{ "deformable_conv", { "out" } },
{ "depthwise_conv2d", { "out" } },
{ "depthwise_conv2d_transpose", { "out" } },
{ "dirichlet", { "out" } },
{ "distribute_fpn_proposals", { "multi_fpn_rois","multi_level_rois_num","restore_index" } },
{ "divide", { "out" } },
{ "dropout", { "out","mask" } },
{ "edit_distance", { "sequencenum","out" } },
{ "eigvalsh", { "eigenvalues","eigenvectors" } },
{ "einsum", { "out","inner_cache","x_shape" } },
{ "elementwise_pow", { "out" } },
{ "embedding", { "out" } },
{ "empty", { "out" } },
{ "empty_like", { "out" } },
{ "equal", { "out" } },
{ "expand", { "out" } },
{ "expand_as", { "out" } },
{ "exponential_", { "out" } },
{ "eye", { "out" } },
{ "fill", { "out" } },
{ "fill_", { "out" } },
{ "fill_diagonal", { "out" } },
{ "fill_diagonal_", { "out" } },
{ "flatten", { "out","xshape" } },
{ "flatten_", { "out","xshape" } },
{ "floor_divide", { "out" } },
{ "fmax", { "out" } },
{ "fmin", { "out" } },
{ "frobenius_norm", { "out" } },
{ "full", { "out" } },
{ "full_", { "out" } },
{ "full_batch_size_like", { "out" } },
{ "full_like", { "out" } },
{ "gather", { "out" } },
{ "gaussian", { "out" } },
{ "generate_proposals", { "rpn_rois","rpn_roi_probs","rpn_rois_num" } },
{ "greater_equal", { "out" } },
{ "greater_than", { "out" } },
{ "group_norm", { "y","mean","variance" } },
{ "hardswish", { "out" } },
{ "hardtanh", { "out" } },
{ "hsigmoid_loss", { "out","pre_out","w_out" } },
{ "huber_loss", { "out","residual" } },
{ "imag", { "out" } },
{ "increment", { "out" } },
{ "increment_", { "out" } },
{ "index_add", { "out" } },
{ "index_add_", { "out" } },
{ "instance_norm", { "y","saved_mean","saved_variance" } },
{ "kldiv_loss", { "out" } },
{ "kron", { "out" } },
{ "lamb_", { "param_out","moment1_out","moment2_out","beta1_pow_out","beta2_pow_out","master_param_outs" } },
{ "layer_norm", { "out","mean","variance" } },
{ "less_equal", { "out" } },
{ "less_than", { "out" } },
{ "linear_interp", { "output" } },
{ "linspace", { "out" } },
{ "log_softmax", { "out" } },
{ "logcumsumexp", { "out" } },
{ "logical_and", { "out" } },
{ "logical_not", { "out" } },
{ "logical_or", { "out" } },
{ "logical_xor", { "out" } },
{ "logsumexp", { "out" } },
{ "lstsq", { "solution","residuals","rank","singular_values" } },
{ "lu", { "out","pivots","infos" } },
{ "margin_cross_entropy", { "softmax","loss" } },
{ "matmul", { "out" } },
{ "matrix_nms", { "out","index","roisnum" } },
{ "matrix_rank", { "out" } },
{ "matrix_rank_tol", { "out" } },
{ "max", { "out" } },
{ "max_pool2d_with_index", { "out","mask" } },
{ "max_pool3d_with_index", { "out","mask" } },
{ "maximum", { "out" } },
{ "mean", { "out" } },
{ "mean_all", { "out" } },
{ "merge_selected_rows", { "out" } },
{ "merged_adam_", { "param_out","moment1_out","moment2_out","beta1_pow_out","beta2_pow_out","master_param_out" } },
{ "merged_momentum_", { "param_out","velocity_out","master_param_out" } },
{ "meshgrid", { "outputs" } },
{ "min", { "out" } },
{ "minimum", { "out" } },
{ "mish", { "out" } },
{ "momentum_", { "param_out","velocity_out","master_param_out" } },
{ "multi_dot", { "out" } },
{ "multiclass_nms3", { "out","index","nms_rois_num" } },
{ "multiplex", { "out" } },
{ "multiply", { "out" } },
{ "nearest_interp", { "output" } },
{ "nms", { "out" } },
{ "nonzero", { "out" } },
{ "norm", { "out","norm" } },
{ "not_equal", { "out" } },
{ "numel", { "size" } },
{ "one_hot", { "out" } },
{ "ones", { "out" } },
{ "ones_like", { "out" } },
{ "p_norm", { "out" } },
{ "pad", { "out" } },
{ "pad3d", { "out" } },
{ "pool2d", { "out" } },
{ "pool3d", { "out" } },
{ "pow", { "out" } },
{ "prelu", { "out" } },
{ "prior_box", { "out","var" } },
{ "prod", { "out" } },
{ "psroi_pool", { "out" } },
{ "randint", { "out" } },
{ "randperm", { "out" } },
{ "real", { "out" } },
{ "relu6", { "out" } },
{ "remainder", { "out" } },
{ "remainder_", { "out" } },
{ "repeat_interleave", { "out" } },
{ "repeat_interleave_with_tensor_index", { "out" } },
{ "reshape", { "out" } },
{ "reshape_", { "out","xshape" } },
{ "reverse", { "out" } },
{ "rmsprop_", { "param_out","moment_out","mean_square_out","mean_grad_out" } },
{ "rnn", { "out","dropout_state_out","state","reserve" } },
{ "roi_align", { "out" } },
{ "roi_pool", { "out","arg_max" } },
{ "scale", { "out" } },
{ "scale_", { "out" } },
{ "segment_pool", { "out","summed_ids" } },
{ "send_u_recv", { "out","dst_count" } },
{ "send_ue_recv", { "out","dst_count" } },
{ "sgd_", { "param_out","master_param_out" } },
{ "shape", { "out" } },
{ "sigmoid_cross_entropy_with_logits", { "out" } },
{ "sign", { "out" } },
{ "slice", { "out" } },
{ "slogdet", { "out" } },
{ "softmax", { "out" } },
{ "softmax_", { "out" } },
{ "spectral_norm", { "out" } },
{ "split", { "out" } },
{ "split_with_num", { "out" } },
{ "squared_l2_norm", { "out" } },
{ "stack", { "out" } },
{ "strided_slice", { "out" } },
{ "subtract", { "out" } },
{ "subtract_", { "out" } },
{ "sum", { "out" } },
{ "swish", { "out" } },
{ "sync_batch_norm_", { "out","mean_out","variance_out","saved_mean","saved_variance","reserve_space" } },
{ "temporal_shift", { "out" } },
{ "tile", { "out" } },
{ "transpose", { "out" } },
{ "triangular_solve", { "out" } },
{ "tril", { "out" } },
{ "tril_indices", { "out" } },
{ "trilinear_interp", { "output" } },
{ "triu", { "out" } },
{ "triu_indices", { "out" } },
{ "truncated_gaussian_random", { "out" } },
{ "unbind", { "out" } },
{ "uniform", { "out" } },
{ "uniform_inplace", { "out" } },
{ "uniform_inplace_", { "out" } },
{ "unique", { "out","indices","inverse","counts" } },
{ "unique_consecutive", { "out","index","counts" } },
{ "unpool", { "out" } },
{ "unpool3d", { "out" } },
{ "update_loss_scaling_", { "out","loss_scaling","out_good_steps","out_bad_steps" } },
{ "warpctc", { "loss","warpctcgrad" } },
{ "yolo_box", { "boxes","scores" } },
{ "yolo_loss", { "loss","objectness_mask","gt_match_mask" } },
{ "zeros", { "out" } },
{ "zeros_like", { "out" } },
{ "batch_norm_", { "out","mean_out","variance_out","saved_mean","saved_variance","reserve_space" } },
{ "divide_scalar", { "out" } },
{ "sparse_coo_tensor", { "out" } },
{ "to_dense", { "out" } },
{ "to_sparse_coo", { "out" } },
{ "to_sparse_csr", { "out" } },
{ "values", { "out" } },
{ "coalesce", { "out" } },
{ "fused_attention", { "out","softmax" } },
{ "masked_matmul", { "out" } },
{ "maxpool", { "out","rulebook","counter" } },
{ "lower", { "out" } },
{ "upper", { "out" } },
};



paddle::experimental::Tensor acos_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "acos";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("acos dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("acos");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return acos_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("acos");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = acos_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "acos";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::acos(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acos", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("acos node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AcosGradNode>(new AcosGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: acos";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor acosh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "acosh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("acosh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("acosh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return acosh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("acosh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = acosh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "acosh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::acosh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acosh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("acosh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AcoshGradNode>(new AcoshGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: acosh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor addmm_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, float beta, float alpha) {
  VLOG(3) << "Running AD API: " << "addmm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("addmm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("addmm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return addmm_ad_func(new_input, new_x, new_y, beta, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{x},{y} };
    
    auto op_name = phi::TransToFluidOpName("addmm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = addmm_ad_func(new_input, new_x, new_y, beta, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "addmm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::addmm(input, x, y, beta, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("addmm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("addmm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AddmmGradNode>(new AddmmGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    grad_node->SetAttributebeta(beta);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(x, 1);
    grad_node->SetGradOutMeta(y, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: addmm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor angle_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "angle";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("angle dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("angle");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return angle_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("angle");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = angle_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "angle";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::angle(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("angle", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("angle node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AngleGradNode>(new AngleGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: angle";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> argsort_ad_func(const paddle::experimental::Tensor& x, int axis, bool descending) {
  VLOG(3) << "Running AD API: " << "argsort";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("argsort dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("argsort");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return argsort_ad_func(new_x, axis, descending);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("argsort");
    auto transformer = egr::EagerLayoutAutotune<int, bool>(op_name, tensors_vector, &axis, &descending);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = argsort_ad_func(new_x, axis, descending);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& indices = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&indices);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "argsort";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::argsort(x, axis, descending);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("argsort", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& indices = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* indices_autograd_meta = egr::EagerUtils::autograd_meta(&indices);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("argsort node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,indices_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ArgsortGradNode>(new ArgsortGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributedescending(descending);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(indices_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetHistory(indices_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(indices, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(indices);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperindices(indices);
  }


  VLOG(4) << "Finish AD API: argsort";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string output_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    output_str += output_indices_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
}


paddle::experimental::Tensor as_complex_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "as_complex";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("as_complex dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("as_complex");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return as_complex_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("as_complex");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = as_complex_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "as_complex";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::as_complex(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("as_complex", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("as_complex node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsComplexGradNode>(new AsComplexGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: as_complex";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor as_real_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "as_real";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("as_real dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("as_real");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return as_real_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("as_real");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = as_real_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "as_real";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::as_real(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("as_real", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("as_real node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsRealGradNode>(new AsRealGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: as_real";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor asin_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "asin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("asin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("asin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return asin_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("asin");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = asin_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "asin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::asin(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("asin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsinGradNode>(new AsinGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: asin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor asinh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "asinh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("asinh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("asinh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return asinh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("asinh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = asinh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "asinh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::asinh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asinh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("asinh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsinhGradNode>(new AsinhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: asinh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor atan_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "atan";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("atan dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("atan");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return atan_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("atan");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = atan_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "atan";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::atan(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("atan node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AtanGradNode>(new AtanGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: atan";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor atan2_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "atan2";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("atan2 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("atan2");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return atan2_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("atan2");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = atan2_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "atan2";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::atan2(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan2", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("atan2 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Atan2GradNode>(new Atan2GradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: atan2";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor atanh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "atanh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("atanh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("atanh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return atanh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("atanh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = atanh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "atanh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::atanh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atanh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("atanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AtanhGradNode>(new AtanhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: atanh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bernoulli_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "bernoulli";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bernoulli dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bernoulli");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bernoulli_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("bernoulli");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bernoulli_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bernoulli";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bernoulli(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bernoulli";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bmm_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "bmm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bmm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bmm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bmm_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("bmm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bmm_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "bmm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::bmm(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bmm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("bmm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BmmGradNode>(new BmmGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: bmm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor ceil_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "ceil";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("ceil dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("ceil");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return ceil_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("ceil");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = ceil_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "ceil";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::ceil(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("ceil", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("ceil node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CeilGradNode>(new CeilGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: ceil";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& ceil__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "ceil_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("ceil_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for ceil__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("ceil_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = ceil__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "ceil_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::ceil_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("ceil_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("ceil node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CeilGradNode>(new CeilGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: ceil_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor celu_ad_func(const paddle::experimental::Tensor& x, float alpha) {
  VLOG(3) << "Running AD API: " << "celu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("celu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("celu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return celu_ad_func(new_x, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("celu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = celu_ad_func(new_x, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "celu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::celu(x, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("celu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("celu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CeluGradNode>(new CeluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: celu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cholesky_ad_func(const paddle::experimental::Tensor& x, bool upper) {
  VLOG(3) << "Running AD API: " << "cholesky";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cholesky dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cholesky");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cholesky_ad_func(new_x, upper);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cholesky");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cholesky_ad_func(new_x, upper);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cholesky";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cholesky(x, upper);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cholesky", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cholesky node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CholeskyGradNode>(new CholeskyGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeupper(upper);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: cholesky";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cholesky_solve_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, bool upper) {
  VLOG(3) << "Running AD API: " << "cholesky_solve";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cholesky_solve dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cholesky_solve");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cholesky_solve_ad_func(new_x, new_y, upper);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("cholesky_solve");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cholesky_solve_ad_func(new_x, new_y, upper);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "cholesky_solve";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cholesky_solve(x, y, upper);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cholesky_solve", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cholesky_solve node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CholeskySolveGradNode>(new CholeskySolveGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeupper(upper);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: cholesky_solve";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor complex_ad_func(const paddle::experimental::Tensor& real, const paddle::experimental::Tensor& imag) {
  VLOG(3) << "Running AD API: " << "complex";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("complex dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("complex");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {real},{imag} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_real = egr::EagerAmpAutoCast("real", real, amp_dst_dtype, op_name);
    auto new_imag = egr::EagerAmpAutoCast("imag", imag, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return complex_ad_func(new_real, new_imag);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {real},{imag} };
    
    auto op_name = phi::TransToFluidOpName("complex");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_real = transformer->TransInTensor("real", real);
    auto new_imag = transformer->TransInTensor("imag", imag);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = complex_ad_func(new_real, new_imag);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* real_autograd_meta = egr::EagerUtils::nullable_autograd_meta(real);
  egr::AutogradMeta* imag_autograd_meta = egr::EagerUtils::nullable_autograd_meta(imag);

  VLOG(5) << "Running C++ API: " << "complex";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_REAL_TEMPLATE = " \n( real , [%s]), ";
    std::string input_real_str = paddle::string::Sprintf(TENSOR_REAL_TEMPLATE, egr::EagerUtils::TensorStr(real));
    input_str += input_real_str; 
    const char* TENSOR_IMAG_TEMPLATE = " \n( imag , [%s]), ";
    std::string input_imag_str = paddle::string::Sprintf(TENSOR_IMAG_TEMPLATE, egr::EagerUtils::TensorStr(imag));
    input_str += input_imag_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::complex(real, imag);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("complex", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,real_autograd_meta,imag_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("complex node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ComplexGradNode>(new ComplexGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperreal(real);
    grad_node->SetTensorWrapperimag(imag);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(real, 0);
    grad_node->SetGradOutMeta(imag, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: complex";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_REAL_TEMPLATE = " \n( real , [%s]), ";
    std::string input_real_str = paddle::string::Sprintf(TENSOR_REAL_TEMPLATE, egr::EagerUtils::TensorStr(real));
    input_str += input_real_str; 
    const char* TENSOR_IMAG_TEMPLATE = " \n( imag , [%s]), ";
    std::string input_imag_str = paddle::string::Sprintf(TENSOR_IMAG_TEMPLATE, egr::EagerUtils::TensorStr(imag));
    input_str += input_imag_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor conj_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "conj";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("conj dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("conj");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return conj_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("conj");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = conj_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "conj";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::conj(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conj", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("conj node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ConjGradNode>(new ConjGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: conj";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cos_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "cos";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cos dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cos");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cos_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cos");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cos_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cos";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cos(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cos", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cos node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CosGradNode>(new CosGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cos";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cosh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "cosh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cosh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cosh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cosh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cosh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cosh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cosh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cosh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cosh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cosh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CoshGradNode>(new CoshGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cosh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor crop_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray shape, paddle::experimental::IntArray offsets) {
  VLOG(3) << "Running AD API: " << "crop";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("crop dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("crop");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return crop_ad_func(new_x, shape, offsets);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("crop");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = crop_ad_func(new_x, shape, offsets);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "crop";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::crop(x, shape, offsets);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("crop", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("crop node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CropGradNode>(new CropGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoffsets(offsets);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: crop";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cross_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, int axis) {
  VLOG(3) << "Running AD API: " << "cross";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cross dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cross");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cross_ad_func(new_x, new_y, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("cross");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cross_ad_func(new_x, new_y, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "cross";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cross(x, y, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cross", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cross node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CrossGradNode>(new CrossGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cross";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor det_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "det";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("det dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("det");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return det_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("det");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = det_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "det";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::det(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("det", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("det node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DetGradNode>(new DetGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: det";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor diag_ad_func(const paddle::experimental::Tensor& x, int offset, float padding_value) {
  VLOG(3) << "Running AD API: " << "diag";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("diag dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("diag");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return diag_ad_func(new_x, offset, padding_value);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("diag");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = diag_ad_func(new_x, offset, padding_value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "diag";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::diag(x, offset, padding_value);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("diag", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("diag node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DiagGradNode>(new DiagGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoffset(offset);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: diag";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor diag_embed_ad_func(const paddle::experimental::Tensor& input, int offset, int dim1, int dim2) {
  VLOG(3) << "Running AD API: " << "diag_embed";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("diag_embed dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("diag_embed");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return diag_embed_ad_func(new_input, offset, dim1, dim2);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("diag_embed");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &dim1, &dim2);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = diag_embed_ad_func(new_input, offset, dim1, dim2);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "diag_embed";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::diag_embed(input, offset, dim1, dim2);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: diag_embed";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor diagonal_ad_func(const paddle::experimental::Tensor& x, int offset, int axis1, int axis2) {
  VLOG(3) << "Running AD API: " << "diagonal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("diagonal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("diagonal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return diagonal_ad_func(new_x, offset, axis1, axis2);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("diagonal");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &axis1, &axis2);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = diagonal_ad_func(new_x, offset, axis1, axis2);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "diagonal";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::diagonal(x, offset, axis1, axis2);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("diagonal", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("diagonal node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DiagonalGradNode>(new DiagonalGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributeaxis1(axis1);
    grad_node->SetAttributeaxis2(axis2);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: diagonal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor digamma_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "digamma";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("digamma dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("digamma");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return digamma_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("digamma");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = digamma_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "digamma";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::digamma(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("digamma", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("digamma node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DigammaGradNode>(new DigammaGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: digamma";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor dist_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, float p) {
  VLOG(3) << "Running AD API: " << "dist";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("dist dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("dist");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return dist_ad_func(new_x, new_y, p);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("dist");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = dist_ad_func(new_x, new_y, p);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "dist";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::dist(x, y, p);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dist", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("dist node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DistGradNode>(new DistGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributep(p);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: dist";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor dot_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "dot";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("dot dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("dot");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return dot_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("dot");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = dot_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "dot";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::dot(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dot", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("dot node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DotGradNode>(new DotGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: dot";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> eig_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "eig";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("eig dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("eig");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return eig_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("eig");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = eig_ad_func(new_x);

        auto& out_w = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out_w);
        auto& out_v = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&out_v);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out_w, out_v};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "eig";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::eig(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eig", api_result); }

  // Get Outputs
  auto& out_w = std::get<0>(api_result);
  auto& out_v = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_w_autograd_meta = egr::EagerUtils::autograd_meta(&out_w);
  egr::AutogradMeta* out_v_autograd_meta = egr::EagerUtils::autograd_meta(&out_v);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("eig node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_w_autograd_meta,out_v_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EigGradNode>(new EigGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_w_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_w_autograd_meta, 0);
    }
    if (out_v_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_v_autograd_meta, 1);
    }
    if (out_w_autograd_meta) {
      egr::EagerUtils::SetHistory(out_w_autograd_meta, grad_node);
    }
    if (out_v_autograd_meta) {
      egr::EagerUtils::SetHistory(out_v_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out_w, 0);
    grad_node->SetGradInMeta(out_v, 1);
    egr::EagerUtils::CheckAndRetainGrad(out_w);
    egr::EagerUtils::CheckAndRetainGrad(out_v);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout_w(out_w);
    grad_node->SetTensorWrapperout_v(out_v);
  }


  VLOG(4) << "Finish AD API: eig";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string output_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    output_str += output_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string output_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    output_str += output_out_v_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out_w, out_v};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> eigh_ad_func(const paddle::experimental::Tensor& x, std::string UPLO) {
  VLOG(3) << "Running AD API: " << "eigh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("eigh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("eigh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return eigh_ad_func(new_x, UPLO);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("eigh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = eigh_ad_func(new_x, UPLO);

        auto& out_w = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out_w);
        auto& out_v = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&out_v);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out_w, out_v};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "eigh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::eigh(x, UPLO);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eigh", api_result); }

  // Get Outputs
  auto& out_w = std::get<0>(api_result);
  auto& out_v = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_w_autograd_meta = egr::EagerUtils::autograd_meta(&out_w);
  egr::AutogradMeta* out_v_autograd_meta = egr::EagerUtils::autograd_meta(&out_v);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("eigh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_w_autograd_meta,out_v_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EighGradNode>(new EighGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_w_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_w_autograd_meta, 0);
    }
    if (out_v_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_v_autograd_meta, 1);
    }
    if (out_w_autograd_meta) {
      egr::EagerUtils::SetHistory(out_w_autograd_meta, grad_node);
    }
    if (out_v_autograd_meta) {
      egr::EagerUtils::SetHistory(out_v_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out_w, 0);
    grad_node->SetGradInMeta(out_v, 1);
    egr::EagerUtils::CheckAndRetainGrad(out_w);
    egr::EagerUtils::CheckAndRetainGrad(out_v);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout_w(out_w);
    grad_node->SetTensorWrapperout_v(out_v);
  }


  VLOG(4) << "Finish AD API: eigh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string output_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    output_str += output_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string output_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    output_str += output_out_v_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out_w, out_v};
}


paddle::experimental::Tensor eigvals_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "eigvals";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("eigvals dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("eigvals");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return eigvals_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("eigvals");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = eigvals_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "eigvals";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::eigvals(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: eigvals";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor elu_ad_func(const paddle::experimental::Tensor& x, float alpha) {
  VLOG(3) << "Running AD API: " << "elu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("elu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("elu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return elu_ad_func(new_x, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("elu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = elu_ad_func(new_x, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "elu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::elu(x, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("elu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EluGradNode>(new EluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: elu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& elu__ad_func(paddle::experimental::Tensor& x, float alpha) {
  VLOG(3) << "Running AD API: " << "elu_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("elu_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for elu__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("elu_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = elu__ad_func(new_x, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "elu_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::elu_(x, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elu_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("elu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EluGradNode>(new EluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: elu_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor equal_all_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "equal_all";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("equal_all dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("equal_all");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return equal_all_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("equal_all");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = equal_all_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "equal_all";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::equal_all(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: equal_all";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor erf_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "erf";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("erf dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("erf");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return erf_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("erf");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = erf_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "erf";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::erf(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("erf", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("erf node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ErfGradNode>(new ErfGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: erf";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor erfinv_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "erfinv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("erfinv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("erfinv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return erfinv_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("erfinv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = erfinv_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "erfinv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::erfinv(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("erfinv", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("erfinv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ErfinvGradNode>(new ErfinvGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: erfinv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& erfinv__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "erfinv_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("erfinv_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for erfinv__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("erfinv_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = erfinv__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "erfinv_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::erfinv_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("erfinv_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("erfinv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ErfinvGradNode>(new ErfinvGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: erfinv_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor exp_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "exp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("exp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("exp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return exp_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("exp");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = exp_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "exp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::exp(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("exp", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("exp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ExpGradNode>(new ExpGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: exp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& exp__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "exp_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("exp_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for exp__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("exp_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = exp__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "exp_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::exp_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("exp_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("exp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ExpGradNode>(new ExpGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: exp_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor expm1_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "expm1";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("expm1 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("expm1");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return expm1_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("expm1");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = expm1_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "expm1";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::expm1(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expm1", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("expm1 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Expm1GradNode>(new Expm1GradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: expm1";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fft_c2c_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axes, std::string normalization, bool forward) {
  VLOG(3) << "Running AD API: " << "fft_c2c";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fft_c2c dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fft_c2c");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fft_c2c_ad_func(new_x, axes, normalization, forward);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fft_c2c");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>>(op_name, tensors_vector, &axes);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fft_c2c_ad_func(new_x, axes, normalization, forward);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fft_c2c";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fft_c2c(x, axes, normalization, forward);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_c2c", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fft_c2c node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FftC2cGradNode>(new FftC2cGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributenormalization(normalization);
    grad_node->SetAttributeforward(forward);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fft_c2c";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fft_c2r_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axes, std::string normalization, bool forward, int64_t last_dim_size) {
  VLOG(3) << "Running AD API: " << "fft_c2r";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fft_c2r dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fft_c2r");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fft_c2r_ad_func(new_x, axes, normalization, forward, last_dim_size);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fft_c2r");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, int64_t>(op_name, tensors_vector, &axes, &last_dim_size);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fft_c2r_ad_func(new_x, axes, normalization, forward, last_dim_size);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fft_c2r";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fft_c2r(x, axes, normalization, forward, last_dim_size);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_c2r", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fft_c2r node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FftC2rGradNode>(new FftC2rGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributenormalization(normalization);
    grad_node->SetAttributeforward(forward);
    grad_node->SetAttributelast_dim_size(last_dim_size);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fft_c2r";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fft_r2c_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axes, std::string normalization, bool forward, bool onesided) {
  VLOG(3) << "Running AD API: " << "fft_r2c";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fft_r2c dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fft_r2c");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fft_r2c_ad_func(new_x, axes, normalization, forward, onesided);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fft_r2c");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>>(op_name, tensors_vector, &axes);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fft_r2c_ad_func(new_x, axes, normalization, forward, onesided);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fft_r2c";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fft_r2c(x, axes, normalization, forward, onesided);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_r2c", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fft_r2c node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FftR2cGradNode>(new FftR2cGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributenormalization(normalization);
    grad_node->SetAttributeforward(forward);
    grad_node->SetAttributeonesided(onesided);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fft_r2c";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fill_diagonal_tensor_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, int64_t offset, int dim1, int dim2) {
  VLOG(3) << "Running AD API: " << "fill_diagonal_tensor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill_diagonal_tensor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fill_diagonal_tensor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fill_diagonal_tensor_ad_func(new_x, new_y, offset, dim1, dim2);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("fill_diagonal_tensor");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &dim1, &dim2);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fill_diagonal_tensor_ad_func(new_x, new_y, offset, dim1, dim2);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill_diagonal_tensor";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fill_diagonal_tensor(x, y, offset, dim1, dim2);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal_tensor", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill_diagonal_tensor node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillDiagonalTensorGradNode>(new FillDiagonalTensorGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributedim1(dim1);
    grad_node->SetAttributedim2(dim2);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill_diagonal_tensor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& fill_diagonal_tensor__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, int64_t offset, int dim1, int dim2) {
  VLOG(3) << "Running AD API: " << "fill_diagonal_tensor_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill_diagonal_tensor_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for fill_diagonal_tensor__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("fill_diagonal_tensor_");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &dim1, &dim2);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = fill_diagonal_tensor__ad_func(new_x, new_y, offset, dim1, dim2);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill_diagonal_tensor_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::fill_diagonal_tensor_(x, y, offset, dim1, dim2);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal_tensor_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill_diagonal_tensor node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillDiagonalTensorGradNode>(new FillDiagonalTensorGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributedim1(dim1);
    grad_node->SetAttributedim2(dim2);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill_diagonal_tensor_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor flip_ad_func(const paddle::experimental::Tensor& x, std::vector<int> axis) {
  VLOG(3) << "Running AD API: " << "flip";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("flip dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("flip");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return flip_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("flip");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = flip_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "flip";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::flip(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("flip", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("flip node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FlipGradNode>(new FlipGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: flip";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor floor_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "floor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("floor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("floor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return floor_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("floor");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = floor_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "floor";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::floor(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("floor", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("floor node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FloorGradNode>(new FloorGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: floor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& floor__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "floor_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("floor_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for floor__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("floor_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = floor__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "floor_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::floor_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("floor_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("floor node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FloorGradNode>(new FloorGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: floor_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fold_ad_func(const paddle::experimental::Tensor& x, std::vector<int> output_sizes, std::vector<int> kernel_sizes, std::vector<int> strides, std::vector<int> paddings, std::vector<int> dilations) {
  VLOG(3) << "Running AD API: " << "fold";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fold dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fold");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fold_ad_func(new_x, output_sizes, kernel_sizes, strides, paddings, dilations);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fold");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fold_ad_func(new_x, output_sizes, kernel_sizes, strides, paddings, dilations);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fold";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fold(x, output_sizes, kernel_sizes, strides, paddings, dilations);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fold", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fold node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FoldGradNode>(new FoldGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoutput_sizes(output_sizes);
    grad_node->SetAttributekernel_sizes(kernel_sizes);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributedilations(dilations);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fold";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor frame_ad_func(const paddle::experimental::Tensor& x, int frame_length, int hop_length, int axis) {
  VLOG(3) << "Running AD API: " << "frame";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("frame dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("frame");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return frame_ad_func(new_x, frame_length, hop_length, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("frame");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = frame_ad_func(new_x, frame_length, hop_length, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "frame";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::frame(x, frame_length, hop_length, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("frame", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("frame node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FrameGradNode>(new FrameGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeframe_length(frame_length);
    grad_node->SetAttributehop_length(hop_length);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: frame";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gather_nd_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index) {
  VLOG(3) << "Running AD API: " << "gather_nd";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gather_nd dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("gather_nd");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return gather_nd_ad_func(new_x, new_index);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index} };
    
    auto op_name = phi::TransToFluidOpName("gather_nd");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = gather_nd_ad_func(new_x, new_index);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "gather_nd";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::gather_nd(x, index);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gather_nd", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("gather_nd node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GatherNdGradNode>(new GatherNdGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindex(index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: gather_nd";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gather_tree_ad_func(const paddle::experimental::Tensor& ids, const paddle::experimental::Tensor& parents) {
  VLOG(3) << "Running AD API: " << "gather_tree";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gather_tree dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("gather_tree");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {ids},{parents} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_ids = egr::EagerAmpAutoCast("ids", ids, amp_dst_dtype, op_name);
    auto new_parents = egr::EagerAmpAutoCast("parents", parents, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return gather_tree_ad_func(new_ids, new_parents);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {ids},{parents} };
    
    auto op_name = phi::TransToFluidOpName("gather_tree");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_ids = transformer->TransInTensor("ids", ids);
    auto new_parents = transformer->TransInTensor("parents", parents);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = gather_tree_ad_func(new_ids, new_parents);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "gather_tree";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_IDS_TEMPLATE = " \n( ids , [%s]), ";
    std::string input_ids_str = paddle::string::Sprintf(TENSOR_IDS_TEMPLATE, egr::EagerUtils::TensorStr(ids));
    input_str += input_ids_str; 
    const char* TENSOR_PARENTS_TEMPLATE = " \n( parents , [%s]), ";
    std::string input_parents_str = paddle::string::Sprintf(TENSOR_PARENTS_TEMPLATE, egr::EagerUtils::TensorStr(parents));
    input_str += input_parents_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::gather_tree(ids, parents);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: gather_tree";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_IDS_TEMPLATE = " \n( ids , [%s]), ";
    std::string input_ids_str = paddle::string::Sprintf(TENSOR_IDS_TEMPLATE, egr::EagerUtils::TensorStr(ids));
    input_str += input_ids_str; 
    const char* TENSOR_PARENTS_TEMPLATE = " \n( parents , [%s]), ";
    std::string input_parents_str = paddle::string::Sprintf(TENSOR_PARENTS_TEMPLATE, egr::EagerUtils::TensorStr(parents));
    input_str += input_parents_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gelu_ad_func(const paddle::experimental::Tensor& x, bool approximate) {
  VLOG(3) << "Running AD API: " << "gelu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gelu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("gelu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return gelu_ad_func(new_x, approximate);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("gelu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = gelu_ad_func(new_x, approximate);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "gelu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::gelu(x, approximate);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gelu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("gelu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GeluGradNode>(new GeluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeapproximate(approximate);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: gelu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor grid_sample_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& grid, std::string mode, std::string padding_mode, bool align_corners) {
  VLOG(3) << "Running AD API: " << "grid_sample";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("grid_sample dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("grid_sample");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{grid} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_grid = egr::EagerAmpAutoCast("grid", grid, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return grid_sample_ad_func(new_x, new_grid, mode, padding_mode, align_corners);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{grid} };
    
    auto op_name = phi::TransToFluidOpName("grid_sample");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_grid = transformer->TransInTensor("grid", grid);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = grid_sample_ad_func(new_x, new_grid, mode, padding_mode, align_corners);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* grid_autograd_meta = egr::EagerUtils::nullable_autograd_meta(grid);

  VLOG(5) << "Running C++ API: " << "grid_sample";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRID_TEMPLATE = " \n( grid , [%s]), ";
    std::string input_grid_str = paddle::string::Sprintf(TENSOR_GRID_TEMPLATE, egr::EagerUtils::TensorStr(grid));
    input_str += input_grid_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::grid_sample(x, grid, mode, padding_mode, align_corners);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("grid_sample", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,grid_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("grid_sample node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GridSampleGradNode>(new GridSampleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributemode(mode);
    grad_node->SetAttributepadding_mode(padding_mode);
    grad_node->SetAttributealign_corners(align_corners);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrid(grid);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grid, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: grid_sample";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRID_TEMPLATE = " \n( grid , [%s]), ";
    std::string input_grid_str = paddle::string::Sprintf(TENSOR_GRID_TEMPLATE, egr::EagerUtils::TensorStr(grid));
    input_str += input_grid_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gumbel_softmax_ad_func(const paddle::experimental::Tensor& x, float temperature, bool hard, int axis) {
  VLOG(3) << "Running AD API: " << "gumbel_softmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gumbel_softmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("gumbel_softmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return gumbel_softmax_ad_func(new_x, temperature, hard, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("gumbel_softmax");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = gumbel_softmax_ad_func(new_x, temperature, hard, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "gumbel_softmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::gumbel_softmax(x, temperature, hard, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gumbel_softmax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("gumbel_softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GumbelSoftmaxGradNode>(new GumbelSoftmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: gumbel_softmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor hardshrink_ad_func(const paddle::experimental::Tensor& x, float threshold) {
  VLOG(3) << "Running AD API: " << "hardshrink";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("hardshrink dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("hardshrink");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return hardshrink_ad_func(new_x, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("hardshrink");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = hardshrink_ad_func(new_x, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "hardshrink";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::hardshrink(x, threshold);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardshrink", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("hardshrink node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HardshrinkGradNode>(new HardshrinkGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(threshold);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: hardshrink";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor hardsigmoid_ad_func(const paddle::experimental::Tensor& x, float slope, float offset) {
  VLOG(3) << "Running AD API: " << "hardsigmoid";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("hardsigmoid dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("hardsigmoid");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return hardsigmoid_ad_func(new_x, slope, offset);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("hardsigmoid");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = hardsigmoid_ad_func(new_x, slope, offset);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "hardsigmoid";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::hardsigmoid(x, slope, offset);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardsigmoid", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("hardsigmoid node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HardsigmoidGradNode>(new HardsigmoidGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeslope(slope);
    grad_node->SetAttributeoffset(offset);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: hardsigmoid";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor histogram_ad_func(const paddle::experimental::Tensor& input, int64_t bins, int min, int max) {
  VLOG(3) << "Running AD API: " << "histogram";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("histogram dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("histogram");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return histogram_ad_func(new_input, bins, min, max);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("histogram");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = histogram_ad_func(new_input, bins, min, max);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "histogram";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::histogram(input, bins, min, max);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: histogram";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor index_sample_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index) {
  VLOG(3) << "Running AD API: " << "index_sample";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("index_sample dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("index_sample");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return index_sample_ad_func(new_x, new_index);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index} };
    
    auto op_name = phi::TransToFluidOpName("index_sample");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = index_sample_ad_func(new_x, new_index);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "index_sample";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::index_sample(x, index);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_sample", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("index_sample node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<IndexSampleGradNode>(new IndexSampleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindex(index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: index_sample";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor index_select_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, int axis) {
  VLOG(3) << "Running AD API: " << "index_select";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("index_select dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("index_select");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return index_select_ad_func(new_x, new_index, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index} };
    
    auto op_name = phi::TransToFluidOpName("index_select");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = index_select_ad_func(new_x, new_index, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "index_select";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::index_select(x, index, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_select", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("index_select node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<IndexSelectGradNode>(new IndexSelectGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindex(index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: index_select";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor inverse_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "inverse";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("inverse dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("inverse");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return inverse_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("inverse");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = inverse_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "inverse";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::inverse(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("inverse", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("inverse node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<InverseGradNode>(new InverseGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: inverse";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor is_empty_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "is_empty";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("is_empty dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("is_empty");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return is_empty_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("is_empty");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = is_empty_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "is_empty";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::is_empty(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: is_empty";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor isclose_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, paddle::experimental::Scalar rtol, paddle::experimental::Scalar atol, bool equal_nan) {
  VLOG(3) << "Running AD API: " << "isclose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("isclose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("isclose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return isclose_ad_func(new_x, new_y, rtol, atol, equal_nan);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("isclose");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = isclose_ad_func(new_x, new_y, rtol, atol, equal_nan);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "isclose";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::isclose(x, y, rtol, atol, equal_nan);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: isclose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor isfinite_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "isfinite";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("isfinite dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("isfinite");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return isfinite_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("isfinite");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = isfinite_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "isfinite";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::isfinite(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: isfinite";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor isinf_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "isinf";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("isinf dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("isinf");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return isinf_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("isinf");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = isinf_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "isinf";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::isinf(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: isinf";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor isnan_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "isnan";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("isnan dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("isnan");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return isnan_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("isnan");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = isnan_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "isnan";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::isnan(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: isnan";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> kthvalue_ad_func(const paddle::experimental::Tensor& x, int k, int axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "kthvalue";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("kthvalue dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("kthvalue");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return kthvalue_ad_func(new_x, k, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("kthvalue");
    auto transformer = egr::EagerLayoutAutotune<int, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = kthvalue_ad_func(new_x, k, axis, keepdim);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& indices = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&indices);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "kthvalue";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::kthvalue(x, k, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kthvalue", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& indices = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* indices_autograd_meta = egr::EagerUtils::autograd_meta(&indices);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("kthvalue node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,indices_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<KthvalueGradNode>(new KthvalueGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributek(k);
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(indices_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetHistory(indices_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(indices, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(indices);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperindices(indices);
  }


  VLOG(4) << "Finish AD API: kthvalue";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string output_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    output_str += output_indices_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
}


paddle::experimental::Tensor label_smooth_ad_func(const paddle::experimental::Tensor& label, const paddle::optional<paddle::experimental::Tensor>& prior_dist, float epsilon) {
  VLOG(3) << "Running AD API: " << "label_smooth";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("label_smooth dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("label_smooth");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {label} };
    if (prior_dist) amp_tensors_vector.push_back({ *prior_dist });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_prior_dist = egr::EagerAmpAutoCast("prior_dist", prior_dist, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return label_smooth_ad_func(new_label, new_prior_dist, epsilon);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {label} };
    if (prior_dist) tensors_vector.push_back({ *prior_dist });

    auto op_name = phi::TransToFluidOpName("label_smooth");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_label = transformer->TransInTensor("label", label);
auto new_prior_dist = transformer->TransInTensor("prior_dist", prior_dist);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = label_smooth_ad_func(new_label, new_prior_dist, epsilon);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* label_autograd_meta = egr::EagerUtils::nullable_autograd_meta(label);

  VLOG(5) << "Running C++ API: " << "label_smooth";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_PRIOR_DIST_TEMPLATE = " \n( prior_dist , [%s]), ";
    std::string input_prior_dist_str = paddle::string::Sprintf(TENSOR_PRIOR_DIST_TEMPLATE, egr::EagerUtils::TensorStr(prior_dist));
    input_str += input_prior_dist_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::label_smooth(label, prior_dist, epsilon);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("label_smooth", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,label_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("label_smooth node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LabelSmoothGradNode>(new LabelSmoothGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(label, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: label_smooth";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_PRIOR_DIST_TEMPLATE = " \n( prior_dist , [%s]), ";
    std::string input_prior_dist_str = paddle::string::Sprintf(TENSOR_PRIOR_DIST_TEMPLATE, egr::EagerUtils::TensorStr(prior_dist));
    input_str += input_prior_dist_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor leaky_relu_ad_func(const paddle::experimental::Tensor& x, float negative_slope) {
  VLOG(3) << "Running AD API: " << "leaky_relu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("leaky_relu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("leaky_relu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return leaky_relu_ad_func(new_x, negative_slope);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("leaky_relu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = leaky_relu_ad_func(new_x, negative_slope);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "leaky_relu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::leaky_relu(x, negative_slope);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("leaky_relu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("leaky_relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LeakyReluGradNode>(new LeakyReluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributenegative_slope(negative_slope);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: leaky_relu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor lerp_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& weight) {
  VLOG(3) << "Running AD API: " << "lerp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lerp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lerp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y},{weight} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    auto new_weight = egr::EagerAmpAutoCast("weight", weight, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lerp_ad_func(new_x, new_y, new_weight);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{weight} };
    
    auto op_name = phi::TransToFluidOpName("lerp");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_weight = transformer->TransInTensor("weight", weight);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = lerp_ad_func(new_x, new_y, new_weight);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "lerp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::lerp(x, y, weight);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lerp", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("lerp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LerpGradNode>(new LerpGradNode(1, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperweight(weight);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: lerp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& lerp__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& weight) {
  VLOG(3) << "Running AD API: " << "lerp_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lerp_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for lerp__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{weight} };
    
    auto op_name = phi::TransToFluidOpName("lerp_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_weight = transformer->TransInTensor("weight", weight);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = lerp__ad_func(new_x, new_y, new_weight);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "lerp_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::lerp_(x, y, weight);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lerp_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("lerp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LerpGradNode>(new LerpGradNode(1, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperweight(weight);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: lerp_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor lgamma_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "lgamma";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lgamma dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lgamma");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lgamma_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("lgamma");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = lgamma_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "lgamma";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::lgamma(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lgamma", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("lgamma node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LgammaGradNode>(new LgammaGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: lgamma";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "log";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogGradNode>(new LogGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log10_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "log10";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log10 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log10");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log10_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log10");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log10_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log10";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log10(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log10", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log10 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Log10GradNode>(new Log10GradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log10";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log1p_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "log1p";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log1p dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log1p");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log1p_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log1p");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log1p_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log1p";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log1p(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log1p", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log1p node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Log1pGradNode>(new Log1pGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log1p";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log2_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "log2";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log2 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log2");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log2_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log2");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log2_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log2";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log2(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log2", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log2 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Log2GradNode>(new Log2GradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log2";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log_loss_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label, float epsilon) {
  VLOG(3) << "Running AD API: " << "log_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log_loss_ad_func(new_input, new_label, epsilon);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label} };
    
    auto op_name = phi::TransToFluidOpName("log_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log_loss_ad_func(new_input, new_label, epsilon);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "log_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log_loss(input, label, epsilon);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_loss", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogLossGradNode>(new LogLossGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logit_ad_func(const paddle::experimental::Tensor& x, float eps) {
  VLOG(3) << "Running AD API: " << "logit";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logit dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logit");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logit_ad_func(new_x, eps);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("logit");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logit_ad_func(new_x, eps);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "logit";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::logit(x, eps);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logit", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("logit node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogitGradNode>(new LogitGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeeps(eps);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: logit";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logsigmoid_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "logsigmoid";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logsigmoid dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logsigmoid");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logsigmoid_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("logsigmoid");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logsigmoid_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "logsigmoid";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::logsigmoid(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logsigmoid", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("logsigmoid node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogsigmoidGradNode>(new LogsigmoidGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: logsigmoid";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> lu_unpack_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, bool unpack_ludata, bool unpack_pivots) {
  VLOG(3) << "Running AD API: " << "lu_unpack";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lu_unpack dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lu_unpack");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lu_unpack_ad_func(new_x, new_y, unpack_ludata, unpack_pivots);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("lu_unpack");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = lu_unpack_ad_func(new_x, new_y, unpack_ludata, unpack_pivots);

        auto& pmat = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&pmat);
        auto& l = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&l);
        auto& u = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&u);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{pmat, l, u};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "lu_unpack";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::lu_unpack(x, y, unpack_ludata, unpack_pivots);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lu_unpack", api_result); }

  // Get Outputs
  auto& pmat = std::get<0>(api_result);
  auto& l = std::get<1>(api_result);
  auto& u = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* pmat_autograd_meta = egr::EagerUtils::autograd_meta(&pmat);
  egr::AutogradMeta* l_autograd_meta = egr::EagerUtils::autograd_meta(&l);
  egr::AutogradMeta* u_autograd_meta = egr::EagerUtils::autograd_meta(&u);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("lu_unpack node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,pmat_autograd_meta,l_autograd_meta,u_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LuUnpackGradNode>(new LuUnpackGradNode(3, 2));
    // SetAttributes if needed
    grad_node->SetAttributeunpack_ludata(unpack_ludata);
    grad_node->SetAttributeunpack_pivots(unpack_pivots);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (pmat_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(pmat_autograd_meta, 0);
    }
    if (l_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(l_autograd_meta, 1);
    }
    if (u_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(u_autograd_meta, 2);
    }
    if (pmat_autograd_meta) {
      egr::EagerUtils::SetHistory(pmat_autograd_meta, grad_node);
    }
    if (l_autograd_meta) {
      egr::EagerUtils::SetHistory(l_autograd_meta, grad_node);
    }
    if (u_autograd_meta) {
      egr::EagerUtils::SetHistory(u_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(pmat, 0);
    grad_node->SetGradInMeta(l, 1);
    grad_node->SetGradInMeta(u, 2);
    egr::EagerUtils::CheckAndRetainGrad(pmat);
    egr::EagerUtils::CheckAndRetainGrad(l);
    egr::EagerUtils::CheckAndRetainGrad(u);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperl(l);
    grad_node->SetTensorWrapperu(u);
    grad_node->SetTensorWrapperpmat(pmat);
  }


  VLOG(4) << "Finish AD API: lu_unpack";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_PMAT_TEMPLATE = " \n( pmat , [%s]), ";
    std::string output_pmat_str = paddle::string::Sprintf(TENSOR_PMAT_TEMPLATE, egr::EagerUtils::TensorStr(pmat));
    output_str += output_pmat_str; 
    const char* TENSOR_L_TEMPLATE = " \n( l , [%s]), ";
    std::string output_l_str = paddle::string::Sprintf(TENSOR_L_TEMPLATE, egr::EagerUtils::TensorStr(l));
    output_str += output_l_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string output_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    output_str += output_u_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{pmat, l, u};
}


paddle::experimental::Tensor masked_select_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& mask) {
  VLOG(3) << "Running AD API: " << "masked_select";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("masked_select dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("masked_select");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{mask} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_mask = egr::EagerAmpAutoCast("mask", mask, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return masked_select_ad_func(new_x, new_mask);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{mask} };
    
    auto op_name = phi::TransToFluidOpName("masked_select");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_mask = transformer->TransInTensor("mask", mask);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = masked_select_ad_func(new_x, new_mask);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "masked_select";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::masked_select(x, mask);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("masked_select", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("masked_select node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaskedSelectGradNode>(new MaskedSelectGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappermask(mask);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: masked_select";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor matrix_power_ad_func(const paddle::experimental::Tensor& x, int n) {
  VLOG(3) << "Running AD API: " << "matrix_power";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matrix_power dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matrix_power");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matrix_power_ad_func(new_x, n);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("matrix_power");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = matrix_power_ad_func(new_x, n);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "matrix_power";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::matrix_power(x, n);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matrix_power", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("matrix_power node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MatrixPowerGradNode>(new MatrixPowerGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributen(n);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: matrix_power";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor maxout_ad_func(const paddle::experimental::Tensor& x, int groups, int axis) {
  VLOG(3) << "Running AD API: " << "maxout";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("maxout dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("maxout");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return maxout_ad_func(new_x, groups, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("maxout");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = maxout_ad_func(new_x, groups, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "maxout";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::maxout(x, groups, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maxout", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("maxout node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaxoutGradNode>(new MaxoutGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: maxout";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> mode_ad_func(const paddle::experimental::Tensor& x, int axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "mode";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mode dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mode");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mode_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("mode");
    auto transformer = egr::EagerLayoutAutotune<int, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = mode_ad_func(new_x, axis, keepdim);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& indices = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&indices);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "mode";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::mode(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mode", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& indices = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* indices_autograd_meta = egr::EagerUtils::autograd_meta(&indices);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mode node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,indices_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ModeGradNode>(new ModeGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(indices_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetHistory(indices_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(indices, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(indices);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperindices(indices);
  }


  VLOG(4) << "Finish AD API: mode";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string output_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    output_str += output_indices_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
}


paddle::experimental::Tensor multinomial_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar num_samples, bool replacement) {
  VLOG(3) << "Running AD API: " << "multinomial";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multinomial dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multinomial");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multinomial_ad_func(new_x, num_samples, replacement);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("multinomial");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = multinomial_ad_func(new_x, num_samples, replacement);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "multinomial";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::multinomial(x, num_samples, replacement);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: multinomial";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor mv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& vec) {
  VLOG(3) << "Running AD API: " << "mv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{vec} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_vec = egr::EagerAmpAutoCast("vec", vec, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mv_ad_func(new_x, new_vec);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{vec} };
    
    auto op_name = phi::TransToFluidOpName("mv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_vec = transformer->TransInTensor("vec", vec);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = mv_ad_func(new_x, new_vec);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* vec_autograd_meta = egr::EagerUtils::nullable_autograd_meta(vec);

  VLOG(5) << "Running C++ API: " << "mv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::mv(x, vec);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mv", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,vec_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MvGradNode>(new MvGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappervec(vec);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(vec, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: mv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> nll_loss_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label, const paddle::optional<paddle::experimental::Tensor>& weight, int64_t ignore_index, std::string reduction) {
  VLOG(3) << "Running AD API: " << "nll_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("nll_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("nll_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label} };
    if (weight) amp_tensors_vector.push_back({ *weight });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_weight = egr::EagerAmpAutoCast("weight", weight, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return nll_loss_ad_func(new_input, new_label, new_weight, ignore_index, reduction);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label} };
    if (weight) tensors_vector.push_back({ *weight });

    auto op_name = phi::TransToFluidOpName("nll_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);
auto new_weight = transformer->TransInTensor("weight", weight);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = nll_loss_ad_func(new_input, new_label, new_weight, ignore_index, reduction);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& total_weight = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&total_weight);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, total_weight};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "nll_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::nll_loss(input, label, weight, ignore_index, reduction);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("nll_loss", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& total_weight = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* total_weight_autograd_meta = egr::EagerUtils::autograd_meta(&total_weight);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("nll_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,total_weight_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<NllLossGradNode>(new NllLossGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributeignore_index(ignore_index);
    grad_node->SetAttributereduction(reduction);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperlabel(label);
    if(weight) grad_node->SetTensorWrapperweight(*weight);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (total_weight_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(total_weight_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (total_weight_autograd_meta) {
      egr::EagerUtils::SetHistory(total_weight_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(total_weight, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(total_weight);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappertotal_weight(total_weight);
  }


  VLOG(4) << "Finish AD API: nll_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_TOTAL_WEIGHT_TEMPLATE = " \n( total_weight , [%s]), ";
    std::string output_total_weight_str = paddle::string::Sprintf(TENSOR_TOTAL_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(total_weight));
    output_str += output_total_weight_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, total_weight};
}


paddle::experimental::Tensor npu_identity_ad_func(const paddle::experimental::Tensor& x, int format) {
  VLOG(3) << "Running AD API: " << "npu_identity";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("npu_identity dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("npu_identity");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return npu_identity_ad_func(new_x, format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("npu_identity");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = npu_identity_ad_func(new_x, format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "npu_identity";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::npu_identity(x, format);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: npu_identity";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor overlap_add_ad_func(const paddle::experimental::Tensor& x, int hop_length, int axis) {
  VLOG(3) << "Running AD API: " << "overlap_add";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("overlap_add dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("overlap_add");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return overlap_add_ad_func(new_x, hop_length, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("overlap_add");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = overlap_add_ad_func(new_x, hop_length, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "overlap_add";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::overlap_add(x, hop_length, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("overlap_add", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("overlap_add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<OverlapAddGradNode>(new OverlapAddGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributehop_length(hop_length);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: overlap_add";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pixel_shuffle_ad_func(const paddle::experimental::Tensor& x, int upscale_factor, std::string data_format) {
  VLOG(3) << "Running AD API: " << "pixel_shuffle";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pixel_shuffle dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pixel_shuffle");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pixel_shuffle_ad_func(new_x, upscale_factor, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pixel_shuffle");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pixel_shuffle_ad_func(new_x, upscale_factor, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pixel_shuffle";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pixel_shuffle(x, upscale_factor, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pixel_shuffle", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pixel_shuffle node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PixelShuffleGradNode>(new PixelShuffleGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeupscale_factor(upscale_factor);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: pixel_shuffle";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor poisson_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "poisson";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("poisson dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("poisson");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return poisson_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("poisson");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = poisson_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "poisson";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::poisson(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("poisson", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("poisson node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PoissonGradNode>(new PoissonGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: poisson";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor put_along_axis_ad_func(const paddle::experimental::Tensor& arr, const paddle::experimental::Tensor& indices, const paddle::experimental::Tensor& value, int axis, std::string reduce) {
  VLOG(3) << "Running AD API: " << "put_along_axis";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("put_along_axis dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("put_along_axis");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {arr},{indices},{value} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_arr = egr::EagerAmpAutoCast("arr", arr, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    auto new_value = egr::EagerAmpAutoCast("value", value, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return put_along_axis_ad_func(new_arr, new_indices, new_value, axis, reduce);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {arr},{indices},{value} };
    
    auto op_name = phi::TransToFluidOpName("put_along_axis");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_arr = transformer->TransInTensor("arr", arr);
    auto new_indices = transformer->TransInTensor("indices", indices);
    auto new_value = transformer->TransInTensor("value", value);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = put_along_axis_ad_func(new_arr, new_indices, new_value, axis, reduce);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* arr_autograd_meta = egr::EagerUtils::nullable_autograd_meta(arr);
  egr::AutogradMeta* value_autograd_meta = egr::EagerUtils::nullable_autograd_meta(value);

  VLOG(5) << "Running C++ API: " << "put_along_axis";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::put_along_axis(arr, indices, value, axis, reduce);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("put_along_axis", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,arr_autograd_meta,value_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("put_along_axis node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PutAlongAxisGradNode>(new PutAlongAxisGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributereduce(reduce);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperarr(arr);
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(arr, 0);
    grad_node->SetGradOutMeta(value, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: put_along_axis";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& put_along_axis__ad_func(paddle::experimental::Tensor& arr, const paddle::experimental::Tensor& indices, const paddle::experimental::Tensor& value, int axis, std::string reduce) {
  VLOG(3) << "Running AD API: " << "put_along_axis_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("put_along_axis_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for put_along_axis__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {arr},{indices},{value} };
    
    auto op_name = phi::TransToFluidOpName("put_along_axis_");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_arr = transformer->TransInTensor("arr", arr);
    auto new_indices = transformer->TransInTensor("indices", indices);
    auto new_value = transformer->TransInTensor("value", value);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = put_along_axis__ad_func(new_arr, new_indices, new_value, axis, reduce);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* arr_autograd_meta = egr::EagerUtils::nullable_autograd_meta(arr);
  egr::AutogradMeta* value_autograd_meta = egr::EagerUtils::nullable_autograd_meta(value);

  VLOG(5) << "Running C++ API: " << "put_along_axis_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::put_along_axis_(arr, indices, value, axis, reduce);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("put_along_axis_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,arr_autograd_meta,value_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(arr, arr_autograd_meta, require_any_grad);

  // Bump Inplace Version
  arr.bump_inplace_version();
  VLOG(3) << "Tensor(" << arr.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("put_along_axis node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PutAlongAxisGradNode>(new PutAlongAxisGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributereduce(reduce);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperarr(arr);
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(arr, 0);
    grad_node->SetGradOutMeta(value, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: put_along_axis_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> qr_ad_func(const paddle::experimental::Tensor& x, std::string mode) {
  VLOG(3) << "Running AD API: " << "qr";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("qr dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("qr");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return qr_ad_func(new_x, mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("qr");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = qr_ad_func(new_x, mode);

        auto& q = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&q);
        auto& r = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&r);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{q, r};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "qr";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::qr(x, mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("qr", api_result); }

  // Get Outputs
  auto& q = std::get<0>(api_result);
  auto& r = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* q_autograd_meta = egr::EagerUtils::autograd_meta(&q);
  egr::AutogradMeta* r_autograd_meta = egr::EagerUtils::autograd_meta(&r);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("qr node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,q_autograd_meta,r_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<QrGradNode>(new QrGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributemode(mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (q_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(q_autograd_meta, 0);
    }
    if (r_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(r_autograd_meta, 1);
    }
    if (q_autograd_meta) {
      egr::EagerUtils::SetHistory(q_autograd_meta, grad_node);
    }
    if (r_autograd_meta) {
      egr::EagerUtils::SetHistory(r_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(q, 0);
    grad_node->SetGradInMeta(r, 1);
    egr::EagerUtils::CheckAndRetainGrad(q);
    egr::EagerUtils::CheckAndRetainGrad(r);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperq(q);
    grad_node->SetTensorWrapperr(r);
  }


  VLOG(4) << "Finish AD API: qr";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Q_TEMPLATE = " \n( q , [%s]), ";
    std::string output_q_str = paddle::string::Sprintf(TENSOR_Q_TEMPLATE, egr::EagerUtils::TensorStr(q));
    output_str += output_q_str; 
    const char* TENSOR_R_TEMPLATE = " \n( r , [%s]), ";
    std::string output_r_str = paddle::string::Sprintf(TENSOR_R_TEMPLATE, egr::EagerUtils::TensorStr(r));
    output_str += output_r_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{q, r};
}


paddle::experimental::Tensor reciprocal_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "reciprocal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reciprocal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("reciprocal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return reciprocal_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reciprocal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = reciprocal_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reciprocal";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::reciprocal(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reciprocal", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reciprocal node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReciprocalGradNode>(new ReciprocalGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: reciprocal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& reciprocal__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "reciprocal_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reciprocal_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for reciprocal__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reciprocal_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = reciprocal__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reciprocal_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::reciprocal_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reciprocal_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reciprocal node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReciprocalGradNode>(new ReciprocalGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: reciprocal_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor relu_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "relu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("relu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("relu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return relu_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("relu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = relu_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "relu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::relu(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReluGradNode>(new ReluGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: relu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& relu__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "relu_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("relu_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for relu__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("relu_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = relu__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "relu_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::relu_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReluGradNode>(new ReluGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: relu_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor renorm_ad_func(const paddle::experimental::Tensor& x, float p, int axis, float max_norm) {
  VLOG(3) << "Running AD API: " << "renorm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("renorm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("renorm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return renorm_ad_func(new_x, p, axis, max_norm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("renorm");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = renorm_ad_func(new_x, p, axis, max_norm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "renorm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::renorm(x, p, axis, max_norm);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("renorm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("renorm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RenormGradNode>(new RenormGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributep(p);
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributemax_norm(max_norm);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: renorm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor roll_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray shifts, std::vector<int64_t> axis) {
  VLOG(3) << "Running AD API: " << "roll";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("roll dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("roll");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return roll_ad_func(new_x, shifts, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("roll");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = roll_ad_func(new_x, shifts, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "roll";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::roll(x, shifts, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roll", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("roll node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RollGradNode>(new RollGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeshifts(shifts);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: roll";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor round_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "round";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("round dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("round");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return round_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("round");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = round_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "round";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::round(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("round", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("round node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RoundGradNode>(new RoundGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: round";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& round__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "round_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("round_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for round__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("round_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = round__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "round_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::round_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("round_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("round node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RoundGradNode>(new RoundGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: round_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor rsqrt_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "rsqrt";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("rsqrt dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("rsqrt");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return rsqrt_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("rsqrt");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = rsqrt_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "rsqrt";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::rsqrt(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rsqrt", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("rsqrt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RsqrtGradNode>(new RsqrtGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: rsqrt";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& rsqrt__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "rsqrt_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("rsqrt_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for rsqrt__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("rsqrt_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = rsqrt__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "rsqrt_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::rsqrt_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rsqrt_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("rsqrt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RsqrtGradNode>(new RsqrtGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: rsqrt_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor scatter_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, const paddle::experimental::Tensor& updates, bool overwrite) {
  VLOG(3) << "Running AD API: " << "scatter";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scatter dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("scatter");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index},{updates} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    auto new_updates = egr::EagerAmpAutoCast("updates", updates, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return scatter_ad_func(new_x, new_index, new_updates, overwrite);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index},{updates} };
    
    auto op_name = phi::TransToFluidOpName("scatter");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);
    auto new_updates = transformer->TransInTensor("updates", updates);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = scatter_ad_func(new_x, new_index, new_updates, overwrite);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* updates_autograd_meta = egr::EagerUtils::nullable_autograd_meta(updates);

  VLOG(5) << "Running C++ API: " << "scatter";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::scatter(x, index, updates, overwrite);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scatter", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,updates_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scatter node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScatterGradNode>(new ScatterGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeoverwrite(overwrite);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindex(index);
    grad_node->SetTensorWrapperupdates(updates);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(updates, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scatter";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& scatter__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, const paddle::experimental::Tensor& updates, bool overwrite) {
  VLOG(3) << "Running AD API: " << "scatter_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scatter_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for scatter__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index},{updates} };
    
    auto op_name = phi::TransToFluidOpName("scatter_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);
    auto new_updates = transformer->TransInTensor("updates", updates);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = scatter__ad_func(new_x, new_index, new_updates, overwrite);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* updates_autograd_meta = egr::EagerUtils::nullable_autograd_meta(updates);

  VLOG(5) << "Running C++ API: " << "scatter_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::scatter_(x, index, updates, overwrite);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scatter_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,updates_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scatter node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScatterGradNode>(new ScatterGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeoverwrite(overwrite);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindex(index);
    grad_node->SetTensorWrapperupdates(updates);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(updates, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scatter_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor scatter_nd_add_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, const paddle::experimental::Tensor& updates) {
  VLOG(3) << "Running AD API: " << "scatter_nd_add";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scatter_nd_add dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("scatter_nd_add");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index},{updates} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    auto new_updates = egr::EagerAmpAutoCast("updates", updates, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return scatter_nd_add_ad_func(new_x, new_index, new_updates);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index},{updates} };
    
    auto op_name = phi::TransToFluidOpName("scatter_nd_add");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);
    auto new_updates = transformer->TransInTensor("updates", updates);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = scatter_nd_add_ad_func(new_x, new_index, new_updates);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* updates_autograd_meta = egr::EagerUtils::nullable_autograd_meta(updates);

  VLOG(5) << "Running C++ API: " << "scatter_nd_add";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::scatter_nd_add(x, index, updates);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scatter_nd_add", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,updates_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scatter_nd_add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScatterNdAddGradNode>(new ScatterNdAddGradNode(1, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindex(index);
    grad_node->SetTensorWrapperupdates(updates);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(updates, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scatter_nd_add";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor searchsorted_ad_func(const paddle::experimental::Tensor& sorted_sequence, const paddle::experimental::Tensor& values, bool out_int32, bool right) {
  VLOG(3) << "Running AD API: " << "searchsorted";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("searchsorted dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("searchsorted");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {sorted_sequence},{values} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_sorted_sequence = egr::EagerAmpAutoCast("sorted_sequence", sorted_sequence, amp_dst_dtype, op_name);
    auto new_values = egr::EagerAmpAutoCast("values", values, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return searchsorted_ad_func(new_sorted_sequence, new_values, out_int32, right);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {sorted_sequence},{values} };
    
    auto op_name = phi::TransToFluidOpName("searchsorted");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_sorted_sequence = transformer->TransInTensor("sorted_sequence", sorted_sequence);
    auto new_values = transformer->TransInTensor("values", values);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = searchsorted_ad_func(new_sorted_sequence, new_values, out_int32, right);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "searchsorted";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_SORTED_SEQUENCE_TEMPLATE = " \n( sorted_sequence , [%s]), ";
    std::string input_sorted_sequence_str = paddle::string::Sprintf(TENSOR_SORTED_SEQUENCE_TEMPLATE, egr::EagerUtils::TensorStr(sorted_sequence));
    input_str += input_sorted_sequence_str; 
    const char* TENSOR_VALUES_TEMPLATE = " \n( values , [%s]), ";
    std::string input_values_str = paddle::string::Sprintf(TENSOR_VALUES_TEMPLATE, egr::EagerUtils::TensorStr(values));
    input_str += input_values_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::searchsorted(sorted_sequence, values, out_int32, right);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: searchsorted";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_SORTED_SEQUENCE_TEMPLATE = " \n( sorted_sequence , [%s]), ";
    std::string input_sorted_sequence_str = paddle::string::Sprintf(TENSOR_SORTED_SEQUENCE_TEMPLATE, egr::EagerUtils::TensorStr(sorted_sequence));
    input_str += input_sorted_sequence_str; 
    const char* TENSOR_VALUES_TEMPLATE = " \n( values , [%s]), ";
    std::string input_values_str = paddle::string::Sprintf(TENSOR_VALUES_TEMPLATE, egr::EagerUtils::TensorStr(values));
    input_str += input_values_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor selu_ad_func(const paddle::experimental::Tensor& x, float scale, float alpha) {
  VLOG(3) << "Running AD API: " << "selu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("selu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("selu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return selu_ad_func(new_x, scale, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("selu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = selu_ad_func(new_x, scale, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "selu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::selu(x, scale, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("selu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("selu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SeluGradNode>(new SeluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: selu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor send_uv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& src_index, const paddle::experimental::Tensor& dst_index, std::string message_op) {
  VLOG(3) << "Running AD API: " << "send_uv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("send_uv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("send_uv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y},{src_index},{dst_index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    auto new_src_index = egr::EagerAmpAutoCast("src_index", src_index, amp_dst_dtype, op_name);
    auto new_dst_index = egr::EagerAmpAutoCast("dst_index", dst_index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return send_uv_ad_func(new_x, new_y, new_src_index, new_dst_index, message_op);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{src_index},{dst_index} };
    
    auto op_name = phi::TransToFluidOpName("send_uv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_src_index = transformer->TransInTensor("src_index", src_index);
    auto new_dst_index = transformer->TransInTensor("dst_index", dst_index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = send_uv_ad_func(new_x, new_y, new_src_index, new_dst_index, message_op);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "send_uv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::send_uv(x, y, src_index, dst_index, message_op);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_uv", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("send_uv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SendUvGradNode>(new SendUvGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributemessage_op(message_op);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappersrc_index(src_index);
    grad_node->SetTensorWrapperdst_index(dst_index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: send_uv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor shard_index_ad_func(const paddle::experimental::Tensor& input, int index_num, int nshards, int shard_id, int ignore_value) {
  VLOG(3) << "Running AD API: " << "shard_index";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("shard_index dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("shard_index");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return shard_index_ad_func(new_input, index_num, nshards, shard_id, ignore_value);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("shard_index");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = shard_index_ad_func(new_input, index_num, nshards, shard_id, ignore_value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "shard_index";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::shard_index(input, index_num, nshards, shard_id, ignore_value);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: shard_index";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sigmoid_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sigmoid";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sigmoid dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sigmoid");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sigmoid_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sigmoid");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sigmoid_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sigmoid";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sigmoid(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sigmoid node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SigmoidGradNode>(new SigmoidGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: sigmoid";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor silu_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "silu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("silu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("silu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return silu_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("silu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = silu_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "silu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::silu(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("silu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("silu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SiluGradNode>(new SiluGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: silu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sin_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sin_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sin");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sin_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sin(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SinGradNode>(new SinGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sinh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sinh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sinh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sinh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sinh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sinh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sinh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sinh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sinh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sinh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sinh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SinhGradNode>(new SinhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sinh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor softplus_ad_func(const paddle::experimental::Tensor& x, float beta, float threshold) {
  VLOG(3) << "Running AD API: " << "softplus";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softplus dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("softplus");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return softplus_ad_func(new_x, beta, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softplus");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = softplus_ad_func(new_x, beta, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softplus";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::softplus(x, beta, threshold);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softplus", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softplus node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftplusGradNode>(new SoftplusGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributebeta(beta);
    grad_node->SetAttributethreshold(threshold);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: softplus";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor softshrink_ad_func(const paddle::experimental::Tensor& x, float threshold) {
  VLOG(3) << "Running AD API: " << "softshrink";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softshrink dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("softshrink");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return softshrink_ad_func(new_x, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softshrink");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = softshrink_ad_func(new_x, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softshrink";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::softshrink(x, threshold);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softshrink", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softshrink node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftshrinkGradNode>(new SoftshrinkGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(threshold);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: softshrink";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor softsign_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "softsign";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softsign dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("softsign");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return softsign_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softsign");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = softsign_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softsign";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::softsign(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softsign", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softsign node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftsignGradNode>(new SoftsignGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: softsign";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor solve_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "solve";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("solve dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("solve");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return solve_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("solve");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = solve_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "solve";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::solve(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("solve", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("solve node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SolveGradNode>(new SolveGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: solve";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sqrt_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sqrt";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sqrt dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sqrt");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sqrt_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sqrt");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sqrt_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sqrt";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sqrt(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sqrt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SqrtGradNode>(new SqrtGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: sqrt";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& sqrt__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sqrt_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sqrt_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for sqrt__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sqrt_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = sqrt__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sqrt_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::sqrt_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sqrt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SqrtGradNode>(new SqrtGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: sqrt_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor square_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "square";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("square dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("square");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return square_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("square");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = square_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "square";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::square(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("square", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("square node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SquareGradNode>(new SquareGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: square";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor squeeze_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis) {
  VLOG(3) << "Running AD API: " << "squeeze";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("squeeze dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("squeeze");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return squeeze_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("squeeze");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = squeeze_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "squeeze";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::squeeze_intermediate(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squeeze_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("squeeze node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SqueezeGradNode>(new SqueezeGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: squeeze";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& squeeze__ad_func(paddle::experimental::Tensor& x, paddle::experimental::IntArray axis) {
  VLOG(3) << "Running AD API: " << "squeeze_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("squeeze_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for squeeze__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("squeeze_");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = squeeze__ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "squeeze_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::squeeze_intermediate_(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squeeze_intermediate_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("squeeze node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SqueezeGradNode>(new SqueezeGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: squeeze_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> svd_ad_func(const paddle::experimental::Tensor& x, bool full_matrices) {
  VLOG(3) << "Running AD API: " << "svd";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("svd dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("svd");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return svd_ad_func(new_x, full_matrices);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("svd");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = svd_ad_func(new_x, full_matrices);

        auto& u = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&u);
        auto& s = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&s);
        auto& vh = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&vh);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{u, s, vh};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "svd";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::svd(x, full_matrices);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("svd", api_result); }

  // Get Outputs
  auto& u = std::get<0>(api_result);
  auto& s = std::get<1>(api_result);
  auto& vh = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* u_autograd_meta = egr::EagerUtils::autograd_meta(&u);
  egr::AutogradMeta* s_autograd_meta = egr::EagerUtils::autograd_meta(&s);
  egr::AutogradMeta* vh_autograd_meta = egr::EagerUtils::autograd_meta(&vh);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("svd node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,u_autograd_meta,s_autograd_meta,vh_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SvdGradNode>(new SvdGradNode(3, 1));
    // SetAttributes if needed
    grad_node->SetAttributefull_matrices(full_matrices);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (u_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(u_autograd_meta, 0);
    }
    if (s_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(s_autograd_meta, 1);
    }
    if (vh_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(vh_autograd_meta, 2);
    }
    if (u_autograd_meta) {
      egr::EagerUtils::SetHistory(u_autograd_meta, grad_node);
    }
    if (s_autograd_meta) {
      egr::EagerUtils::SetHistory(s_autograd_meta, grad_node);
    }
    if (vh_autograd_meta) {
      egr::EagerUtils::SetHistory(vh_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(u, 0);
    grad_node->SetGradInMeta(s, 1);
    grad_node->SetGradInMeta(vh, 2);
    egr::EagerUtils::CheckAndRetainGrad(u);
    egr::EagerUtils::CheckAndRetainGrad(s);
    egr::EagerUtils::CheckAndRetainGrad(vh);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperu(u);
    grad_node->SetTensorWrappervh(vh);
    grad_node->SetTensorWrappers(s);
  }


  VLOG(4) << "Finish AD API: svd";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string output_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    output_str += output_u_str; 
    const char* TENSOR_S_TEMPLATE = " \n( s , [%s]), ";
    std::string output_s_str = paddle::string::Sprintf(TENSOR_S_TEMPLATE, egr::EagerUtils::TensorStr(s));
    output_str += output_s_str; 
    const char* TENSOR_VH_TEMPLATE = " \n( vh , [%s]), ";
    std::string output_vh_str = paddle::string::Sprintf(TENSOR_VH_TEMPLATE, egr::EagerUtils::TensorStr(vh));
    output_str += output_vh_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{u, s, vh};
}


paddle::experimental::Tensor take_along_axis_ad_func(const paddle::experimental::Tensor& arr, const paddle::experimental::Tensor& indices, int axis) {
  VLOG(3) << "Running AD API: " << "take_along_axis";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("take_along_axis dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("take_along_axis");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {arr},{indices} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_arr = egr::EagerAmpAutoCast("arr", arr, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return take_along_axis_ad_func(new_arr, new_indices, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {arr},{indices} };
    
    auto op_name = phi::TransToFluidOpName("take_along_axis");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_arr = transformer->TransInTensor("arr", arr);
    auto new_indices = transformer->TransInTensor("indices", indices);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = take_along_axis_ad_func(new_arr, new_indices, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* arr_autograd_meta = egr::EagerUtils::nullable_autograd_meta(arr);

  VLOG(5) << "Running C++ API: " << "take_along_axis";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::take_along_axis(arr, indices, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("take_along_axis", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,arr_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("take_along_axis node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TakeAlongAxisGradNode>(new TakeAlongAxisGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperarr(arr);
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(arr, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: take_along_axis";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tan_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tan";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tan dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tan");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tan_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tan");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tan_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tan";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::tan(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tan", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tan node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanGradNode>(new TanGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: tan";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tanh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tanh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tanh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tanh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tanh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tanh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tanh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tanh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::tanh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhGradNode>(new TanhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: tanh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& tanh__ad_func(paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tanh_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tanh_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for tanh__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tanh_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = tanh__ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tanh_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::tanh_(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhGradNode>(new TanhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: tanh_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tanh_shrink_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tanh_shrink";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tanh_shrink dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tanh_shrink");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tanh_shrink_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tanh_shrink");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tanh_shrink_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tanh_shrink";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::tanh_shrink(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_shrink", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tanh_shrink node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhShrinkGradNode>(new TanhShrinkGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: tanh_shrink";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor thresholded_relu_ad_func(const paddle::experimental::Tensor& x, float threshold) {
  VLOG(3) << "Running AD API: " << "thresholded_relu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("thresholded_relu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("thresholded_relu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return thresholded_relu_ad_func(new_x, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("thresholded_relu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = thresholded_relu_ad_func(new_x, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "thresholded_relu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::thresholded_relu(x, threshold);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("thresholded_relu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("thresholded_relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ThresholdedReluGradNode>(new ThresholdedReluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(threshold);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: thresholded_relu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> topk_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar k, int axis, bool largest, bool sorted) {
  VLOG(3) << "Running AD API: " << "topk";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("topk dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("topk");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return topk_ad_func(new_x, k, axis, largest, sorted);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("topk");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = topk_ad_func(new_x, k, axis, largest, sorted);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& indices = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&indices);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "topk";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::topk(x, k, axis, largest, sorted);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("topk", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& indices = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* indices_autograd_meta = egr::EagerUtils::autograd_meta(&indices);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("topk node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,indices_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TopkGradNode>(new TopkGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributek(k);
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributelargest(largest);
    grad_node->SetAttributesorted(sorted);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(indices_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (indices_autograd_meta) {
      egr::EagerUtils::SetHistory(indices_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(indices, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(indices);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperindices(indices);
  }


  VLOG(4) << "Finish AD API: topk";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string output_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    output_str += output_indices_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices};
}


paddle::experimental::Tensor trace_ad_func(const paddle::experimental::Tensor& x, int offset, int axis1, int axis2) {
  VLOG(3) << "Running AD API: " << "trace";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("trace dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("trace");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return trace_ad_func(new_x, offset, axis1, axis2);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("trace");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &axis1, &axis2);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = trace_ad_func(new_x, offset, axis1, axis2);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "trace";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::trace(x, offset, axis1, axis2);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trace", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("trace node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TraceGradNode>(new TraceGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributeaxis1(axis1);
    grad_node->SetAttributeaxis2(axis2);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: trace";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor trunc_ad_func(const paddle::experimental::Tensor& input) {
  VLOG(3) << "Running AD API: " << "trunc";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("trunc dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("trunc");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return trunc_ad_func(new_input);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("trunc");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = trunc_ad_func(new_input);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "trunc";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::trunc(input);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trunc", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("trunc node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TruncGradNode>(new TruncGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: trunc";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor unfold_ad_func(const paddle::experimental::Tensor& x, std::vector<int> kernel_sizes, std::vector<int> strides, std::vector<int> paddings, std::vector<int> dilations) {
  VLOG(3) << "Running AD API: " << "unfold";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unfold dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unfold");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unfold_ad_func(new_x, kernel_sizes, strides, paddings, dilations);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unfold");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = unfold_ad_func(new_x, kernel_sizes, strides, paddings, dilations);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unfold";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unfold(x, kernel_sizes, strides, paddings, dilations);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unfold", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unfold node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnfoldGradNode>(new UnfoldGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_sizes(kernel_sizes);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributedilations(dilations);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: unfold";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor unsqueeze_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axes) {
  VLOG(3) << "Running AD API: " << "unsqueeze";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unsqueeze dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unsqueeze");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unsqueeze_ad_func(new_x, axes);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unsqueeze");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray>(op_name, tensors_vector, &axes);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = unsqueeze_ad_func(new_x, axes);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unsqueeze";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unsqueeze_intermediate(x, axes);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unsqueeze_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unsqueeze node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnsqueezeGradNode>(new UnsqueezeGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: unsqueeze";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& unsqueeze__ad_func(paddle::experimental::Tensor& x, paddle::experimental::IntArray axes) {
  VLOG(3) << "Running AD API: " << "unsqueeze_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unsqueeze_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for unsqueeze__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unsqueeze_");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray>(op_name, tensors_vector, &axes);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = unsqueeze__ad_func(new_x, axes);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unsqueeze_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unsqueeze_intermediate_(x, axes);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unsqueeze_intermediate_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unsqueeze node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnsqueezeGradNode>(new UnsqueezeGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: unsqueeze_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::vector<paddle::experimental::Tensor> unstack_ad_func(const paddle::experimental::Tensor& x, int axis, int num) {
  VLOG(3) << "Running AD API: " << "unstack";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unstack dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unstack");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unstack_ad_func(new_x, axis, num);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unstack");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> out = unstack_ad_func(new_x, axis, num);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unstack";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unstack(x, axis, num);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unstack", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> out_autograd_meta_vec = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*>* out_autograd_meta = &out_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unstack node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnstackGradNode>(new UnstackGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: unstack";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> viterbi_decode_ad_func(const paddle::experimental::Tensor& potentials, const paddle::experimental::Tensor& transition_params, const paddle::experimental::Tensor& lengths, bool include_bos_eos_tag) {
  VLOG(3) << "Running AD API: " << "viterbi_decode";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("viterbi_decode dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("viterbi_decode");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {potentials},{transition_params},{lengths} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_potentials = egr::EagerAmpAutoCast("potentials", potentials, amp_dst_dtype, op_name);
    auto new_transition_params = egr::EagerAmpAutoCast("transition_params", transition_params, amp_dst_dtype, op_name);
    auto new_lengths = egr::EagerAmpAutoCast("lengths", lengths, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return viterbi_decode_ad_func(new_potentials, new_transition_params, new_lengths, include_bos_eos_tag);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {potentials},{transition_params},{lengths} };
    
    auto op_name = phi::TransToFluidOpName("viterbi_decode");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_potentials = transformer->TransInTensor("potentials", potentials);
    auto new_transition_params = transformer->TransInTensor("transition_params", transition_params);
    auto new_lengths = transformer->TransInTensor("lengths", lengths);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = viterbi_decode_ad_func(new_potentials, new_transition_params, new_lengths, include_bos_eos_tag);

        auto& scores = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&scores);
        auto& path = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&path);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{scores, path};
  }

  VLOG(5) << "Running C++ API: " << "viterbi_decode";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_POTENTIALS_TEMPLATE = " \n( potentials , [%s]), ";
    std::string input_potentials_str = paddle::string::Sprintf(TENSOR_POTENTIALS_TEMPLATE, egr::EagerUtils::TensorStr(potentials));
    input_str += input_potentials_str; 
    const char* TENSOR_TRANSITION_PARAMS_TEMPLATE = " \n( transition_params , [%s]), ";
    std::string input_transition_params_str = paddle::string::Sprintf(TENSOR_TRANSITION_PARAMS_TEMPLATE, egr::EagerUtils::TensorStr(transition_params));
    input_str += input_transition_params_str; 
    const char* TENSOR_LENGTHS_TEMPLATE = " \n( lengths , [%s]), ";
    std::string input_lengths_str = paddle::string::Sprintf(TENSOR_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(lengths));
    input_str += input_lengths_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::viterbi_decode(potentials, transition_params, lengths, include_bos_eos_tag);
  // Get Outputs
  auto& scores = std::get<0>(api_result);
  auto& path = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: viterbi_decode";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_POTENTIALS_TEMPLATE = " \n( potentials , [%s]), ";
    std::string input_potentials_str = paddle::string::Sprintf(TENSOR_POTENTIALS_TEMPLATE, egr::EagerUtils::TensorStr(potentials));
    input_str += input_potentials_str; 
    const char* TENSOR_TRANSITION_PARAMS_TEMPLATE = " \n( transition_params , [%s]), ";
    std::string input_transition_params_str = paddle::string::Sprintf(TENSOR_TRANSITION_PARAMS_TEMPLATE, egr::EagerUtils::TensorStr(transition_params));
    input_str += input_transition_params_str; 
    const char* TENSOR_LENGTHS_TEMPLATE = " \n( lengths , [%s]), ";
    std::string input_lengths_str = paddle::string::Sprintf(TENSOR_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(lengths));
    input_str += input_lengths_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string output_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    output_str += output_scores_str; 
    const char* TENSOR_PATH_TEMPLATE = " \n( path , [%s]), ";
    std::string output_path_str = paddle::string::Sprintf(TENSOR_PATH_TEMPLATE, egr::EagerUtils::TensorStr(path));
    output_str += output_path_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{scores, path};
}


paddle::experimental::Tensor warprnnt_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label, const paddle::experimental::Tensor& input_lengths, const paddle::experimental::Tensor& label_lengths, int blank, float fastemit_lambda) {
  VLOG(3) << "Running AD API: " << "warprnnt";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("warprnnt dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("warprnnt");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label},{input_lengths},{label_lengths} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_input_lengths = egr::EagerAmpAutoCast("input_lengths", input_lengths, amp_dst_dtype, op_name);
    auto new_label_lengths = egr::EagerAmpAutoCast("label_lengths", label_lengths, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return warprnnt_ad_func(new_input, new_label, new_input_lengths, new_label_lengths, blank, fastemit_lambda);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label},{input_lengths},{label_lengths} };
    
    auto op_name = phi::TransToFluidOpName("warprnnt");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);
    auto new_input_lengths = transformer->TransInTensor("input_lengths", input_lengths);
    auto new_label_lengths = transformer->TransInTensor("label_lengths", label_lengths);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor loss = warprnnt_ad_func(new_input, new_label, new_input_lengths, new_label_lengths, blank, fastemit_lambda);

    transformer -> SetOutTensorLayout(&loss);

    // Returns
    return loss;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "warprnnt";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_INPUT_LENGTHS_TEMPLATE = " \n( input_lengths , [%s]), ";
    std::string input_input_lengths_str = paddle::string::Sprintf(TENSOR_INPUT_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(input_lengths));
    input_str += input_input_lengths_str; 
    const char* TENSOR_LABEL_LENGTHS_TEMPLATE = " \n( label_lengths , [%s]), ";
    std::string input_label_lengths_str = paddle::string::Sprintf(TENSOR_LABEL_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(label_lengths));
    input_str += input_label_lengths_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::warprnnt_intermediate(input, label, input_lengths, label_lengths, blank, fastemit_lambda);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("warprnnt_intermediate", api_result); }

  // Get Outputs
  auto& loss = std::get<0>(api_result);
  auto& warprnntgrad = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* loss_autograd_meta = egr::EagerUtils::autograd_meta(&loss);
  egr::AutogradMeta* warprnntgrad_autograd_meta = egr::EagerUtils::autograd_meta(&warprnntgrad);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("warprnnt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,loss_autograd_meta,warprnntgrad_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<WarprnntGradNode>(new WarprnntGradNode(2, 4));
    // SetAttributes if needed
    grad_node->SetAttributeblank(blank);
    grad_node->SetAttributefastemit_lambda(fastemit_lambda);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperinput_lengths(input_lengths);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (loss_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(loss_autograd_meta, 0);
    }
    if (warprnntgrad_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(warprnntgrad_autograd_meta, 1);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetHistory(loss_autograd_meta, grad_node);
    }
    if (warprnntgrad_autograd_meta) {
      egr::EagerUtils::SetHistory(warprnntgrad_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(loss, 0);
    grad_node->SetGradInMeta(warprnntgrad, 1);
    egr::EagerUtils::CheckAndRetainGrad(loss);
    egr::EagerUtils::CheckAndRetainGrad(warprnntgrad);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperwarprnntgrad(warprnntgrad);
  }


  VLOG(4) << "Finish AD API: warprnnt";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_INPUT_LENGTHS_TEMPLATE = " \n( input_lengths , [%s]), ";
    std::string input_input_lengths_str = paddle::string::Sprintf(TENSOR_INPUT_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(input_lengths));
    input_str += input_input_lengths_str; 
    const char* TENSOR_LABEL_LENGTHS_TEMPLATE = " \n( label_lengths , [%s]), ";
    std::string input_label_lengths_str = paddle::string::Sprintf(TENSOR_LABEL_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(label_lengths));
    input_str += input_label_lengths_str; 
    const char* TENSOR_LOSS_TEMPLATE = " \n( loss , [%s]), ";
    std::string output_loss_str = paddle::string::Sprintf(TENSOR_LOSS_TEMPLATE, egr::EagerUtils::TensorStr(loss));
    output_str += output_loss_str; 
    const char* TENSOR_WARPRNNTGRAD_TEMPLATE = " \n( warprnntgrad , [%s]), ";
    std::string output_warprnntgrad_str = paddle::string::Sprintf(TENSOR_WARPRNNTGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warprnntgrad));
    output_str += output_warprnntgrad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return loss;
}


paddle::experimental::Tensor where_ad_func(const paddle::experimental::Tensor& condition, const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "where";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("where dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("where");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {condition},{x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_condition = egr::EagerAmpAutoCast("condition", condition, amp_dst_dtype, op_name);
    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return where_ad_func(new_condition, new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {condition},{x},{y} };
    
    auto op_name = phi::TransToFluidOpName("where");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_condition = transformer->TransInTensor("condition", condition);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = where_ad_func(new_condition, new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "where";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::where(condition, x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("where", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("where node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<WhereGradNode>(new WhereGradNode(1, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappercondition(condition);
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 1);
    grad_node->SetGradOutMeta(y, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: where";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}



paddle::experimental::Tensor abs_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "abs";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("abs dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("abs");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return abs_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("abs");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = abs_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "abs";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::abs(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("abs", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("abs node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AbsGradNode>(new AbsGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: abs";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> accuracy_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& indices, const paddle::experimental::Tensor& label) {
  VLOG(3) << "Running AD API: " << "accuracy";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("accuracy dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("accuracy");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{indices},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return accuracy_ad_func(new_x, new_indices, new_label);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{indices},{label} };
    
    auto op_name = phi::TransToFluidOpName("accuracy");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_indices = transformer->TransInTensor("indices", indices);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = accuracy_ad_func(new_x, new_indices, new_label);

        auto& accuracy = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&accuracy);
        auto& correct = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&correct);
        auto& total = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&total);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{accuracy, correct, total};
  }

  VLOG(5) << "Running C++ API: " << "accuracy";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::accuracy(x, indices, label);
  // Get Outputs
  auto& accuracy = std::get<0>(api_result);
  auto& correct = std::get<1>(api_result);
  auto& total = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: accuracy";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_ACCURACY_TEMPLATE = " \n( accuracy , [%s]), ";
    std::string output_accuracy_str = paddle::string::Sprintf(TENSOR_ACCURACY_TEMPLATE, egr::EagerUtils::TensorStr(accuracy));
    output_str += output_accuracy_str; 
    const char* TENSOR_CORRECT_TEMPLATE = " \n( correct , [%s]), ";
    std::string output_correct_str = paddle::string::Sprintf(TENSOR_CORRECT_TEMPLATE, egr::EagerUtils::TensorStr(correct));
    output_str += output_correct_str; 
    const char* TENSOR_TOTAL_TEMPLATE = " \n( total , [%s]), ";
    std::string output_total_str = paddle::string::Sprintf(TENSOR_TOTAL_TEMPLATE, egr::EagerUtils::TensorStr(total));
    output_str += output_total_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{accuracy, correct, total};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> adadelta__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, paddle::experimental::Tensor& avg_squared_grad, paddle::experimental::Tensor& avg_squared_update, float rho, float epsilon) {
  VLOG(3) << "Running AD API: " << "adadelta_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("adadelta_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for adadelta__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{avg_squared_grad},{avg_squared_update} };
    
    auto op_name = phi::TransToFluidOpName("adadelta_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_avg_squared_grad = transformer->TransInTensor("avg_squared_grad", avg_squared_grad);
    auto new_avg_squared_update = transformer->TransInTensor("avg_squared_update", avg_squared_update);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> api_result = adadelta__ad_func(new_param, new_grad, new_avg_squared_grad, new_avg_squared_update, rho, epsilon);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment_out);
        auto& inf_norm_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&inf_norm_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, moment_out, inf_norm_out};
  }

  VLOG(5) << "Running C++ API: " << "adadelta_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_AVG_SQUARED_GRAD_TEMPLATE = " \n( avg_squared_grad , [%s]), ";
    std::string input_avg_squared_grad_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_grad));
    input_str += input_avg_squared_grad_str; 
    const char* TENSOR_AVG_SQUARED_UPDATE_TEMPLATE = " \n( avg_squared_update , [%s]), ";
    std::string input_avg_squared_update_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_update));
    input_str += input_avg_squared_update_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::adadelta_(param, grad, avg_squared_grad, avg_squared_update, rho, epsilon);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment_out = std::get<1>(api_result);
  auto& inf_norm_out = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: adadelta_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_AVG_SQUARED_GRAD_TEMPLATE = " \n( avg_squared_grad , [%s]), ";
    std::string input_avg_squared_grad_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_grad));
    input_str += input_avg_squared_grad_str; 
    const char* TENSOR_AVG_SQUARED_UPDATE_TEMPLATE = " \n( avg_squared_update , [%s]), ";
    std::string input_avg_squared_update_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_update));
    input_str += input_avg_squared_update_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT_OUT_TEMPLATE = " \n( moment_out , [%s]), ";
    std::string output_moment_out_str = paddle::string::Sprintf(TENSOR_MOMENT_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment_out));
    output_str += output_moment_out_str; 
    const char* TENSOR_INF_NORM_OUT_TEMPLATE = " \n( inf_norm_out , [%s]), ";
    std::string output_inf_norm_out_str = paddle::string::Sprintf(TENSOR_INF_NORM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(inf_norm_out));
    output_str += output_inf_norm_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, moment_out, inf_norm_out};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&> adagrad__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, paddle::experimental::Tensor& moment, const paddle::experimental::Tensor& learning_rate, float epsilon) {
  VLOG(3) << "Running AD API: " << "adagrad_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("adagrad_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for adagrad__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{moment},{learning_rate} };
    
    auto op_name = phi::TransToFluidOpName("adagrad_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_moment = transformer->TransInTensor("moment", moment);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&> api_result = adagrad__ad_func(new_param, new_grad, new_moment, new_learning_rate, epsilon);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, moment_out};
  }

  VLOG(5) << "Running C++ API: " << "adagrad_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::adagrad_(param, grad, moment, learning_rate, epsilon);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment_out = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: adagrad_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT_OUT_TEMPLATE = " \n( moment_out , [%s]), ";
    std::string output_moment_out_str = paddle::string::Sprintf(TENSOR_MOMENT_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment_out));
    output_str += output_moment_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, moment_out};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> adam__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, const paddle::experimental::Tensor& learning_rate, paddle::experimental::Tensor& moment1, paddle::experimental::Tensor& moment2, paddle::experimental::Tensor& beta1_pow, paddle::experimental::Tensor& beta2_pow, paddle::optional<paddle::experimental::Tensor>& master_param, const paddle::optional<paddle::experimental::Tensor>& skip_update, paddle::experimental::Scalar beta1, paddle::experimental::Scalar beta2, paddle::experimental::Scalar epsilon, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(3) << "Running AD API: " << "adam_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("adam_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for adam__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{learning_rate},{moment1},{moment2},{beta1_pow},{beta2_pow} };
    if (master_param) tensors_vector.push_back({ *master_param });
    if (skip_update) tensors_vector.push_back({ *skip_update });

    auto op_name = phi::TransToFluidOpName("adam_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
    auto new_moment1 = transformer->TransInTensor("moment1", moment1);
    auto new_moment2 = transformer->TransInTensor("moment2", moment2);
    auto new_beta1_pow = transformer->TransInTensor("beta1_pow", beta1_pow);
    auto new_beta2_pow = transformer->TransInTensor("beta2_pow", beta2_pow);
auto new_master_param = transformer->TransInTensor("master_param", master_param);
    auto new_skip_update = transformer->TransInTensor("skip_update", skip_update);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = adam__ad_func(new_param, new_grad, new_learning_rate, new_moment1, new_moment2, new_beta1_pow, new_beta2_pow, new_master_param, new_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment1_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment1_out);
        auto& moment2_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&moment2_out);
        auto& beta1_pow_out = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&beta1_pow_out);
        auto& beta2_pow_out = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&beta2_pow_out);
        auto& master_param_outs = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&master_param_outs);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
  }

  VLOG(5) << "Running C++ API: " << "adam_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::adam_(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment1_out = std::get<1>(api_result);
  auto& moment2_out = std::get<2>(api_result);
  auto& beta1_pow_out = std::get<3>(api_result);
  auto& beta2_pow_out = std::get<4>(api_result);
  auto& master_param_outs = std::get<5>(api_result);

  VLOG(4) << "Finish AD API: adam_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT1_OUT_TEMPLATE = " \n( moment1_out , [%s]), ";
    std::string output_moment1_out_str = paddle::string::Sprintf(TENSOR_MOMENT1_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment1_out));
    output_str += output_moment1_out_str; 
    const char* TENSOR_MOMENT2_OUT_TEMPLATE = " \n( moment2_out , [%s]), ";
    std::string output_moment2_out_str = paddle::string::Sprintf(TENSOR_MOMENT2_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment2_out));
    output_str += output_moment2_out_str; 
    const char* TENSOR_BETA1_POW_OUT_TEMPLATE = " \n( beta1_pow_out , [%s]), ";
    std::string output_beta1_pow_out_str = paddle::string::Sprintf(TENSOR_BETA1_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow_out));
    output_str += output_beta1_pow_out_str; 
    const char* TENSOR_BETA2_POW_OUT_TEMPLATE = " \n( beta2_pow_out , [%s]), ";
    std::string output_beta2_pow_out_str = paddle::string::Sprintf(TENSOR_BETA2_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow_out));
    output_str += output_beta2_pow_out_str; 
    const char* TENSOR_MASTER_PARAM_OUTS_TEMPLATE = " \n( master_param_outs , [%s]), ";
    std::string output_master_param_outs_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUTS_TEMPLATE, egr::EagerUtils::TensorStr(master_param_outs));
    output_str += output_master_param_outs_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> adamax__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, const paddle::experimental::Tensor& learning_rate, paddle::experimental::Tensor& moment, paddle::experimental::Tensor& inf_norm, const paddle::experimental::Tensor& beta1_pow, float beta1, float beta2, float epsilon) {
  VLOG(3) << "Running AD API: " << "adamax_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("adamax_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for adamax__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{learning_rate},{moment},{inf_norm},{beta1_pow} };
    
    auto op_name = phi::TransToFluidOpName("adamax_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
    auto new_moment = transformer->TransInTensor("moment", moment);
    auto new_inf_norm = transformer->TransInTensor("inf_norm", inf_norm);
    auto new_beta1_pow = transformer->TransInTensor("beta1_pow", beta1_pow);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> api_result = adamax__ad_func(new_param, new_grad, new_learning_rate, new_moment, new_inf_norm, new_beta1_pow, beta1, beta2, epsilon);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& avg_squared_grad_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&avg_squared_grad_out);
        auto& avg_squared_update_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&avg_squared_update_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, avg_squared_grad_out, avg_squared_update_out};
  }

  VLOG(5) << "Running C++ API: " << "adamax_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_INF_NORM_TEMPLATE = " \n( inf_norm , [%s]), ";
    std::string input_inf_norm_str = paddle::string::Sprintf(TENSOR_INF_NORM_TEMPLATE, egr::EagerUtils::TensorStr(inf_norm));
    input_str += input_inf_norm_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::adamax_(param, grad, learning_rate, moment, inf_norm, beta1_pow, beta1, beta2, epsilon);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& avg_squared_grad_out = std::get<1>(api_result);
  auto& avg_squared_update_out = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: adamax_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_INF_NORM_TEMPLATE = " \n( inf_norm , [%s]), ";
    std::string input_inf_norm_str = paddle::string::Sprintf(TENSOR_INF_NORM_TEMPLATE, egr::EagerUtils::TensorStr(inf_norm));
    input_str += input_inf_norm_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_AVG_SQUARED_GRAD_OUT_TEMPLATE = " \n( avg_squared_grad_out , [%s]), ";
    std::string output_avg_squared_grad_out_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_grad_out));
    output_str += output_avg_squared_grad_out_str; 
    const char* TENSOR_AVG_SQUARED_UPDATE_OUT_TEMPLATE = " \n( avg_squared_update_out , [%s]), ";
    std::string output_avg_squared_update_out_str = paddle::string::Sprintf(TENSOR_AVG_SQUARED_UPDATE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(avg_squared_update_out));
    output_str += output_avg_squared_update_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{param_out, avg_squared_grad_out, avg_squared_update_out};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> adamw__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, const paddle::experimental::Tensor& learning_rate, paddle::experimental::Tensor& moment1, paddle::experimental::Tensor& moment2, paddle::experimental::Tensor& beta1_pow, paddle::experimental::Tensor& beta2_pow, paddle::optional<paddle::experimental::Tensor>& master_param, const paddle::optional<paddle::experimental::Tensor>& skip_update, paddle::experimental::Scalar beta1, paddle::experimental::Scalar beta2, paddle::experimental::Scalar epsilon, float lr_ratio, float coeff, bool with_decay, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(3) << "Running AD API: " << "adamw_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("adamw_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for adamw__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{learning_rate},{moment1},{moment2},{beta1_pow},{beta2_pow} };
    if (master_param) tensors_vector.push_back({ *master_param });
    if (skip_update) tensors_vector.push_back({ *skip_update });

    auto op_name = phi::TransToFluidOpName("adamw_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
    auto new_moment1 = transformer->TransInTensor("moment1", moment1);
    auto new_moment2 = transformer->TransInTensor("moment2", moment2);
    auto new_beta1_pow = transformer->TransInTensor("beta1_pow", beta1_pow);
    auto new_beta2_pow = transformer->TransInTensor("beta2_pow", beta2_pow);
auto new_master_param = transformer->TransInTensor("master_param", master_param);
    auto new_skip_update = transformer->TransInTensor("skip_update", skip_update);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = adamw__ad_func(new_param, new_grad, new_learning_rate, new_moment1, new_moment2, new_beta1_pow, new_beta2_pow, new_master_param, new_skip_update, beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment1_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment1_out);
        auto& moment2_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&moment2_out);
        auto& beta1_pow_out = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&beta1_pow_out);
        auto& beta2_pow_out = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&beta2_pow_out);
        auto& master_param_outs = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&master_param_outs);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
  }

  VLOG(5) << "Running C++ API: " << "adamw_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::adamw_(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update, beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment1_out = std::get<1>(api_result);
  auto& moment2_out = std::get<2>(api_result);
  auto& beta1_pow_out = std::get<3>(api_result);
  auto& beta2_pow_out = std::get<4>(api_result);
  auto& master_param_outs = std::get<5>(api_result);

  VLOG(4) << "Finish AD API: adamw_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT1_OUT_TEMPLATE = " \n( moment1_out , [%s]), ";
    std::string output_moment1_out_str = paddle::string::Sprintf(TENSOR_MOMENT1_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment1_out));
    output_str += output_moment1_out_str; 
    const char* TENSOR_MOMENT2_OUT_TEMPLATE = " \n( moment2_out , [%s]), ";
    std::string output_moment2_out_str = paddle::string::Sprintf(TENSOR_MOMENT2_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment2_out));
    output_str += output_moment2_out_str; 
    const char* TENSOR_BETA1_POW_OUT_TEMPLATE = " \n( beta1_pow_out , [%s]), ";
    std::string output_beta1_pow_out_str = paddle::string::Sprintf(TENSOR_BETA1_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow_out));
    output_str += output_beta1_pow_out_str; 
    const char* TENSOR_BETA2_POW_OUT_TEMPLATE = " \n( beta2_pow_out , [%s]), ";
    std::string output_beta2_pow_out_str = paddle::string::Sprintf(TENSOR_BETA2_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow_out));
    output_str += output_beta2_pow_out_str; 
    const char* TENSOR_MASTER_PARAM_OUTS_TEMPLATE = " \n( master_param_outs , [%s]), ";
    std::string output_master_param_outs_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUTS_TEMPLATE, egr::EagerUtils::TensorStr(master_param_outs));
    output_str += output_master_param_outs_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
}


paddle::experimental::Tensor add_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "add";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("add dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("add");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return add_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("add");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = add_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "add";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::add(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AddGradNode>(new AddGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: add";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& add__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "add_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("add_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for add__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("add_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = add__ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "add_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::add_(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AddGradNode>(new AddGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: add_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor affine_grid_ad_func(const paddle::experimental::Tensor& input, paddle::experimental::IntArray outputShape, bool align_corners) {
  VLOG(3) << "Running AD API: " << "affine_grid";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("affine_grid dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("affine_grid");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return affine_grid_ad_func(new_input, outputShape, align_corners);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("affine_grid");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = affine_grid_ad_func(new_input, outputShape, align_corners);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "affine_grid";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::affine_grid(input, outputShape, align_corners);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("affine_grid", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("affine_grid node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AffineGridGradNode>(new AffineGridGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeoutputShape(outputShape);
    grad_node->SetAttributealign_corners(align_corners);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: affine_grid";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor all_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "all";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("all dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("all");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return all_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("all");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = all_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "all";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::all(x, axis, keepdim);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: all";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor allclose_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, paddle::experimental::Scalar rtol, paddle::experimental::Scalar atol, bool equal_nan) {
  VLOG(3) << "Running AD API: " << "allclose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("allclose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("allclose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return allclose_ad_func(new_x, new_y, rtol, atol, equal_nan);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("allclose");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = allclose_ad_func(new_x, new_y, rtol, atol, equal_nan);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "allclose";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::allclose(x, y, rtol, atol, equal_nan);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: allclose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor amax_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "amax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("amax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("amax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return amax_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("amax");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = amax_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "amax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::amax(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("amax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("amax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AmaxGradNode>(new AmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: amax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor amin_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "amin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("amin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("amin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return amin_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("amin");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = amin_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "amin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::amin(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("amin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("amin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AminGradNode>(new AminGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: amin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor any_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "any";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("any dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("any");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return any_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("any");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = any_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "any";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::any(x, axis, keepdim);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: any";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor arange_ad_func(const paddle::experimental::Tensor& start, const paddle::experimental::Tensor& end, const paddle::experimental::Tensor& step, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "arange";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("arange dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("arange");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {start},{end},{step} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_start = egr::EagerAmpAutoCast("start", start, amp_dst_dtype, op_name);
    auto new_end = egr::EagerAmpAutoCast("end", end, amp_dst_dtype, op_name);
    auto new_step = egr::EagerAmpAutoCast("step", step, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return arange_ad_func(new_start, new_end, new_step, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {start},{end},{step} };
    
    auto op_name = phi::TransToFluidOpName("arange");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_start = transformer->TransInTensor("start", start);
    auto new_end = transformer->TransInTensor("end", end);
    auto new_step = transformer->TransInTensor("step", step);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = arange_ad_func(new_start, new_end, new_step, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "arange";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_START_TEMPLATE = " \n( start , [%s]), ";
    std::string input_start_str = paddle::string::Sprintf(TENSOR_START_TEMPLATE, egr::EagerUtils::TensorStr(start));
    input_str += input_start_str; 
    const char* TENSOR_END_TEMPLATE = " \n( end , [%s]), ";
    std::string input_end_str = paddle::string::Sprintf(TENSOR_END_TEMPLATE, egr::EagerUtils::TensorStr(end));
    input_str += input_end_str; 
    const char* TENSOR_STEP_TEMPLATE = " \n( step , [%s]), ";
    std::string input_step_str = paddle::string::Sprintf(TENSOR_STEP_TEMPLATE, egr::EagerUtils::TensorStr(step));
    input_str += input_step_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::arange(start, end, step, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: arange";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_START_TEMPLATE = " \n( start , [%s]), ";
    std::string input_start_str = paddle::string::Sprintf(TENSOR_START_TEMPLATE, egr::EagerUtils::TensorStr(start));
    input_str += input_start_str; 
    const char* TENSOR_END_TEMPLATE = " \n( end , [%s]), ";
    std::string input_end_str = paddle::string::Sprintf(TENSOR_END_TEMPLATE, egr::EagerUtils::TensorStr(end));
    input_str += input_end_str; 
    const char* TENSOR_STEP_TEMPLATE = " \n( step , [%s]), ";
    std::string input_step_str = paddle::string::Sprintf(TENSOR_STEP_TEMPLATE, egr::EagerUtils::TensorStr(step));
    input_str += input_step_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor argmax_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar axis, bool keepdims, bool flatten, int dtype) {
  VLOG(3) << "Running AD API: " << "argmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("argmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("argmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return argmax_ad_func(new_x, axis, keepdims, flatten, dtype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("argmax");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar, bool>(op_name, tensors_vector, &axis, &keepdims);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = argmax_ad_func(new_x, axis, keepdims, flatten, dtype);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "argmax";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::argmax(x, axis, keepdims, flatten, dtype);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: argmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor argmin_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar axis, bool keepdims, bool flatten, int dtype) {
  VLOG(3) << "Running AD API: " << "argmin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("argmin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("argmin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return argmin_ad_func(new_x, axis, keepdims, flatten, dtype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("argmin");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar, bool>(op_name, tensors_vector, &axis, &keepdims);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = argmin_ad_func(new_x, axis, keepdims, flatten, dtype);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "argmin";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::argmin(x, axis, keepdims, flatten, dtype);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: argmin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor assign_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "assign";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("assign dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("assign");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return assign_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("assign");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = assign_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "assign";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::assign(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("assign", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("assign node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AssignGradNode>(new AssignGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: assign";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor& assign_out__ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Tensor& output) {
  VLOG(3) << "Running AD API: " << "assign_out_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("assign_out_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for assign_out__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{output} };
    
    auto op_name = phi::TransToFluidOpName("assign_out_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_output = transformer->TransInTensor("output", output);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = assign_out__ad_func(new_x, new_output);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "assign_out_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::assign_out_(x, output);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("assign_out_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Bump Inplace Version
  output.bump_inplace_version();
  VLOG(3) << "Tensor(" << output.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("assign_out_ node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AssignOutGradNode>(new AssignOutGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: assign_out_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor& assign_value__ad_func(paddle::experimental::Tensor& output, std::vector<int> shape, paddle::experimental::DataType dtype, std::vector<phi::Scalar> values, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "assign_value_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("assign_value_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for assign_value__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {output} };
    
    auto op_name = phi::TransToFluidOpName("assign_value_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_output = transformer->TransInTensor("output", output);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = assign_value__ad_func(new_output, shape, dtype, values, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "assign_value_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto& api_result = paddle::experimental::assign_value_(output, shape, dtype, values, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: assign_value_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> auc_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& label, const paddle::experimental::Tensor& stat_pos, const paddle::experimental::Tensor& stat_neg, const paddle::optional<paddle::experimental::Tensor>& ins_tag_weight, std::string curve, int num_thresholds, int slide_steps) {
  VLOG(3) << "Running AD API: " << "auc";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("auc dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("auc");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{label},{stat_pos},{stat_neg} };
    if (ins_tag_weight) amp_tensors_vector.push_back({ *ins_tag_weight });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_stat_pos = egr::EagerAmpAutoCast("stat_pos", stat_pos, amp_dst_dtype, op_name);
    auto new_stat_neg = egr::EagerAmpAutoCast("stat_neg", stat_neg, amp_dst_dtype, op_name);
    auto new_ins_tag_weight = egr::EagerAmpAutoCast("ins_tag_weight", ins_tag_weight, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return auc_ad_func(new_x, new_label, new_stat_pos, new_stat_neg, new_ins_tag_weight, curve, num_thresholds, slide_steps);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{label},{stat_pos},{stat_neg} };
    if (ins_tag_weight) tensors_vector.push_back({ *ins_tag_weight });

    auto op_name = phi::TransToFluidOpName("auc");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_label = transformer->TransInTensor("label", label);
    auto new_stat_pos = transformer->TransInTensor("stat_pos", stat_pos);
    auto new_stat_neg = transformer->TransInTensor("stat_neg", stat_neg);
auto new_ins_tag_weight = transformer->TransInTensor("ins_tag_weight", ins_tag_weight);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = auc_ad_func(new_x, new_label, new_stat_pos, new_stat_neg, new_ins_tag_weight, curve, num_thresholds, slide_steps);

        auto& auc = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&auc);
        auto& stat_pos_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&stat_pos_out);
        auto& stat_neg_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&stat_neg_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{auc, stat_pos_out, stat_neg_out};
  }

  VLOG(5) << "Running C++ API: " << "auc";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_STAT_POS_TEMPLATE = " \n( stat_pos , [%s]), ";
    std::string input_stat_pos_str = paddle::string::Sprintf(TENSOR_STAT_POS_TEMPLATE, egr::EagerUtils::TensorStr(stat_pos));
    input_str += input_stat_pos_str; 
    const char* TENSOR_STAT_NEG_TEMPLATE = " \n( stat_neg , [%s]), ";
    std::string input_stat_neg_str = paddle::string::Sprintf(TENSOR_STAT_NEG_TEMPLATE, egr::EagerUtils::TensorStr(stat_neg));
    input_str += input_stat_neg_str; 
    const char* TENSOR_INS_TAG_WEIGHT_TEMPLATE = " \n( ins_tag_weight , [%s]), ";
    std::string input_ins_tag_weight_str = paddle::string::Sprintf(TENSOR_INS_TAG_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(ins_tag_weight));
    input_str += input_ins_tag_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::auc(x, label, stat_pos, stat_neg, ins_tag_weight, curve, num_thresholds, slide_steps);
  // Get Outputs
  auto& auc = std::get<0>(api_result);
  auto& stat_pos_out = std::get<1>(api_result);
  auto& stat_neg_out = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: auc";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_STAT_POS_TEMPLATE = " \n( stat_pos , [%s]), ";
    std::string input_stat_pos_str = paddle::string::Sprintf(TENSOR_STAT_POS_TEMPLATE, egr::EagerUtils::TensorStr(stat_pos));
    input_str += input_stat_pos_str; 
    const char* TENSOR_STAT_NEG_TEMPLATE = " \n( stat_neg , [%s]), ";
    std::string input_stat_neg_str = paddle::string::Sprintf(TENSOR_STAT_NEG_TEMPLATE, egr::EagerUtils::TensorStr(stat_neg));
    input_str += input_stat_neg_str; 
    const char* TENSOR_INS_TAG_WEIGHT_TEMPLATE = " \n( ins_tag_weight , [%s]), ";
    std::string input_ins_tag_weight_str = paddle::string::Sprintf(TENSOR_INS_TAG_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(ins_tag_weight));
    input_str += input_ins_tag_weight_str; 
    const char* TENSOR_AUC_TEMPLATE = " \n( auc , [%s]), ";
    std::string output_auc_str = paddle::string::Sprintf(TENSOR_AUC_TEMPLATE, egr::EagerUtils::TensorStr(auc));
    output_str += output_auc_str; 
    const char* TENSOR_STAT_POS_OUT_TEMPLATE = " \n( stat_pos_out , [%s]), ";
    std::string output_stat_pos_out_str = paddle::string::Sprintf(TENSOR_STAT_POS_OUT_TEMPLATE, egr::EagerUtils::TensorStr(stat_pos_out));
    output_str += output_stat_pos_out_str; 
    const char* TENSOR_STAT_NEG_OUT_TEMPLATE = " \n( stat_neg_out , [%s]), ";
    std::string output_stat_neg_out_str = paddle::string::Sprintf(TENSOR_STAT_NEG_OUT_TEMPLATE, egr::EagerUtils::TensorStr(stat_neg_out));
    output_str += output_stat_neg_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{auc, stat_pos_out, stat_neg_out};
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> average_accumulates__ad_func(const paddle::experimental::Tensor& param, paddle::experimental::Tensor& in_sum_1, paddle::experimental::Tensor& in_sum_2, paddle::experimental::Tensor& in_sum_3, paddle::experimental::Tensor& in_num_accumulates, paddle::experimental::Tensor& in_old_num_accumulates, paddle::experimental::Tensor& in_num_updates, float average_window, int64_t max_average_window, int64_t min_average_window) {
  VLOG(3) << "Running AD API: " << "average_accumulates_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("average_accumulates_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for average_accumulates__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{in_sum_1},{in_sum_2},{in_sum_3},{in_num_accumulates},{in_old_num_accumulates},{in_num_updates} };
    
    auto op_name = phi::TransToFluidOpName("average_accumulates_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_in_sum_1 = transformer->TransInTensor("in_sum_1", in_sum_1);
    auto new_in_sum_2 = transformer->TransInTensor("in_sum_2", in_sum_2);
    auto new_in_sum_3 = transformer->TransInTensor("in_sum_3", in_sum_3);
    auto new_in_num_accumulates = transformer->TransInTensor("in_num_accumulates", in_num_accumulates);
    auto new_in_old_num_accumulates = transformer->TransInTensor("in_old_num_accumulates", in_old_num_accumulates);
    auto new_in_num_updates = transformer->TransInTensor("in_num_updates", in_num_updates);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> api_result = average_accumulates__ad_func(new_param, new_in_sum_1, new_in_sum_2, new_in_sum_3, new_in_num_accumulates, new_in_old_num_accumulates, new_in_num_updates, average_window, max_average_window, min_average_window);

        auto& out_sum_1 = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out_sum_1);
        auto& out_sum_2 = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&out_sum_2);
        auto& out_sum_3 = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&out_sum_3);
        auto& out_num_accumulates = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&out_num_accumulates);
        auto& out_old_num_accumulates = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&out_old_num_accumulates);
        auto& out_num_updates = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&out_num_updates);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{out_sum_1, out_sum_2, out_sum_3, out_num_accumulates, out_old_num_accumulates, out_num_updates};
  }

  VLOG(5) << "Running C++ API: " << "average_accumulates_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_IN_SUM_1_TEMPLATE = " \n( in_sum_1 , [%s]), ";
    std::string input_in_sum_1_str = paddle::string::Sprintf(TENSOR_IN_SUM_1_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_1));
    input_str += input_in_sum_1_str; 
    const char* TENSOR_IN_SUM_2_TEMPLATE = " \n( in_sum_2 , [%s]), ";
    std::string input_in_sum_2_str = paddle::string::Sprintf(TENSOR_IN_SUM_2_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_2));
    input_str += input_in_sum_2_str; 
    const char* TENSOR_IN_SUM_3_TEMPLATE = " \n( in_sum_3 , [%s]), ";
    std::string input_in_sum_3_str = paddle::string::Sprintf(TENSOR_IN_SUM_3_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_3));
    input_str += input_in_sum_3_str; 
    const char* TENSOR_IN_NUM_ACCUMULATES_TEMPLATE = " \n( in_num_accumulates , [%s]), ";
    std::string input_in_num_accumulates_str = paddle::string::Sprintf(TENSOR_IN_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(in_num_accumulates));
    input_str += input_in_num_accumulates_str; 
    const char* TENSOR_IN_OLD_NUM_ACCUMULATES_TEMPLATE = " \n( in_old_num_accumulates , [%s]), ";
    std::string input_in_old_num_accumulates_str = paddle::string::Sprintf(TENSOR_IN_OLD_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(in_old_num_accumulates));
    input_str += input_in_old_num_accumulates_str; 
    const char* TENSOR_IN_NUM_UPDATES_TEMPLATE = " \n( in_num_updates , [%s]), ";
    std::string input_in_num_updates_str = paddle::string::Sprintf(TENSOR_IN_NUM_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(in_num_updates));
    input_str += input_in_num_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::average_accumulates_(param, in_sum_1, in_sum_2, in_sum_3, in_num_accumulates, in_old_num_accumulates, in_num_updates, average_window, max_average_window, min_average_window);
  // Get Outputs
  auto& out_sum_1 = std::get<0>(api_result);
  auto& out_sum_2 = std::get<1>(api_result);
  auto& out_sum_3 = std::get<2>(api_result);
  auto& out_num_accumulates = std::get<3>(api_result);
  auto& out_old_num_accumulates = std::get<4>(api_result);
  auto& out_num_updates = std::get<5>(api_result);

  VLOG(4) << "Finish AD API: average_accumulates_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_IN_SUM_1_TEMPLATE = " \n( in_sum_1 , [%s]), ";
    std::string input_in_sum_1_str = paddle::string::Sprintf(TENSOR_IN_SUM_1_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_1));
    input_str += input_in_sum_1_str; 
    const char* TENSOR_IN_SUM_2_TEMPLATE = " \n( in_sum_2 , [%s]), ";
    std::string input_in_sum_2_str = paddle::string::Sprintf(TENSOR_IN_SUM_2_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_2));
    input_str += input_in_sum_2_str; 
    const char* TENSOR_IN_SUM_3_TEMPLATE = " \n( in_sum_3 , [%s]), ";
    std::string input_in_sum_3_str = paddle::string::Sprintf(TENSOR_IN_SUM_3_TEMPLATE, egr::EagerUtils::TensorStr(in_sum_3));
    input_str += input_in_sum_3_str; 
    const char* TENSOR_IN_NUM_ACCUMULATES_TEMPLATE = " \n( in_num_accumulates , [%s]), ";
    std::string input_in_num_accumulates_str = paddle::string::Sprintf(TENSOR_IN_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(in_num_accumulates));
    input_str += input_in_num_accumulates_str; 
    const char* TENSOR_IN_OLD_NUM_ACCUMULATES_TEMPLATE = " \n( in_old_num_accumulates , [%s]), ";
    std::string input_in_old_num_accumulates_str = paddle::string::Sprintf(TENSOR_IN_OLD_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(in_old_num_accumulates));
    input_str += input_in_old_num_accumulates_str; 
    const char* TENSOR_IN_NUM_UPDATES_TEMPLATE = " \n( in_num_updates , [%s]), ";
    std::string input_in_num_updates_str = paddle::string::Sprintf(TENSOR_IN_NUM_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(in_num_updates));
    input_str += input_in_num_updates_str; 
    const char* TENSOR_OUT_SUM_1_TEMPLATE = " \n( out_sum_1 , [%s]), ";
    std::string output_out_sum_1_str = paddle::string::Sprintf(TENSOR_OUT_SUM_1_TEMPLATE, egr::EagerUtils::TensorStr(out_sum_1));
    output_str += output_out_sum_1_str; 
    const char* TENSOR_OUT_SUM_2_TEMPLATE = " \n( out_sum_2 , [%s]), ";
    std::string output_out_sum_2_str = paddle::string::Sprintf(TENSOR_OUT_SUM_2_TEMPLATE, egr::EagerUtils::TensorStr(out_sum_2));
    output_str += output_out_sum_2_str; 
    const char* TENSOR_OUT_SUM_3_TEMPLATE = " \n( out_sum_3 , [%s]), ";
    std::string output_out_sum_3_str = paddle::string::Sprintf(TENSOR_OUT_SUM_3_TEMPLATE, egr::EagerUtils::TensorStr(out_sum_3));
    output_str += output_out_sum_3_str; 
    const char* TENSOR_OUT_NUM_ACCUMULATES_TEMPLATE = " \n( out_num_accumulates , [%s]), ";
    std::string output_out_num_accumulates_str = paddle::string::Sprintf(TENSOR_OUT_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(out_num_accumulates));
    output_str += output_out_num_accumulates_str; 
    const char* TENSOR_OUT_OLD_NUM_ACCUMULATES_TEMPLATE = " \n( out_old_num_accumulates , [%s]), ";
    std::string output_out_old_num_accumulates_str = paddle::string::Sprintf(TENSOR_OUT_OLD_NUM_ACCUMULATES_TEMPLATE, egr::EagerUtils::TensorStr(out_old_num_accumulates));
    output_str += output_out_old_num_accumulates_str; 
    const char* TENSOR_OUT_NUM_UPDATES_TEMPLATE = " \n( out_num_updates , [%s]), ";
    std::string output_out_num_updates_str = paddle::string::Sprintf(TENSOR_OUT_NUM_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(out_num_updates));
    output_str += output_out_num_updates_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{out_sum_1, out_sum_2, out_sum_3, out_num_accumulates, out_old_num_accumulates, out_num_updates};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> batch_norm_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& mean, const paddle::experimental::Tensor& variance, const paddle::experimental::Tensor& scale, const paddle::experimental::Tensor& bias, bool is_test, float momentum, float epsilon, std::string data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(3) << "Running AD API: " << "batch_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("batch_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("batch_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{mean},{variance},{scale},{bias} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_mean = egr::EagerAmpAutoCast("mean", mean, amp_dst_dtype, op_name);
    auto new_variance = egr::EagerAmpAutoCast("variance", variance, amp_dst_dtype, op_name);
    auto new_scale = egr::EagerAmpAutoCast("scale", scale, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return batch_norm_ad_func(new_x, new_mean, new_variance, new_scale, new_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{mean},{variance},{scale},{bias} };
    
    auto op_name = phi::TransToFluidOpName("batch_norm");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_mean = transformer->TransInTensor("mean", mean);
    auto new_variance = transformer->TransInTensor("variance", variance);
    auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = batch_norm_ad_func(new_x, new_mean, new_variance, new_scale, new_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mean_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mean_out);
        auto& variance_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&variance_out);
        auto& saved_mean = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&saved_mean);
        auto& saved_variance = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&saved_variance);
        auto& reserve_space = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&reserve_space);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "batch_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::batch_norm(x, mean, variance, scale, bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("batch_norm", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mean_out = std::get<1>(api_result);
  auto& variance_out = std::get<2>(api_result);
  auto& saved_mean = std::get<3>(api_result);
  auto& saved_variance = std::get<4>(api_result);
  auto& reserve_space = std::get<5>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mean_out_autograd_meta = egr::EagerUtils::autograd_meta(&mean_out);
  egr::AutogradMeta* variance_out_autograd_meta = egr::EagerUtils::autograd_meta(&variance_out);
  egr::AutogradMeta* saved_mean_autograd_meta = egr::EagerUtils::autograd_meta(&saved_mean);
  egr::AutogradMeta* saved_variance_autograd_meta = egr::EagerUtils::autograd_meta(&saved_variance);
  egr::AutogradMeta* reserve_space_autograd_meta = egr::EagerUtils::autograd_meta(&reserve_space);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("batch_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mean_out_autograd_meta,variance_out_autograd_meta,saved_mean_autograd_meta,saved_variance_autograd_meta,reserve_space_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BatchNormGradNode>(new BatchNormGradNode(6, 5));
    // SetAttributes if needed
    grad_node->SetAttributemomentum(momentum);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributeuse_global_stats(use_global_stats);
    grad_node->SetAttributetrainable_statistics(trainable_statistics);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperscale(scale);
    grad_node->SetTensorWrapperbias(bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(scale, 3);
    grad_node->SetGradOutMeta(bias, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_out_autograd_meta, 1);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_out_autograd_meta, 2);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_mean_autograd_meta, 3);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_variance_autograd_meta, 4);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(reserve_space_autograd_meta, 5);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_out_autograd_meta, grad_node);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_out_autograd_meta, grad_node);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_mean_autograd_meta, grad_node);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_variance_autograd_meta, grad_node);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetHistory(reserve_space_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mean_out, 1);
    grad_node->SetGradInMeta(variance_out, 2);
    grad_node->SetGradInMeta(saved_mean, 3);
    grad_node->SetGradInMeta(saved_variance, 4);
    grad_node->SetGradInMeta(reserve_space, 5);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mean_out);
    egr::EagerUtils::CheckAndRetainGrad(variance_out);
    egr::EagerUtils::CheckAndRetainGrad(saved_mean);
    egr::EagerUtils::CheckAndRetainGrad(saved_variance);
    egr::EagerUtils::CheckAndRetainGrad(reserve_space);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermean_out(mean_out);
    grad_node->SetTensorWrappervariance_out(variance_out);
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrapperreserve_space(reserve_space);
  }


  VLOG(4) << "Finish AD API: batch_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string output_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    output_str += output_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string output_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    output_str += output_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string output_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    output_str += output_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string output_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    output_str += output_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string output_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    output_str += output_reserve_space_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
}


paddle::experimental::Tensor bce_loss_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label) {
  VLOG(3) << "Running AD API: " << "bce_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bce_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bce_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bce_loss_ad_func(new_input, new_label);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label} };
    
    auto op_name = phi::TransToFluidOpName("bce_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bce_loss_ad_func(new_input, new_label);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "bce_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::bce_loss(input, label);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bce_loss", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("bce_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BceLossGradNode>(new BceLossGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: bce_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bicubic_interp_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& out_size, const paddle::optional<std::vector<paddle::experimental::Tensor>>& size_tensor, const paddle::optional<paddle::experimental::Tensor>& scale_tensor, std::string data_layout, int out_d, int out_h, int out_w, std::vector<float> scale, std::string interp_method, bool align_corners, int align_mode) {
  VLOG(3) << "Running AD API: " << "bicubic_interp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bicubic_interp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bicubic_interp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (out_size) amp_tensors_vector.push_back({ *out_size });
    if (size_tensor) amp_tensors_vector.push_back( *size_tensor );
    if (scale_tensor) amp_tensors_vector.push_back({ *scale_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_out_size = egr::EagerAmpAutoCast("out_size", out_size, amp_dst_dtype, op_name);
    auto new_size_tensor = egr::EagerAmpAutoCasts("size_tensor", size_tensor, amp_dst_dtype, op_name);
    auto new_scale_tensor = egr::EagerAmpAutoCast("scale_tensor", scale_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bicubic_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (out_size) tensors_vector.push_back({ *out_size });
    if (scale_tensor) tensors_vector.push_back({ *scale_tensor });

    auto op_name = phi::TransToFluidOpName("bicubic_interp");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_out_size = transformer->TransInTensor("out_size", out_size);
    auto new_size_tensor = transformer->TransInTensors("size_tensor", size_tensor);
    auto new_scale_tensor = transformer->TransInTensor("scale_tensor", scale_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = bicubic_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "bicubic_interp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::bicubic_interp(x, out_size, size_tensor, scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bicubic_interp", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("bicubic_interp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BicubicInterpGradNode>(new BicubicInterpGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeout_d(out_d);
    grad_node->SetAttributeout_h(out_h);
    grad_node->SetAttributeout_w(out_w);
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributeinterp_method(interp_method);
    grad_node->SetAttributealign_corners(align_corners);
    grad_node->SetAttributealign_mode(align_mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(out_size) grad_node->SetTensorWrapperout_size(*out_size);
    if(size_tensor) grad_node->SetTensorWrappersize_tensor(*size_tensor);
    if(scale_tensor) grad_node->SetTensorWrapperscale_tensor(*scale_tensor);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: bicubic_interp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor bilinear_interp_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& out_size, const paddle::optional<std::vector<paddle::experimental::Tensor>>& size_tensor, const paddle::optional<paddle::experimental::Tensor>& scale_tensor, std::string data_layout, int out_d, int out_h, int out_w, std::vector<float> scale, std::string interp_method, bool align_corners, int align_mode) {
  VLOG(3) << "Running AD API: " << "bilinear_interp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bilinear_interp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bilinear_interp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (out_size) amp_tensors_vector.push_back({ *out_size });
    if (size_tensor) amp_tensors_vector.push_back( *size_tensor );
    if (scale_tensor) amp_tensors_vector.push_back({ *scale_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_out_size = egr::EagerAmpAutoCast("out_size", out_size, amp_dst_dtype, op_name);
    auto new_size_tensor = egr::EagerAmpAutoCasts("size_tensor", size_tensor, amp_dst_dtype, op_name);
    auto new_scale_tensor = egr::EagerAmpAutoCast("scale_tensor", scale_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bilinear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (out_size) tensors_vector.push_back({ *out_size });
    if (scale_tensor) tensors_vector.push_back({ *scale_tensor });

    auto op_name = phi::TransToFluidOpName("bilinear_interp");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_out_size = transformer->TransInTensor("out_size", out_size);
    auto new_size_tensor = transformer->TransInTensors("size_tensor", size_tensor);
    auto new_scale_tensor = transformer->TransInTensor("scale_tensor", scale_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = bilinear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "bilinear_interp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::bilinear_interp(x, out_size, size_tensor, scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bilinear_interp", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("bilinear_interp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BilinearInterpGradNode>(new BilinearInterpGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeout_d(out_d);
    grad_node->SetAttributeout_h(out_h);
    grad_node->SetAttributeout_w(out_w);
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributeinterp_method(interp_method);
    grad_node->SetAttributealign_corners(align_corners);
    grad_node->SetAttributealign_mode(align_mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(out_size) grad_node->SetTensorWrapperout_size(*out_size);
    if(size_tensor) grad_node->SetTensorWrappersize_tensor(*size_tensor);
    if(scale_tensor) grad_node->SetTensorWrapperscale_tensor(*scale_tensor);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: bilinear_interp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor bilinear_tensor_product_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& weight, const paddle::optional<paddle::experimental::Tensor>& bias) {
  VLOG(3) << "Running AD API: " << "bilinear_tensor_product";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bilinear_tensor_product dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bilinear_tensor_product");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y},{weight} };
    if (bias) amp_tensors_vector.push_back({ *bias });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    auto new_weight = egr::EagerAmpAutoCast("weight", weight, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bilinear_tensor_product_ad_func(new_x, new_y, new_weight, new_bias);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{weight} };
    if (bias) tensors_vector.push_back({ *bias });

    auto op_name = phi::TransToFluidOpName("bilinear_tensor_product");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_weight = transformer->TransInTensor("weight", weight);
auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bilinear_tensor_product_ad_func(new_x, new_y, new_weight, new_bias);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);
  egr::AutogradMeta* weight_autograd_meta = egr::EagerUtils::nullable_autograd_meta(weight);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "bilinear_tensor_product";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::bilinear_tensor_product(x, y, weight, bias);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bilinear_tensor_product", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta,weight_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("bilinear_tensor_product node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BilinearTensorProductGradNode>(new BilinearTensorProductGradNode(1, 4));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperweight(weight);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(weight, 2);
    if(bias.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(bias.get_ptr()), 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: bilinear_tensor_product";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bincount_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& weights, paddle::experimental::Scalar minlength) {
  VLOG(3) << "Running AD API: " << "bincount";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bincount dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bincount");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (weights) amp_tensors_vector.push_back({ *weights });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_weights = egr::EagerAmpAutoCast("weights", weights, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bincount_ad_func(new_x, new_weights, minlength);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (weights) tensors_vector.push_back({ *weights });

    auto op_name = phi::TransToFluidOpName("bincount");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
auto new_weights = transformer->TransInTensor("weights", weights);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bincount_ad_func(new_x, new_weights, minlength);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bincount";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHTS_TEMPLATE = " \n( weights , [%s]), ";
    std::string input_weights_str = paddle::string::Sprintf(TENSOR_WEIGHTS_TEMPLATE, egr::EagerUtils::TensorStr(weights));
    input_str += input_weights_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bincount(x, weights, minlength);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bincount";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHTS_TEMPLATE = " \n( weights , [%s]), ";
    std::string input_weights_str = paddle::string::Sprintf(TENSOR_WEIGHTS_TEMPLATE, egr::EagerUtils::TensorStr(weights));
    input_str += input_weights_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bitwise_and_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "bitwise_and";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bitwise_and dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bitwise_and");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bitwise_and_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("bitwise_and");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bitwise_and_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bitwise_and";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bitwise_and(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bitwise_and";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bitwise_not_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "bitwise_not";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bitwise_not dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bitwise_not");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bitwise_not_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("bitwise_not");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bitwise_not_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bitwise_not";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bitwise_not(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bitwise_not";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bitwise_or_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "bitwise_or";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bitwise_or dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bitwise_or");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bitwise_or_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("bitwise_or");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bitwise_or_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bitwise_or";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bitwise_or(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bitwise_or";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor bitwise_xor_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "bitwise_xor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("bitwise_xor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("bitwise_xor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return bitwise_xor_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("bitwise_xor");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = bitwise_xor_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "bitwise_xor";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::bitwise_xor(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: bitwise_xor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor box_coder_ad_func(const paddle::experimental::Tensor& prior_box, const paddle::optional<paddle::experimental::Tensor>& prior_box_var, const paddle::experimental::Tensor& target_box, std::string code_type, bool box_normalized, int axis, std::vector<float> variance) {
  VLOG(3) << "Running AD API: " << "box_coder";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("box_coder dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("box_coder");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {prior_box},{target_box} };
    if (prior_box_var) amp_tensors_vector.push_back({ *prior_box_var });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_prior_box = egr::EagerAmpAutoCast("prior_box", prior_box, amp_dst_dtype, op_name);
    auto new_target_box = egr::EagerAmpAutoCast("target_box", target_box, amp_dst_dtype, op_name);
    auto new_prior_box_var = egr::EagerAmpAutoCast("prior_box_var", prior_box_var, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return box_coder_ad_func(new_prior_box, new_prior_box_var, new_target_box, code_type, box_normalized, axis, variance);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {prior_box},{target_box} };
    if (prior_box_var) tensors_vector.push_back({ *prior_box_var });

    auto op_name = phi::TransToFluidOpName("box_coder");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_prior_box = transformer->TransInTensor("prior_box", prior_box);
    auto new_target_box = transformer->TransInTensor("target_box", target_box);
auto new_prior_box_var = transformer->TransInTensor("prior_box_var", prior_box_var);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output_box = box_coder_ad_func(new_prior_box, new_prior_box_var, new_target_box, code_type, box_normalized, axis, variance);

    transformer -> SetOutTensorLayout(&output_box);

    // Returns
    return output_box;
  }

  VLOG(5) << "Running C++ API: " << "box_coder";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PRIOR_BOX_TEMPLATE = " \n( prior_box , [%s]), ";
    std::string input_prior_box_str = paddle::string::Sprintf(TENSOR_PRIOR_BOX_TEMPLATE, egr::EagerUtils::TensorStr(prior_box));
    input_str += input_prior_box_str; 
    const char* TENSOR_PRIOR_BOX_VAR_TEMPLATE = " \n( prior_box_var , [%s]), ";
    std::string input_prior_box_var_str = paddle::string::Sprintf(TENSOR_PRIOR_BOX_VAR_TEMPLATE, egr::EagerUtils::TensorStr(prior_box_var));
    input_str += input_prior_box_var_str; 
    const char* TENSOR_TARGET_BOX_TEMPLATE = " \n( target_box , [%s]), ";
    std::string input_target_box_str = paddle::string::Sprintf(TENSOR_TARGET_BOX_TEMPLATE, egr::EagerUtils::TensorStr(target_box));
    input_str += input_target_box_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::box_coder(prior_box, prior_box_var, target_box, code_type, box_normalized, axis, variance);
  // Get Outputs
  auto& output_box = api_result;

  VLOG(4) << "Finish AD API: box_coder";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PRIOR_BOX_TEMPLATE = " \n( prior_box , [%s]), ";
    std::string input_prior_box_str = paddle::string::Sprintf(TENSOR_PRIOR_BOX_TEMPLATE, egr::EagerUtils::TensorStr(prior_box));
    input_str += input_prior_box_str; 
    const char* TENSOR_PRIOR_BOX_VAR_TEMPLATE = " \n( prior_box_var , [%s]), ";
    std::string input_prior_box_var_str = paddle::string::Sprintf(TENSOR_PRIOR_BOX_VAR_TEMPLATE, egr::EagerUtils::TensorStr(prior_box_var));
    input_str += input_prior_box_var_str; 
    const char* TENSOR_TARGET_BOX_TEMPLATE = " \n( target_box , [%s]), ";
    std::string input_target_box_str = paddle::string::Sprintf(TENSOR_TARGET_BOX_TEMPLATE, egr::EagerUtils::TensorStr(target_box));
    input_str += input_target_box_str; 
    const char* TENSOR_OUTPUT_BOX_TEMPLATE = " \n( output_box , [%s]), ";
    std::string output_output_box_str = paddle::string::Sprintf(TENSOR_OUTPUT_BOX_TEMPLATE, egr::EagerUtils::TensorStr(output_box));
    output_str += output_output_box_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output_box;
}


std::vector<paddle::experimental::Tensor> broadcast_tensors_ad_func(const std::vector<paddle::experimental::Tensor>& input) {
  VLOG(3) << "Running AD API: " << "broadcast_tensors";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("broadcast_tensors dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("broadcast_tensors");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { input };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCasts("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return broadcast_tensors_ad_func(new_input);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { input };
    
    auto op_name = phi::TransToFluidOpName("broadcast_tensors");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensors("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> out = broadcast_tensors_ad_func(new_input);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> input_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(input);
  std::vector<egr::AutogradMeta*>* input_autograd_meta = &input_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "broadcast_tensors";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::broadcast_tensors(input);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("broadcast_tensors", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> out_autograd_meta_vec = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*>* out_autograd_meta = &out_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("broadcast_tensors node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BroadcastTensorsGradNode>(new BroadcastTensorsGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: broadcast_tensors";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cast_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::DataType dtype) {
  VLOG(3) << "Running AD API: " << "cast";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cast dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for cast_ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cast");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cast_ad_func(new_x, dtype);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cast";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cast(x, dtype);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cast", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cast node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CastGradNode>(new CastGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cast";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&> check_finite_and_unscale__ad_func(std::vector<paddle::experimental::Tensor>& x, const paddle::experimental::Tensor& scale, paddle::experimental::Tensor& input_found_infinite) {
  VLOG(3) << "Running AD API: " << "check_finite_and_unscale_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("check_finite_and_unscale_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for check_finite_and_unscale__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x,{scale},{input_found_infinite} };
    
    auto op_name = phi::TransToFluidOpName("check_finite_and_unscale_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensors("x", x);
    auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_input_found_infinite = transformer->TransInTensor("input_found_infinite", input_found_infinite);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&> api_result = check_finite_and_unscale__ad_func(new_x, new_scale, new_input_found_infinite);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& output_found_infinite = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&output_found_infinite);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&>{out, output_found_infinite};
  }

  VLOG(5) << "Running C++ API: " << "check_finite_and_unscale_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_INPUT_FOUND_INFINITE_TEMPLATE = " \n( input_found_infinite , [%s]), ";
    std::string input_input_found_infinite_str = paddle::string::Sprintf(TENSOR_INPUT_FOUND_INFINITE_TEMPLATE, egr::EagerUtils::TensorStr(input_found_infinite));
    input_str += input_input_found_infinite_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::check_finite_and_unscale_(x, scale, input_found_infinite);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& output_found_infinite = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: check_finite_and_unscale_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_INPUT_FOUND_INFINITE_TEMPLATE = " \n( input_found_infinite , [%s]), ";
    std::string input_input_found_infinite_str = paddle::string::Sprintf(TENSOR_INPUT_FOUND_INFINITE_TEMPLATE, egr::EagerUtils::TensorStr(input_found_infinite));
    input_str += input_input_found_infinite_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_OUTPUT_FOUND_INFINITE_TEMPLATE = " \n( output_found_infinite , [%s]), ";
    std::string output_output_found_infinite_str = paddle::string::Sprintf(TENSOR_OUTPUT_FOUND_INFINITE_TEMPLATE, egr::EagerUtils::TensorStr(output_found_infinite));
    output_str += output_output_found_infinite_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&>{out, output_found_infinite};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> class_center_sample_ad_func(const paddle::experimental::Tensor& label, int num_classes, int num_samples, int ring_id, int rank, int nranks, bool fix_seed, int seed) {
  VLOG(3) << "Running AD API: " << "class_center_sample";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("class_center_sample dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("class_center_sample");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return class_center_sample_ad_func(new_label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {label} };
    
    auto op_name = phi::TransToFluidOpName("class_center_sample");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = class_center_sample_ad_func(new_label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed);

        auto& remapped_label = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&remapped_label);
        auto& sampled_local_class_center = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&sampled_local_class_center);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{remapped_label, sampled_local_class_center};
  }

  VLOG(5) << "Running C++ API: " << "class_center_sample";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::class_center_sample(label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed);
  // Get Outputs
  auto& remapped_label = std::get<0>(api_result);
  auto& sampled_local_class_center = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: class_center_sample";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_REMAPPED_LABEL_TEMPLATE = " \n( remapped_label , [%s]), ";
    std::string output_remapped_label_str = paddle::string::Sprintf(TENSOR_REMAPPED_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(remapped_label));
    output_str += output_remapped_label_str; 
    const char* TENSOR_SAMPLED_LOCAL_CLASS_CENTER_TEMPLATE = " \n( sampled_local_class_center , [%s]), ";
    std::string output_sampled_local_class_center_str = paddle::string::Sprintf(TENSOR_SAMPLED_LOCAL_CLASS_CENTER_TEMPLATE, egr::EagerUtils::TensorStr(sampled_local_class_center));
    output_str += output_sampled_local_class_center_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{remapped_label, sampled_local_class_center};
}


paddle::experimental::Tensor clip_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar min, paddle::experimental::Scalar max) {
  VLOG(3) << "Running AD API: " << "clip";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("clip dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("clip");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return clip_ad_func(new_x, min, max);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("clip");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = clip_ad_func(new_x, min, max);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "clip";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::clip(x, min, max);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("clip", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("clip node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ClipGradNode>(new ClipGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributemin(min);
    grad_node->SetAttributemax(max);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: clip";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& clip__ad_func(paddle::experimental::Tensor& x, paddle::experimental::Scalar min, paddle::experimental::Scalar max) {
  VLOG(3) << "Running AD API: " << "clip_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("clip_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for clip__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("clip_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = clip__ad_func(new_x, min, max);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "clip_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::clip_(x, min, max);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("clip_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("clip node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ClipGradNode>(new ClipGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributemin(min);
    grad_node->SetAttributemax(max);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: clip_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor clip_by_norm_ad_func(const paddle::experimental::Tensor& x, float max_norm) {
  VLOG(3) << "Running AD API: " << "clip_by_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("clip_by_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("clip_by_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return clip_by_norm_ad_func(new_x, max_norm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("clip_by_norm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = clip_by_norm_ad_func(new_x, max_norm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "clip_by_norm";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::clip_by_norm(x, max_norm);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: clip_by_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor> coalesce_tensor_ad_func(const std::vector<paddle::experimental::Tensor>& input, paddle::experimental::DataType dtype, bool copy_data, bool set_constant, bool persist_output, float constant, bool use_align, int align_size, int size_of_dtype, std::vector<int64_t> concated_shapes, std::vector<int64_t> concated_ranks) {
  VLOG(3) << "Running AD API: " << "coalesce_tensor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("coalesce_tensor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("coalesce_tensor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { input };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCasts("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return coalesce_tensor_ad_func(new_input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { input };
    
    auto op_name = phi::TransToFluidOpName("coalesce_tensor");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensors("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor> api_result = coalesce_tensor_ad_func(new_input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks);

        auto& output = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&output);
        auto& fused_output = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&fused_output);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor>{output, fused_output};
  }

  VLOG(5) << "Running C++ API: " << "coalesce_tensor";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::coalesce_tensor(input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks);
  // Get Outputs
  auto& output = std::get<0>(api_result);
  auto& fused_output = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: coalesce_tensor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
    const char* TENSOR_FUSED_OUTPUT_TEMPLATE = " \n( fused_output , [%s]), ";
    std::string output_fused_output_str = paddle::string::Sprintf(TENSOR_FUSED_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(fused_output));
    output_str += output_fused_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor>{output, fused_output};
}


paddle::experimental::Tensor concat_ad_func(const std::vector<paddle::experimental::Tensor>& x, paddle::experimental::Scalar axis) {
  VLOG(3) << "Running AD API: " << "concat";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("concat dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("concat");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { x };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCasts("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return concat_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x };
    
    auto op_name = phi::TransToFluidOpName("concat");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensors("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = concat_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> x_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(x);
  std::vector<egr::AutogradMeta*>* x_autograd_meta = &x_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "concat";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::concat(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("concat", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("concat node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ConcatGradNode>(new ConcatGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: concat";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor conv2d_transpose_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& filter, std::vector<int> strides, std::vector<int> paddings, std::vector<int> output_padding, paddle::experimental::IntArray output_size, std::string padding_algorithm, int groups, std::vector<int> dilations, std::string data_format) {
  VLOG(3) << "Running AD API: " << "conv2d_transpose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("conv2d_transpose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("conv2d_transpose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{filter} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return conv2d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{filter} };
    
    auto op_name = phi::TransToFluidOpName("conv2d_transpose");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_filter = transformer->TransInTensor("filter", filter);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = conv2d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);

  VLOG(5) << "Running C++ API: " << "conv2d_transpose";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::conv2d_transpose(x, filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv2d_transpose", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,filter_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("conv2d_transpose node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv2dTransposeGradNode>(new Conv2dTransposeGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeoutput_padding(output_padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperfilter(filter);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(filter, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: conv2d_transpose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor conv3d_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& filter, std::vector<int> strides, std::vector<int> paddings, std::string padding_algorithm, int groups, std::vector<int> dilations, std::string data_format) {
  VLOG(3) << "Running AD API: " << "conv3d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("conv3d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("conv3d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{filter} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return conv3d_ad_func(new_input, new_filter, strides, paddings, padding_algorithm, groups, dilations, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{filter} };
    
    auto op_name = phi::TransToFluidOpName("conv3d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_filter = transformer->TransInTensor("filter", filter);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = conv3d_ad_func(new_input, new_filter, strides, paddings, padding_algorithm, groups, dilations, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);

  VLOG(5) << "Running C++ API: " << "conv3d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::conv3d(input, filter, strides, paddings, padding_algorithm, groups, dilations, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta,filter_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("conv3d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv3dGradNode>(new Conv3dGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperfilter(filter);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(filter, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: conv3d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor conv3d_transpose_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& filter, std::vector<int> strides, std::vector<int> paddings, std::vector<int> output_padding, std::vector<int> output_size, std::string padding_algorithm, int groups, std::vector<int> dilations, std::string data_format) {
  VLOG(3) << "Running AD API: " << "conv3d_transpose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("conv3d_transpose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("conv3d_transpose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{filter} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return conv3d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{filter} };
    
    auto op_name = phi::TransToFluidOpName("conv3d_transpose");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_filter = transformer->TransInTensor("filter", filter);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = conv3d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);

  VLOG(5) << "Running C++ API: " << "conv3d_transpose";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::conv3d_transpose(x, filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_transpose", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,filter_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("conv3d_transpose node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv3dTransposeGradNode>(new Conv3dTransposeGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeoutput_padding(output_padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperfilter(filter);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(filter, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: conv3d_transpose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor copy_to_ad_func(const paddle::experimental::Tensor& x, paddle::Place place, bool blocking) {
  VLOG(3) << "Running AD API: " << "copy_to";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("copy_to dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("copy_to");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return copy_to_ad_func(new_x, place, blocking);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("copy_to");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = copy_to_ad_func(new_x, place, blocking);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "copy_to";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::copy_to(x, place, blocking);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: copy_to";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> cross_entropy_with_softmax_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {
  VLOG(3) << "Running AD API: " << "cross_entropy_with_softmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cross_entropy_with_softmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cross_entropy_with_softmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cross_entropy_with_softmax_ad_func(new_input, new_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label} };
    
    auto op_name = phi::TransToFluidOpName("cross_entropy_with_softmax");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = cross_entropy_with_softmax_ad_func(new_input, new_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis);

        auto& softmax = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&softmax);
        auto& loss = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&loss);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{softmax, loss};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "cross_entropy_with_softmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cross_entropy_with_softmax(input, label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cross_entropy_with_softmax", api_result); }

  // Get Outputs
  auto& softmax = std::get<0>(api_result);
  auto& loss = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* softmax_autograd_meta = egr::EagerUtils::autograd_meta(&softmax);
  egr::AutogradMeta* loss_autograd_meta = egr::EagerUtils::autograd_meta(&loss);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cross_entropy_with_softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,softmax_autograd_meta,loss_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CrossEntropyWithSoftmaxGradNode>(new CrossEntropyWithSoftmaxGradNode(2, 2));
    // SetAttributes if needed
    grad_node->SetAttributesoft_label(soft_label);
    grad_node->SetAttributeuse_softmax(use_softmax);
    grad_node->SetAttributenumeric_stable_mode(numeric_stable_mode);
    grad_node->SetAttributeignore_index(ignore_index);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(softmax_autograd_meta, 0);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(loss_autograd_meta, 1);
    }
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetHistory(softmax_autograd_meta, grad_node);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetHistory(loss_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(softmax, 0);
    grad_node->SetGradInMeta(loss, 1);
    egr::EagerUtils::CheckAndRetainGrad(softmax);
    egr::EagerUtils::CheckAndRetainGrad(loss);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersoftmax(softmax);
  }


  VLOG(4) << "Finish AD API: cross_entropy_with_softmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string output_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    output_str += output_softmax_str; 
    const char* TENSOR_LOSS_TEMPLATE = " \n( loss , [%s]), ";
    std::string output_loss_str = paddle::string::Sprintf(TENSOR_LOSS_TEMPLATE, egr::EagerUtils::TensorStr(loss));
    output_str += output_loss_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{softmax, loss};
}


paddle::experimental::Tensor cumprod_ad_func(const paddle::experimental::Tensor& x, int dim) {
  VLOG(3) << "Running AD API: " << "cumprod";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cumprod dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cumprod");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cumprod_ad_func(new_x, dim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cumprod");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &dim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cumprod_ad_func(new_x, dim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cumprod";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cumprod(x, dim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cumprod", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cumprod node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CumprodGradNode>(new CumprodGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributedim(dim);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: cumprod";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor cumsum_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(3) << "Running AD API: " << "cumsum";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cumsum dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("cumsum");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return cumsum_ad_func(new_x, axis, flatten, exclusive, reverse);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cumsum");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cumsum_ad_func(new_x, axis, flatten, exclusive, reverse);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cumsum";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::cumsum(x, axis, flatten, exclusive, reverse);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cumsum", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cumsum node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CumsumGradNode>(new CumsumGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributeflatten(flatten);
    grad_node->SetAttributeexclusive(exclusive);
    grad_node->SetAttributereverse(reverse);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cumsum";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor decode_jpeg_ad_func(const paddle::experimental::Tensor& x, std::string mode, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "decode_jpeg";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("decode_jpeg dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("decode_jpeg");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return decode_jpeg_ad_func(new_x, mode, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("decode_jpeg");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = decode_jpeg_ad_func(new_x, mode, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "decode_jpeg";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::decode_jpeg(x, mode, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: decode_jpeg";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor deformable_conv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& offset, const paddle::experimental::Tensor& filter, const paddle::optional<paddle::experimental::Tensor>& mask, std::vector<int> strides, std::vector<int> paddings, std::vector<int> dilations, int deformable_groups, int groups, int im2col_step) {
  VLOG(3) << "Running AD API: " << "deformable_conv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("deformable_conv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("deformable_conv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{offset},{filter} };
    if (mask) amp_tensors_vector.push_back({ *mask });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_offset = egr::EagerAmpAutoCast("offset", offset, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    auto new_mask = egr::EagerAmpAutoCast("mask", mask, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return deformable_conv_ad_func(new_x, new_offset, new_filter, new_mask, strides, paddings, dilations, deformable_groups, groups, im2col_step);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{offset},{filter} };
    if (mask) tensors_vector.push_back({ *mask });

    auto op_name = phi::TransToFluidOpName("deformable_conv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_offset = transformer->TransInTensor("offset", offset);
    auto new_filter = transformer->TransInTensor("filter", filter);
auto new_mask = transformer->TransInTensor("mask", mask);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = deformable_conv_ad_func(new_x, new_offset, new_filter, new_mask, strides, paddings, dilations, deformable_groups, groups, im2col_step);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* offset_autograd_meta = egr::EagerUtils::nullable_autograd_meta(offset);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);
  egr::AutogradMeta* mask_autograd_meta = egr::EagerUtils::nullable_autograd_meta(mask);

  VLOG(5) << "Running C++ API: " << "deformable_conv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OFFSET_TEMPLATE = " \n( offset , [%s]), ";
    std::string input_offset_str = paddle::string::Sprintf(TENSOR_OFFSET_TEMPLATE, egr::EagerUtils::TensorStr(offset));
    input_str += input_offset_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::deformable_conv(x, offset, filter, mask, strides, paddings, dilations, deformable_groups, groups, im2col_step);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("deformable_conv", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,offset_autograd_meta,filter_autograd_meta,mask_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("deformable_conv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DeformableConvGradNode>(new DeformableConvGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedeformable_groups(deformable_groups);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributeim2col_step(im2col_step);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperoffset(offset);
    grad_node->SetTensorWrapperfilter(filter);
    if(mask) grad_node->SetTensorWrappermask(*mask);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(offset, 1);
    grad_node->SetGradOutMeta(filter, 2);
    if(mask.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(mask.get_ptr()), 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: deformable_conv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OFFSET_TEMPLATE = " \n( offset , [%s]), ";
    std::string input_offset_str = paddle::string::Sprintf(TENSOR_OFFSET_TEMPLATE, egr::EagerUtils::TensorStr(offset));
    input_str += input_offset_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor depthwise_conv2d_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& filter, std::vector<int> strides, std::vector<int> paddings, std::string padding_algorithm, int groups, std::vector<int> dilations, std::string data_format) {
  VLOG(3) << "Running AD API: " << "depthwise_conv2d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("depthwise_conv2d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("depthwise_conv2d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{filter} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return depthwise_conv2d_ad_func(new_input, new_filter, strides, paddings, padding_algorithm, groups, dilations, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{filter} };
    
    auto op_name = phi::TransToFluidOpName("depthwise_conv2d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_filter = transformer->TransInTensor("filter", filter);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = depthwise_conv2d_ad_func(new_input, new_filter, strides, paddings, padding_algorithm, groups, dilations, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);

  VLOG(5) << "Running C++ API: " << "depthwise_conv2d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::depthwise_conv2d(input, filter, strides, paddings, padding_algorithm, groups, dilations, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("depthwise_conv2d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta,filter_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("depthwise_conv2d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DepthwiseConv2dGradNode>(new DepthwiseConv2dGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperfilter(filter);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(filter, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: depthwise_conv2d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor depthwise_conv2d_transpose_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& filter, std::vector<int> strides, std::vector<int> paddings, std::vector<int> output_padding, paddle::experimental::IntArray output_size, std::string padding_algorithm, int groups, std::vector<int> dilations, std::string data_format) {
  VLOG(3) << "Running AD API: " << "depthwise_conv2d_transpose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("depthwise_conv2d_transpose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("depthwise_conv2d_transpose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{filter} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_filter = egr::EagerAmpAutoCast("filter", filter, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return depthwise_conv2d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{filter} };
    
    auto op_name = phi::TransToFluidOpName("depthwise_conv2d_transpose");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_filter = transformer->TransInTensor("filter", filter);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = depthwise_conv2d_transpose_ad_func(new_x, new_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* filter_autograd_meta = egr::EagerUtils::nullable_autograd_meta(filter);

  VLOG(5) << "Running C++ API: " << "depthwise_conv2d_transpose";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::depthwise_conv2d_transpose(x, filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("depthwise_conv2d_transpose", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,filter_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("depthwise_conv2d_transpose node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DepthwiseConv2dTransposeGradNode>(new DepthwiseConv2dTransposeGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeoutput_padding(output_padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperfilter(filter);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(filter, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: depthwise_conv2d_transpose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor dirichlet_ad_func(const paddle::experimental::Tensor& alpha) {
  VLOG(3) << "Running AD API: " << "dirichlet";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("dirichlet dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("dirichlet");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {alpha} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_alpha = egr::EagerAmpAutoCast("alpha", alpha, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return dirichlet_ad_func(new_alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {alpha} };
    
    auto op_name = phi::TransToFluidOpName("dirichlet");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_alpha = transformer->TransInTensor("alpha", alpha);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = dirichlet_ad_func(new_alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "dirichlet";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::dirichlet(alpha);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: dirichlet";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor> distribute_fpn_proposals_ad_func(const paddle::experimental::Tensor& fpn_rois, const paddle::optional<paddle::experimental::Tensor>& rois_num, int min_level, int max_level, int refer_level, int refer_scale, bool pixel_offset) {
  VLOG(3) << "Running AD API: " << "distribute_fpn_proposals";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("distribute_fpn_proposals dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("distribute_fpn_proposals");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {fpn_rois} };
    if (rois_num) amp_tensors_vector.push_back({ *rois_num });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_fpn_rois = egr::EagerAmpAutoCast("fpn_rois", fpn_rois, amp_dst_dtype, op_name);
    auto new_rois_num = egr::EagerAmpAutoCast("rois_num", rois_num, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return distribute_fpn_proposals_ad_func(new_fpn_rois, new_rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {fpn_rois} };
    if (rois_num) tensors_vector.push_back({ *rois_num });

    auto op_name = phi::TransToFluidOpName("distribute_fpn_proposals");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_fpn_rois = transformer->TransInTensor("fpn_rois", fpn_rois);
auto new_rois_num = transformer->TransInTensor("rois_num", rois_num);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor> api_result = distribute_fpn_proposals_ad_func(new_fpn_rois, new_rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset);

        auto& multi_fpn_rois = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&multi_fpn_rois);
        auto& multi_level_rois_num = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&multi_level_rois_num);
        auto& restore_index = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&restore_index);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor>{multi_fpn_rois, multi_level_rois_num, restore_index};
  }

  VLOG(5) << "Running C++ API: " << "distribute_fpn_proposals";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FPN_ROIS_TEMPLATE = " \n( fpn_rois , [%s]), ";
    std::string input_fpn_rois_str = paddle::string::Sprintf(TENSOR_FPN_ROIS_TEMPLATE, egr::EagerUtils::TensorStr(fpn_rois));
    input_str += input_fpn_rois_str; 
    const char* TENSOR_ROIS_NUM_TEMPLATE = " \n( rois_num , [%s]), ";
    std::string input_rois_num_str = paddle::string::Sprintf(TENSOR_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(rois_num));
    input_str += input_rois_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::distribute_fpn_proposals(fpn_rois, rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset);
  // Get Outputs
  auto& multi_fpn_rois = std::get<0>(api_result);
  auto& multi_level_rois_num = std::get<1>(api_result);
  auto& restore_index = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: distribute_fpn_proposals";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FPN_ROIS_TEMPLATE = " \n( fpn_rois , [%s]), ";
    std::string input_fpn_rois_str = paddle::string::Sprintf(TENSOR_FPN_ROIS_TEMPLATE, egr::EagerUtils::TensorStr(fpn_rois));
    input_str += input_fpn_rois_str; 
    const char* TENSOR_ROIS_NUM_TEMPLATE = " \n( rois_num , [%s]), ";
    std::string input_rois_num_str = paddle::string::Sprintf(TENSOR_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(rois_num));
    input_str += input_rois_num_str; 
    const char* TENSOR_MULTI_FPN_ROIS_TEMPLATE = " \n( multi_fpn_rois , [%s]), ";
    std::string output_multi_fpn_rois_str = paddle::string::Sprintf(TENSOR_MULTI_FPN_ROIS_TEMPLATE, egr::EagerUtils::TensorStr(multi_fpn_rois));
    output_str += output_multi_fpn_rois_str; 
    const char* TENSOR_MULTI_LEVEL_ROIS_NUM_TEMPLATE = " \n( multi_level_rois_num , [%s]), ";
    std::string output_multi_level_rois_num_str = paddle::string::Sprintf(TENSOR_MULTI_LEVEL_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(multi_level_rois_num));
    output_str += output_multi_level_rois_num_str; 
    const char* TENSOR_RESTORE_INDEX_TEMPLATE = " \n( restore_index , [%s]), ";
    std::string output_restore_index_str = paddle::string::Sprintf(TENSOR_RESTORE_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(restore_index));
    output_str += output_restore_index_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>, paddle::experimental::Tensor>{multi_fpn_rois, multi_level_rois_num, restore_index};
}


paddle::experimental::Tensor divide_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "divide";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("divide dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("divide");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return divide_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("divide");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = divide_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "divide";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::divide(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("divide node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DivideGradNode>(new DivideGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: divide";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> dropout_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& seed_tensor, paddle::experimental::Scalar p, bool is_test, std::string mode, int seed, bool fix_seed) {
  VLOG(3) << "Running AD API: " << "dropout";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("dropout dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("dropout");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (seed_tensor) amp_tensors_vector.push_back({ *seed_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_seed_tensor = egr::EagerAmpAutoCast("seed_tensor", seed_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return dropout_ad_func(new_x, new_seed_tensor, p, is_test, mode, seed, fix_seed);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (seed_tensor) tensors_vector.push_back({ *seed_tensor });

    auto op_name = phi::TransToFluidOpName("dropout");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
auto new_seed_tensor = transformer->TransInTensor("seed_tensor", seed_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = dropout_ad_func(new_x, new_seed_tensor, p, is_test, mode, seed, fix_seed);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mask = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mask);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "dropout";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEED_TENSOR_TEMPLATE = " \n( seed_tensor , [%s]), ";
    std::string input_seed_tensor_str = paddle::string::Sprintf(TENSOR_SEED_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(seed_tensor));
    input_str += input_seed_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::dropout(x, seed_tensor, p, is_test, mode, seed, fix_seed);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dropout", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mask = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mask_autograd_meta = egr::EagerUtils::autograd_meta(&mask);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("dropout node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mask_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DropoutGradNode>(new DropoutGradNode(2, 2));
    // SetAttributes if needed
    grad_node->SetAttributep(p);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributemode(mode);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mask_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetHistory(mask_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mask, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mask);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermask(mask);
  }


  VLOG(4) << "Finish AD API: dropout";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEED_TENSOR_TEMPLATE = " \n( seed_tensor , [%s]), ";
    std::string input_seed_tensor_str = paddle::string::Sprintf(TENSOR_SEED_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(seed_tensor));
    input_str += input_seed_tensor_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string output_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    output_str += output_mask_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> edit_distance_ad_func(const paddle::experimental::Tensor& hyps, const paddle::experimental::Tensor& refs, const paddle::optional<paddle::experimental::Tensor>& hypslength, const paddle::optional<paddle::experimental::Tensor>& refslength, bool normalized) {
  VLOG(3) << "Running AD API: " << "edit_distance";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("edit_distance dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("edit_distance");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {hyps},{refs} };
    if (hypslength) amp_tensors_vector.push_back({ *hypslength });
    if (refslength) amp_tensors_vector.push_back({ *refslength });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_hyps = egr::EagerAmpAutoCast("hyps", hyps, amp_dst_dtype, op_name);
    auto new_refs = egr::EagerAmpAutoCast("refs", refs, amp_dst_dtype, op_name);
    auto new_hypslength = egr::EagerAmpAutoCast("hypslength", hypslength, amp_dst_dtype, op_name);
    auto new_refslength = egr::EagerAmpAutoCast("refslength", refslength, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return edit_distance_ad_func(new_hyps, new_refs, new_hypslength, new_refslength, normalized);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {hyps},{refs} };
    if (hypslength) tensors_vector.push_back({ *hypslength });
    if (refslength) tensors_vector.push_back({ *refslength });

    auto op_name = phi::TransToFluidOpName("edit_distance");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_hyps = transformer->TransInTensor("hyps", hyps);
    auto new_refs = transformer->TransInTensor("refs", refs);
auto new_hypslength = transformer->TransInTensor("hypslength", hypslength);
    auto new_refslength = transformer->TransInTensor("refslength", refslength);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = edit_distance_ad_func(new_hyps, new_refs, new_hypslength, new_refslength, normalized);

        auto& sequencenum = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&sequencenum);
        auto& out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&out);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{sequencenum, out};
  }

  VLOG(5) << "Running C++ API: " << "edit_distance";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_HYPS_TEMPLATE = " \n( hyps , [%s]), ";
    std::string input_hyps_str = paddle::string::Sprintf(TENSOR_HYPS_TEMPLATE, egr::EagerUtils::TensorStr(hyps));
    input_str += input_hyps_str; 
    const char* TENSOR_REFS_TEMPLATE = " \n( refs , [%s]), ";
    std::string input_refs_str = paddle::string::Sprintf(TENSOR_REFS_TEMPLATE, egr::EagerUtils::TensorStr(refs));
    input_str += input_refs_str; 
    const char* TENSOR_HYPSLENGTH_TEMPLATE = " \n( hypslength , [%s]), ";
    std::string input_hypslength_str = paddle::string::Sprintf(TENSOR_HYPSLENGTH_TEMPLATE, egr::EagerUtils::TensorStr(hypslength));
    input_str += input_hypslength_str; 
    const char* TENSOR_REFSLENGTH_TEMPLATE = " \n( refslength , [%s]), ";
    std::string input_refslength_str = paddle::string::Sprintf(TENSOR_REFSLENGTH_TEMPLATE, egr::EagerUtils::TensorStr(refslength));
    input_str += input_refslength_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::edit_distance(hyps, refs, hypslength, refslength, normalized);
  // Get Outputs
  auto& sequencenum = std::get<0>(api_result);
  auto& out = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: edit_distance";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_HYPS_TEMPLATE = " \n( hyps , [%s]), ";
    std::string input_hyps_str = paddle::string::Sprintf(TENSOR_HYPS_TEMPLATE, egr::EagerUtils::TensorStr(hyps));
    input_str += input_hyps_str; 
    const char* TENSOR_REFS_TEMPLATE = " \n( refs , [%s]), ";
    std::string input_refs_str = paddle::string::Sprintf(TENSOR_REFS_TEMPLATE, egr::EagerUtils::TensorStr(refs));
    input_str += input_refs_str; 
    const char* TENSOR_HYPSLENGTH_TEMPLATE = " \n( hypslength , [%s]), ";
    std::string input_hypslength_str = paddle::string::Sprintf(TENSOR_HYPSLENGTH_TEMPLATE, egr::EagerUtils::TensorStr(hypslength));
    input_str += input_hypslength_str; 
    const char* TENSOR_REFSLENGTH_TEMPLATE = " \n( refslength , [%s]), ";
    std::string input_refslength_str = paddle::string::Sprintf(TENSOR_REFSLENGTH_TEMPLATE, egr::EagerUtils::TensorStr(refslength));
    input_str += input_refslength_str; 
    const char* TENSOR_SEQUENCENUM_TEMPLATE = " \n( sequencenum , [%s]), ";
    std::string output_sequencenum_str = paddle::string::Sprintf(TENSOR_SEQUENCENUM_TEMPLATE, egr::EagerUtils::TensorStr(sequencenum));
    output_str += output_sequencenum_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{sequencenum, out};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> eigvalsh_ad_func(const paddle::experimental::Tensor& x, std::string uplo, bool is_test) {
  VLOG(3) << "Running AD API: " << "eigvalsh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("eigvalsh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("eigvalsh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return eigvalsh_ad_func(new_x, uplo, is_test);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("eigvalsh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = eigvalsh_ad_func(new_x, uplo, is_test);

        auto& eigenvalues = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&eigenvalues);
        auto& eigenvectors = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&eigenvectors);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{eigenvalues, eigenvectors};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "eigvalsh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::eigvalsh(x, uplo, is_test);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eigvalsh", api_result); }

  // Get Outputs
  auto& eigenvalues = std::get<0>(api_result);
  auto& eigenvectors = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* eigenvalues_autograd_meta = egr::EagerUtils::autograd_meta(&eigenvalues);
  egr::AutogradMeta* eigenvectors_autograd_meta = egr::EagerUtils::autograd_meta(&eigenvectors);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("eigvalsh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,eigenvalues_autograd_meta,eigenvectors_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EigvalshGradNode>(new EigvalshGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeuplo(uplo);
    grad_node->SetAttributeis_test(is_test);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (eigenvalues_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(eigenvalues_autograd_meta, 0);
    }
    if (eigenvectors_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(eigenvectors_autograd_meta, 1);
    }
    if (eigenvalues_autograd_meta) {
      egr::EagerUtils::SetHistory(eigenvalues_autograd_meta, grad_node);
    }
    if (eigenvectors_autograd_meta) {
      egr::EagerUtils::SetHistory(eigenvectors_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(eigenvalues, 0);
    grad_node->SetGradInMeta(eigenvectors, 1);
    egr::EagerUtils::CheckAndRetainGrad(eigenvalues);
    egr::EagerUtils::CheckAndRetainGrad(eigenvectors);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappereigenvectors(eigenvectors);
  }


  VLOG(4) << "Finish AD API: eigvalsh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_EIGENVALUES_TEMPLATE = " \n( eigenvalues , [%s]), ";
    std::string output_eigenvalues_str = paddle::string::Sprintf(TENSOR_EIGENVALUES_TEMPLATE, egr::EagerUtils::TensorStr(eigenvalues));
    output_str += output_eigenvalues_str; 
    const char* TENSOR_EIGENVECTORS_TEMPLATE = " \n( eigenvectors , [%s]), ";
    std::string output_eigenvectors_str = paddle::string::Sprintf(TENSOR_EIGENVECTORS_TEMPLATE, egr::EagerUtils::TensorStr(eigenvectors));
    output_str += output_eigenvectors_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{eigenvalues, eigenvectors};
}


std::tuple<paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>> einsum_ad_func(const std::vector<paddle::experimental::Tensor>& x, std::string equation) {
  VLOG(3) << "Running AD API: " << "einsum";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("einsum dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("einsum");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { x };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCasts("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return einsum_ad_func(new_x, equation);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x };
    
    auto op_name = phi::TransToFluidOpName("einsum");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensors("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>> api_result = einsum_ad_func(new_x, equation);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& inner_cache = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&inner_cache);
        auto& x_shape = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&x_shape);

    // Returns
    return std::tuple<paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>>{out, inner_cache, x_shape};
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> x_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(x);
  std::vector<egr::AutogradMeta*>* x_autograd_meta = &x_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "einsum";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::einsum(x, equation);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("einsum", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& inner_cache = std::get<1>(api_result);
  auto& x_shape = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*> inner_cache_autograd_meta_vec = egr::EagerUtils::autograd_meta(&inner_cache);
  std::vector<egr::AutogradMeta*>* inner_cache_autograd_meta = &inner_cache_autograd_meta_vec;
  std::vector<egr::AutogradMeta*> x_shape_autograd_meta_vec = egr::EagerUtils::autograd_meta(&x_shape);
  std::vector<egr::AutogradMeta*>* x_shape_autograd_meta = &x_shape_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("einsum node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,inner_cache_autograd_meta,x_shape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EinsumGradNode>(new EinsumGradNode(3, 1));
    // SetAttributes if needed
    grad_node->SetAttributeequation(equation);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (inner_cache_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(inner_cache_autograd_meta, 1);
    }
    if (x_shape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(x_shape_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (inner_cache_autograd_meta) {
      egr::EagerUtils::SetHistory(inner_cache_autograd_meta, grad_node);
    }
    if (x_shape_autograd_meta) {
      egr::EagerUtils::SetHistory(x_shape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(inner_cache, 1);
    grad_node->SetGradInMeta(x_shape, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(inner_cache);
    egr::EagerUtils::CheckAndRetainGrad(x_shape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperx_shape(x_shape);
    grad_node->SetTensorWrapperinner_cache(inner_cache);
  }


  VLOG(4) << "Finish AD API: einsum";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INNER_CACHE_TEMPLATE = " \n( inner_cache , [%s]), ";
    std::string output_inner_cache_str = paddle::string::Sprintf(TENSOR_INNER_CACHE_TEMPLATE, egr::EagerUtils::TensorStr(inner_cache));
    output_str += output_inner_cache_str; 
    const char* TENSOR_X_SHAPE_TEMPLATE = " \n( x_shape , [%s]), ";
    std::string output_x_shape_str = paddle::string::Sprintf(TENSOR_X_SHAPE_TEMPLATE, egr::EagerUtils::TensorStr(x_shape));
    output_str += output_x_shape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>, std::vector<paddle::experimental::Tensor>>{out, inner_cache, x_shape};
}


paddle::experimental::Tensor elementwise_pow_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "elementwise_pow";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("elementwise_pow dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("elementwise_pow");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return elementwise_pow_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("elementwise_pow");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = elementwise_pow_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "elementwise_pow";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::elementwise_pow(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elementwise_pow", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("elementwise_pow node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ElementwisePowGradNode>(new ElementwisePowGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: elementwise_pow";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor embedding_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& weight, int64_t padding_idx, bool sparse) {
  VLOG(3) << "Running AD API: " << "embedding";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("embedding dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("embedding");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{weight} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_weight = egr::EagerAmpAutoCast("weight", weight, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return embedding_ad_func(new_x, new_weight, padding_idx, sparse);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{weight} };
    
    auto op_name = phi::TransToFluidOpName("embedding");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_weight = transformer->TransInTensor("weight", weight);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = embedding_ad_func(new_x, new_weight, padding_idx, sparse);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* weight_autograd_meta = egr::EagerUtils::nullable_autograd_meta(weight);

  VLOG(5) << "Running C++ API: " << "embedding";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::embedding(x, weight, padding_idx, sparse);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("embedding", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,weight_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("embedding node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<EmbeddingGradNode>(new EmbeddingGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributepadding_idx(padding_idx);
    grad_node->SetAttributesparse(sparse);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperweight(weight);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(weight, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: embedding";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor empty_ad_func(paddle::experimental::IntArray shape, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "empty";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("empty dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for empty_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "empty";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::empty(shape, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: empty";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor empty_like_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "empty_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("empty_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("empty_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return empty_like_ad_func(new_x, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("empty_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = empty_like_ad_func(new_x, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "empty_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::empty_like(x, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: empty_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor equal_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "equal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("equal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("equal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return equal_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("equal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = equal_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "equal";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::equal(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: equal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor expand_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray shape) {
  VLOG(3) << "Running AD API: " << "expand";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("expand dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("expand");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return expand_ad_func(new_x, shape);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("expand");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = expand_ad_func(new_x, shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "expand";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::expand(x, shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expand", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("expand node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ExpandGradNode>(new ExpandGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeshape(shape);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: expand";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor expand_as_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& y, std::vector<int> target_shape) {
  VLOG(3) << "Running AD API: " << "expand_as";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("expand_as dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("expand_as");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (y) amp_tensors_vector.push_back({ *y });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return expand_as_ad_func(new_x, new_y, target_shape);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (y) tensors_vector.push_back({ *y });

    auto op_name = phi::TransToFluidOpName("expand_as");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = expand_as_ad_func(new_x, new_y, target_shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "expand_as";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::expand_as(x, y, target_shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expand_as", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("expand_as node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ExpandAsGradNode>(new ExpandAsGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributetarget_shape(target_shape);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: expand_as";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor& exponential__ad_func(paddle::experimental::Tensor& x, float lam) {
  VLOG(3) << "Running AD API: " << "exponential_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("exponential_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for exponential__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("exponential_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = exponential__ad_func(new_x, lam);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "exponential_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::exponential_(x, lam);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("exponential_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("exponential_ node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ExponentialGradNode>(new ExponentialGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: exponential_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor eye_ad_func(paddle::experimental::Scalar num_rows, paddle::experimental::Scalar num_columns, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "eye";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("eye dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for eye_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "eye";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::eye(num_rows, num_columns, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: eye";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fill_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar value) {
  VLOG(3) << "Running AD API: " << "fill";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fill");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fill_ad_func(new_x, value);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fill");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fill_ad_func(new_x, value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fill(x, value);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillGradNode>(new FillGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributevalue(value);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& fill__ad_func(paddle::experimental::Tensor& x, paddle::experimental::Scalar value) {
  VLOG(3) << "Running AD API: " << "fill_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for fill__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fill_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = fill__ad_func(new_x, value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::fill_(x, value);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillGradNode>(new FillGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributevalue(value);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fill_diagonal_ad_func(const paddle::experimental::Tensor& x, float value, int offset, bool wrap) {
  VLOG(3) << "Running AD API: " << "fill_diagonal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill_diagonal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fill_diagonal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fill_diagonal_ad_func(new_x, value, offset, wrap);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fill_diagonal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fill_diagonal_ad_func(new_x, value, offset, wrap);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill_diagonal";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fill_diagonal(x, value, offset, wrap);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill_diagonal node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillDiagonalGradNode>(new FillDiagonalGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributevalue(value);
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributewrap(wrap);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill_diagonal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& fill_diagonal__ad_func(paddle::experimental::Tensor& x, float value, int offset, bool wrap) {
  VLOG(3) << "Running AD API: " << "fill_diagonal_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fill_diagonal_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for fill_diagonal__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("fill_diagonal_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = fill_diagonal__ad_func(new_x, value, offset, wrap);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "fill_diagonal_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::fill_diagonal_(x, value, offset, wrap);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fill_diagonal node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FillDiagonalGradNode>(new FillDiagonalGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributevalue(value);
    grad_node->SetAttributeoffset(offset);
    grad_node->SetAttributewrap(wrap);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fill_diagonal_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor flatten_ad_func(const paddle::experimental::Tensor& x, int start_axis, int stop_axis) {
  VLOG(3) << "Running AD API: " << "flatten";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("flatten dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("flatten");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return flatten_ad_func(new_x, start_axis, stop_axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("flatten");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &start_axis, &stop_axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = flatten_ad_func(new_x, start_axis, stop_axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "flatten";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::flatten_intermediate(x, start_axis, stop_axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("flatten_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("flatten node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FlattenGradNode>(new FlattenGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: flatten";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& flatten__ad_func(paddle::experimental::Tensor& x, int start_axis, int stop_axis) {
  VLOG(3) << "Running AD API: " << "flatten_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("flatten_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for flatten__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("flatten_");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &start_axis, &stop_axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = flatten__ad_func(new_x, start_axis, stop_axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "flatten_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::flatten_intermediate_(x, start_axis, stop_axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("flatten_intermediate_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("flatten node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FlattenGradNode>(new FlattenGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: flatten_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor floor_divide_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "floor_divide";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("floor_divide dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("floor_divide");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return floor_divide_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("floor_divide");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = floor_divide_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "floor_divide";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::floor_divide(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: floor_divide";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fmax_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "fmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fmax_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("fmax");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fmax_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "fmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fmax(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fmax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FmaxGradNode>(new FmaxGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fmin_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "fmin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fmin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fmin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fmin_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("fmin");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fmin_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "fmin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::fmin(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fmin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fmin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FminGradNode>(new FminGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: fmin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor frobenius_norm_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keep_dim, bool reduce_all) {
  VLOG(3) << "Running AD API: " << "frobenius_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("frobenius_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("frobenius_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return frobenius_norm_ad_func(new_x, axis, keep_dim, reduce_all);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("frobenius_norm");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keep_dim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = frobenius_norm_ad_func(new_x, axis, keep_dim, reduce_all);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "frobenius_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::frobenius_norm(x, axis, keep_dim, reduce_all);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("frobenius_norm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("frobenius_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FrobeniusNormGradNode>(new FrobeniusNormGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeep_dim(keep_dim);
    grad_node->SetAttributereduce_all(reduce_all);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: frobenius_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor full_ad_func(paddle::experimental::IntArray shape, paddle::experimental::Scalar value, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "full";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("full dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for full_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "full";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::full(shape, value, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: full";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor& full__ad_func(paddle::experimental::Tensor& output, paddle::experimental::IntArray shape, paddle::experimental::Scalar value, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "full_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("full_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for full__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {output} };
    
    auto op_name = phi::TransToFluidOpName("full_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_output = transformer->TransInTensor("output", output);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = full__ad_func(new_output, shape, value, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "full_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto& api_result = paddle::experimental::full_(output, shape, value, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: full_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string input_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    input_str += input_output_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor full_batch_size_like_ad_func(const paddle::experimental::Tensor& input, std::vector<int> shape, paddle::experimental::DataType dtype, paddle::experimental::Scalar value, int input_dim_idx, int output_dim_idx, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "full_batch_size_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("full_batch_size_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("full_batch_size_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return full_batch_size_like_ad_func(new_input, shape, dtype, value, input_dim_idx, output_dim_idx, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("full_batch_size_like");
    auto transformer = egr::EagerLayoutAutotune<int, int>(op_name, tensors_vector, &input_dim_idx, &output_dim_idx);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = full_batch_size_like_ad_func(new_input, shape, dtype, value, input_dim_idx, output_dim_idx, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "full_batch_size_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::full_batch_size_like(input, shape, dtype, value, input_dim_idx, output_dim_idx, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: full_batch_size_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor full_like_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar value, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "full_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("full_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("full_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return full_like_ad_func(new_x, value, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("full_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = full_like_ad_func(new_x, value, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "full_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::full_like(x, value, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: full_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gather_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, paddle::experimental::Scalar axis) {
  VLOG(3) << "Running AD API: " << "gather";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gather dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("gather");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return gather_ad_func(new_x, new_index, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index} };
    
    auto op_name = phi::TransToFluidOpName("gather");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = gather_ad_func(new_x, new_index, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "gather";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::gather(x, index, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gather", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("gather node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GatherGradNode>(new GatherGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributeoverwrite(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindex(index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: gather";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor gaussian_ad_func(paddle::experimental::IntArray shape, float mean, float std, int seed, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "gaussian";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("gaussian dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for gaussian_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "gaussian";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::gaussian(shape, mean, std, seed, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: gaussian";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> generate_proposals_ad_func(const paddle::experimental::Tensor& scores, const paddle::experimental::Tensor& bbox_deltas, const paddle::experimental::Tensor& im_shape, const paddle::experimental::Tensor& anchors, const paddle::experimental::Tensor& variances, int pre_nms_top_n, int post_nms_top_n, float nms_thresh, float min_size, float eta, bool pixel_offset) {
  VLOG(3) << "Running AD API: " << "generate_proposals";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("generate_proposals dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("generate_proposals");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {scores},{bbox_deltas},{im_shape},{anchors},{variances} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_scores = egr::EagerAmpAutoCast("scores", scores, amp_dst_dtype, op_name);
    auto new_bbox_deltas = egr::EagerAmpAutoCast("bbox_deltas", bbox_deltas, amp_dst_dtype, op_name);
    auto new_im_shape = egr::EagerAmpAutoCast("im_shape", im_shape, amp_dst_dtype, op_name);
    auto new_anchors = egr::EagerAmpAutoCast("anchors", anchors, amp_dst_dtype, op_name);
    auto new_variances = egr::EagerAmpAutoCast("variances", variances, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return generate_proposals_ad_func(new_scores, new_bbox_deltas, new_im_shape, new_anchors, new_variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {scores},{bbox_deltas},{im_shape},{anchors},{variances} };
    
    auto op_name = phi::TransToFluidOpName("generate_proposals");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_scores = transformer->TransInTensor("scores", scores);
    auto new_bbox_deltas = transformer->TransInTensor("bbox_deltas", bbox_deltas);
    auto new_im_shape = transformer->TransInTensor("im_shape", im_shape);
    auto new_anchors = transformer->TransInTensor("anchors", anchors);
    auto new_variances = transformer->TransInTensor("variances", variances);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = generate_proposals_ad_func(new_scores, new_bbox_deltas, new_im_shape, new_anchors, new_variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset);

        auto& rpn_rois = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&rpn_rois);
        auto& rpn_roi_probs = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&rpn_roi_probs);
        auto& rpn_rois_num = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&rpn_rois_num);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{rpn_rois, rpn_roi_probs, rpn_rois_num};
  }

  VLOG(5) << "Running C++ API: " << "generate_proposals";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
    const char* TENSOR_BBOX_DELTAS_TEMPLATE = " \n( bbox_deltas , [%s]), ";
    std::string input_bbox_deltas_str = paddle::string::Sprintf(TENSOR_BBOX_DELTAS_TEMPLATE, egr::EagerUtils::TensorStr(bbox_deltas));
    input_str += input_bbox_deltas_str; 
    const char* TENSOR_IM_SHAPE_TEMPLATE = " \n( im_shape , [%s]), ";
    std::string input_im_shape_str = paddle::string::Sprintf(TENSOR_IM_SHAPE_TEMPLATE, egr::EagerUtils::TensorStr(im_shape));
    input_str += input_im_shape_str; 
    const char* TENSOR_ANCHORS_TEMPLATE = " \n( anchors , [%s]), ";
    std::string input_anchors_str = paddle::string::Sprintf(TENSOR_ANCHORS_TEMPLATE, egr::EagerUtils::TensorStr(anchors));
    input_str += input_anchors_str; 
    const char* TENSOR_VARIANCES_TEMPLATE = " \n( variances , [%s]), ";
    std::string input_variances_str = paddle::string::Sprintf(TENSOR_VARIANCES_TEMPLATE, egr::EagerUtils::TensorStr(variances));
    input_str += input_variances_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::generate_proposals(scores, bbox_deltas, im_shape, anchors, variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset);
  // Get Outputs
  auto& rpn_rois = std::get<0>(api_result);
  auto& rpn_roi_probs = std::get<1>(api_result);
  auto& rpn_rois_num = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: generate_proposals";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
    const char* TENSOR_BBOX_DELTAS_TEMPLATE = " \n( bbox_deltas , [%s]), ";
    std::string input_bbox_deltas_str = paddle::string::Sprintf(TENSOR_BBOX_DELTAS_TEMPLATE, egr::EagerUtils::TensorStr(bbox_deltas));
    input_str += input_bbox_deltas_str; 
    const char* TENSOR_IM_SHAPE_TEMPLATE = " \n( im_shape , [%s]), ";
    std::string input_im_shape_str = paddle::string::Sprintf(TENSOR_IM_SHAPE_TEMPLATE, egr::EagerUtils::TensorStr(im_shape));
    input_str += input_im_shape_str; 
    const char* TENSOR_ANCHORS_TEMPLATE = " \n( anchors , [%s]), ";
    std::string input_anchors_str = paddle::string::Sprintf(TENSOR_ANCHORS_TEMPLATE, egr::EagerUtils::TensorStr(anchors));
    input_str += input_anchors_str; 
    const char* TENSOR_VARIANCES_TEMPLATE = " \n( variances , [%s]), ";
    std::string input_variances_str = paddle::string::Sprintf(TENSOR_VARIANCES_TEMPLATE, egr::EagerUtils::TensorStr(variances));
    input_str += input_variances_str; 
    const char* TENSOR_RPN_ROIS_TEMPLATE = " \n( rpn_rois , [%s]), ";
    std::string output_rpn_rois_str = paddle::string::Sprintf(TENSOR_RPN_ROIS_TEMPLATE, egr::EagerUtils::TensorStr(rpn_rois));
    output_str += output_rpn_rois_str; 
    const char* TENSOR_RPN_ROI_PROBS_TEMPLATE = " \n( rpn_roi_probs , [%s]), ";
    std::string output_rpn_roi_probs_str = paddle::string::Sprintf(TENSOR_RPN_ROI_PROBS_TEMPLATE, egr::EagerUtils::TensorStr(rpn_roi_probs));
    output_str += output_rpn_roi_probs_str; 
    const char* TENSOR_RPN_ROIS_NUM_TEMPLATE = " \n( rpn_rois_num , [%s]), ";
    std::string output_rpn_rois_num_str = paddle::string::Sprintf(TENSOR_RPN_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(rpn_rois_num));
    output_str += output_rpn_rois_num_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{rpn_rois, rpn_roi_probs, rpn_rois_num};
}


paddle::experimental::Tensor greater_equal_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "greater_equal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("greater_equal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("greater_equal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return greater_equal_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("greater_equal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = greater_equal_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "greater_equal";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::greater_equal(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: greater_equal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor greater_than_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "greater_than";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("greater_than dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("greater_than");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return greater_than_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("greater_than");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = greater_than_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "greater_than";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::greater_than(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: greater_than";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor group_norm_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& scale, const paddle::optional<paddle::experimental::Tensor>& bias, float epsilon, int groups, std::string data_layout) {
  VLOG(3) << "Running AD API: " << "group_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("group_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("group_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (scale) amp_tensors_vector.push_back({ *scale });
    if (bias) amp_tensors_vector.push_back({ *bias });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_scale = egr::EagerAmpAutoCast("scale", scale, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return group_norm_ad_func(new_x, new_scale, new_bias, epsilon, groups, data_layout);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (scale) tensors_vector.push_back({ *scale });
    if (bias) tensors_vector.push_back({ *bias });

    auto op_name = phi::TransToFluidOpName("group_norm");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor y = group_norm_ad_func(new_x, new_scale, new_bias, epsilon, groups, data_layout);

    transformer -> SetOutTensorLayout(&y);

    // Returns
    return y;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "group_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::group_norm_intermediate(x, scale, bias, epsilon, groups, data_layout);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("group_norm_intermediate", api_result); }

  // Get Outputs
  auto& y = std::get<0>(api_result);
  auto& mean = std::get<1>(api_result);
  auto& variance = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::autograd_meta(&y);
  egr::AutogradMeta* mean_autograd_meta = egr::EagerUtils::autograd_meta(&mean);
  egr::AutogradMeta* variance_autograd_meta = egr::EagerUtils::autograd_meta(&variance);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("group_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,y_autograd_meta,mean_autograd_meta,variance_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<GroupNormGradNode>(new GroupNormGradNode(3, 3));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedata_layout(data_layout);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(scale) grad_node->SetTensorWrapperscale(*scale);
    if(bias) grad_node->SetTensorWrapperbias(*bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    if(scale.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(scale.get_ptr()), 1);
    if(bias.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(bias.get_ptr()), 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(y_autograd_meta, 0);
    }
    if (mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_autograd_meta, 1);
    }
    if (variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_autograd_meta, 2);
    }
    if (y_autograd_meta) {
      egr::EagerUtils::SetHistory(y_autograd_meta, grad_node);
    }
    if (mean_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_autograd_meta, grad_node);
    }
    if (variance_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(y, 0);
    grad_node->SetGradInMeta(mean, 1);
    grad_node->SetGradInMeta(variance, 2);
    egr::EagerUtils::CheckAndRetainGrad(y);
    egr::EagerUtils::CheckAndRetainGrad(mean);
    egr::EagerUtils::CheckAndRetainGrad(variance);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappermean(mean);
    grad_node->SetTensorWrappervariance(variance);
  }


  VLOG(4) << "Finish AD API: group_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string output_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    output_str += output_y_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string output_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    output_str += output_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string output_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    output_str += output_variance_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return y;
}


paddle::experimental::Tensor hardswish_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "hardswish";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("hardswish dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("hardswish");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return hardswish_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("hardswish");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = hardswish_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "hardswish";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::hardswish(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardswish", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("hardswish node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HardswishGradNode>(new HardswishGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(6.0);
    grad_node->SetAttributescale(6.0);
    grad_node->SetAttributeoffset(3.0);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: hardswish";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor hardtanh_ad_func(const paddle::experimental::Tensor& x, float t_min, float t_max) {
  VLOG(3) << "Running AD API: " << "hardtanh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("hardtanh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("hardtanh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return hardtanh_ad_func(new_x, t_min, t_max);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("hardtanh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = hardtanh_ad_func(new_x, t_min, t_max);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "hardtanh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::hardtanh(x, t_min, t_max);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardtanh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("hardtanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HardtanhGradNode>(new HardtanhGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributet_min(t_min);
    grad_node->SetAttributet_max(t_max);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: hardtanh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> hsigmoid_loss_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& label, const paddle::experimental::Tensor& w, const paddle::optional<paddle::experimental::Tensor>& bias, const paddle::optional<paddle::experimental::Tensor>& path, const paddle::optional<paddle::experimental::Tensor>& code, int num_classes, bool remote_prefetch, bool is_sparse) {
  VLOG(3) << "Running AD API: " << "hsigmoid_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("hsigmoid_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("hsigmoid_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{label},{w} };
    if (bias) amp_tensors_vector.push_back({ *bias });
    if (path) amp_tensors_vector.push_back({ *path });
    if (code) amp_tensors_vector.push_back({ *code });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_w = egr::EagerAmpAutoCast("w", w, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);
    auto new_path = egr::EagerAmpAutoCast("path", path, amp_dst_dtype, op_name);
    auto new_code = egr::EagerAmpAutoCast("code", code, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return hsigmoid_loss_ad_func(new_x, new_label, new_w, new_bias, new_path, new_code, num_classes, remote_prefetch, is_sparse);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{label},{w} };
    if (bias) tensors_vector.push_back({ *bias });
    if (path) tensors_vector.push_back({ *path });
    if (code) tensors_vector.push_back({ *code });

    auto op_name = phi::TransToFluidOpName("hsigmoid_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_label = transformer->TransInTensor("label", label);
    auto new_w = transformer->TransInTensor("w", w);
auto new_bias = transformer->TransInTensor("bias", bias);
    auto new_path = transformer->TransInTensor("path", path);
    auto new_code = transformer->TransInTensor("code", code);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = hsigmoid_loss_ad_func(new_x, new_label, new_w, new_bias, new_path, new_code, num_classes, remote_prefetch, is_sparse);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& pre_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&pre_out);
        auto& w_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&w_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, pre_out, w_out};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* w_autograd_meta = egr::EagerUtils::nullable_autograd_meta(w);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "hsigmoid_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_W_TEMPLATE = " \n( w , [%s]), ";
    std::string input_w_str = paddle::string::Sprintf(TENSOR_W_TEMPLATE, egr::EagerUtils::TensorStr(w));
    input_str += input_w_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_PATH_TEMPLATE = " \n( path , [%s]), ";
    std::string input_path_str = paddle::string::Sprintf(TENSOR_PATH_TEMPLATE, egr::EagerUtils::TensorStr(path));
    input_str += input_path_str; 
    const char* TENSOR_CODE_TEMPLATE = " \n( code , [%s]), ";
    std::string input_code_str = paddle::string::Sprintf(TENSOR_CODE_TEMPLATE, egr::EagerUtils::TensorStr(code));
    input_str += input_code_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::hsigmoid_loss(x, label, w, bias, path, code, num_classes, remote_prefetch, is_sparse);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hsigmoid_loss", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& pre_out = std::get<1>(api_result);
  auto& w_out = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* pre_out_autograd_meta = egr::EagerUtils::autograd_meta(&pre_out);
  egr::AutogradMeta* w_out_autograd_meta = egr::EagerUtils::autograd_meta(&w_out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,w_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("hsigmoid_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,pre_out_autograd_meta,w_out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HsigmoidLossGradNode>(new HsigmoidLossGradNode(3, 6));
    // SetAttributes if needed
    grad_node->SetAttributenum_classes(num_classes);
    grad_node->SetAttributeremote_prefetch(remote_prefetch);
    grad_node->SetAttributeis_sparse(is_sparse);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperw(w);
    grad_node->SetTensorWrapperlabel(label);
    if(path) grad_node->SetTensorWrapperpath(*path);
    if(code) grad_node->SetTensorWrappercode(*code);
    if(bias) grad_node->SetTensorWrapperbias(*bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(w, 2);
    if(bias.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(bias.get_ptr()), 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (pre_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(pre_out_autograd_meta, 1);
    }
    if (w_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(w_out_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (pre_out_autograd_meta) {
      egr::EagerUtils::SetHistory(pre_out_autograd_meta, grad_node);
    }
    if (w_out_autograd_meta) {
      egr::EagerUtils::SetHistory(w_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(pre_out, 1);
    grad_node->SetGradInMeta(w_out, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(pre_out);
    egr::EagerUtils::CheckAndRetainGrad(w_out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperpre_out(pre_out);
  }


  VLOG(4) << "Finish AD API: hsigmoid_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_W_TEMPLATE = " \n( w , [%s]), ";
    std::string input_w_str = paddle::string::Sprintf(TENSOR_W_TEMPLATE, egr::EagerUtils::TensorStr(w));
    input_str += input_w_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_PATH_TEMPLATE = " \n( path , [%s]), ";
    std::string input_path_str = paddle::string::Sprintf(TENSOR_PATH_TEMPLATE, egr::EagerUtils::TensorStr(path));
    input_str += input_path_str; 
    const char* TENSOR_CODE_TEMPLATE = " \n( code , [%s]), ";
    std::string input_code_str = paddle::string::Sprintf(TENSOR_CODE_TEMPLATE, egr::EagerUtils::TensorStr(code));
    input_str += input_code_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_PRE_OUT_TEMPLATE = " \n( pre_out , [%s]), ";
    std::string output_pre_out_str = paddle::string::Sprintf(TENSOR_PRE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(pre_out));
    output_str += output_pre_out_str; 
    const char* TENSOR_W_OUT_TEMPLATE = " \n( w_out , [%s]), ";
    std::string output_w_out_str = paddle::string::Sprintf(TENSOR_W_OUT_TEMPLATE, egr::EagerUtils::TensorStr(w_out));
    output_str += output_w_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, pre_out, w_out};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> huber_loss_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& label, float delta) {
  VLOG(3) << "Running AD API: " << "huber_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("huber_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("huber_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return huber_loss_ad_func(new_input, new_label, delta);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{label} };
    
    auto op_name = phi::TransToFluidOpName("huber_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = huber_loss_ad_func(new_input, new_label, delta);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& residual = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&residual);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, residual};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);
  egr::AutogradMeta* label_autograd_meta = egr::EagerUtils::nullable_autograd_meta(label);

  VLOG(5) << "Running C++ API: " << "huber_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::huber_loss(input, label, delta);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("huber_loss", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& residual = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* residual_autograd_meta = egr::EagerUtils::autograd_meta(&residual);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta,label_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("huber_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,residual_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<HuberLossGradNode>(new HuberLossGradNode(2, 2));
    // SetAttributes if needed
    grad_node->SetAttributedelta(delta);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(label, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (residual_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(residual_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (residual_autograd_meta) {
      egr::EagerUtils::SetHistory(residual_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(residual, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(residual);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperresidual(residual);
  }


  VLOG(4) << "Finish AD API: huber_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_RESIDUAL_TEMPLATE = " \n( residual , [%s]), ";
    std::string output_residual_str = paddle::string::Sprintf(TENSOR_RESIDUAL_TEMPLATE, egr::EagerUtils::TensorStr(residual));
    output_str += output_residual_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, residual};
}


paddle::experimental::Tensor imag_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "imag";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("imag dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("imag");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return imag_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("imag");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = imag_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "imag";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::imag(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("imag", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("imag node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ImagGradNode>(new ImagGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: imag";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor increment_ad_func(const paddle::experimental::Tensor& x, float value) {
  VLOG(3) << "Running AD API: " << "increment";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("increment dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("increment");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return increment_ad_func(new_x, value);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("increment");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = increment_ad_func(new_x, value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "increment";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::increment(x, value);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: increment";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& increment__ad_func(paddle::experimental::Tensor& x, float value) {
  VLOG(3) << "Running AD API: " << "increment_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("increment_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for increment__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("increment_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = increment__ad_func(new_x, value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "increment_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto& api_result = paddle::experimental::increment_(x, value);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: increment_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor index_add_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, const paddle::experimental::Tensor& add_value, int axis) {
  VLOG(3) << "Running AD API: " << "index_add";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("index_add dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("index_add");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{index},{add_value} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    auto new_add_value = egr::EagerAmpAutoCast("add_value", add_value, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return index_add_ad_func(new_x, new_index, new_add_value, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index},{add_value} };
    
    auto op_name = phi::TransToFluidOpName("index_add");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);
    auto new_add_value = transformer->TransInTensor("add_value", add_value);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = index_add_ad_func(new_x, new_index, new_add_value, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* add_value_autograd_meta = egr::EagerUtils::nullable_autograd_meta(add_value);

  VLOG(5) << "Running C++ API: " << "index_add";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::index_add(x, index, add_value, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_add", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,add_value_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("index_add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<IndexAddGradNode>(new IndexAddGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindex(index);
    grad_node->SetTensorWrapperadd_value(add_value);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(add_value, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: index_add";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& index_add__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& index, const paddle::experimental::Tensor& add_value, int axis) {
  VLOG(3) << "Running AD API: " << "index_add_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("index_add_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for index_add__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{index},{add_value} };
    
    auto op_name = phi::TransToFluidOpName("index_add_");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_index = transformer->TransInTensor("index", index);
    auto new_add_value = transformer->TransInTensor("add_value", add_value);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = index_add__ad_func(new_x, new_index, new_add_value, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* add_value_autograd_meta = egr::EagerUtils::nullable_autograd_meta(add_value);

  VLOG(5) << "Running C++ API: " << "index_add_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::index_add_(x, index, add_value, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_add_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,add_value_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("index_add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<IndexAddGradNode>(new IndexAddGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindex(index);
    grad_node->SetTensorWrapperadd_value(add_value);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(add_value, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: index_add_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor instance_norm_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& scale, const paddle::optional<paddle::experimental::Tensor>& bias, float epsilon) {
  VLOG(3) << "Running AD API: " << "instance_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("instance_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("instance_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (scale) amp_tensors_vector.push_back({ *scale });
    if (bias) amp_tensors_vector.push_back({ *bias });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_scale = egr::EagerAmpAutoCast("scale", scale, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return instance_norm_ad_func(new_x, new_scale, new_bias, epsilon);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (scale) tensors_vector.push_back({ *scale });
    if (bias) tensors_vector.push_back({ *bias });

    auto op_name = phi::TransToFluidOpName("instance_norm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor y = instance_norm_ad_func(new_x, new_scale, new_bias, epsilon);

    transformer -> SetOutTensorLayout(&y);

    // Returns
    return y;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "instance_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::instance_norm_intermediate(x, scale, bias, epsilon);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("instance_norm_intermediate", api_result); }

  // Get Outputs
  auto& y = std::get<0>(api_result);
  auto& saved_mean = std::get<1>(api_result);
  auto& saved_variance = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::autograd_meta(&y);
  egr::AutogradMeta* saved_mean_autograd_meta = egr::EagerUtils::autograd_meta(&saved_mean);
  egr::AutogradMeta* saved_variance_autograd_meta = egr::EagerUtils::autograd_meta(&saved_variance);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("instance_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,y_autograd_meta,saved_mean_autograd_meta,saved_variance_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<InstanceNormGradNode>(new InstanceNormGradNode(3, 3));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(scale) grad_node->SetTensorWrapperscale(*scale);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    if(scale.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(scale.get_ptr()), 1);
    if(bias.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(bias.get_ptr()), 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(y_autograd_meta, 0);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_mean_autograd_meta, 1);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_variance_autograd_meta, 2);
    }
    if (y_autograd_meta) {
      egr::EagerUtils::SetHistory(y_autograd_meta, grad_node);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_mean_autograd_meta, grad_node);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_variance_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(y, 0);
    grad_node->SetGradInMeta(saved_mean, 1);
    grad_node->SetGradInMeta(saved_variance, 2);
    egr::EagerUtils::CheckAndRetainGrad(y);
    egr::EagerUtils::CheckAndRetainGrad(saved_mean);
    egr::EagerUtils::CheckAndRetainGrad(saved_variance);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
  }


  VLOG(4) << "Finish AD API: instance_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string output_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    output_str += output_y_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string output_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    output_str += output_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string output_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    output_str += output_saved_variance_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return y;
}


paddle::experimental::Tensor kldiv_loss_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& label, std::string reduction) {
  VLOG(3) << "Running AD API: " << "kldiv_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("kldiv_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("kldiv_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return kldiv_loss_ad_func(new_x, new_label, reduction);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{label} };
    
    auto op_name = phi::TransToFluidOpName("kldiv_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = kldiv_loss_ad_func(new_x, new_label, reduction);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "kldiv_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::kldiv_loss(x, label, reduction);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kldiv_loss", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("kldiv_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<KldivLossGradNode>(new KldivLossGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributereduction(reduction);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: kldiv_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor kron_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "kron";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("kron dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("kron");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return kron_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("kron");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = kron_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "kron";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::kron(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kron", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("kron node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<KronGradNode>(new KronGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: kron";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> lamb__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, const paddle::experimental::Tensor& learning_rate, paddle::experimental::Tensor& moment1, paddle::experimental::Tensor& moment2, paddle::experimental::Tensor& beta1_pow, paddle::experimental::Tensor& beta2_pow, paddle::optional<paddle::experimental::Tensor>& master_param, const paddle::optional<paddle::experimental::Tensor>& skip_update, float weight_decay, float beta1, float beta2, float epsilon, bool multi_precision) {
  VLOG(3) << "Running AD API: " << "lamb_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lamb_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for lamb__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{learning_rate},{moment1},{moment2},{beta1_pow},{beta2_pow} };
    if (master_param) tensors_vector.push_back({ *master_param });
    if (skip_update) tensors_vector.push_back({ *skip_update });

    auto op_name = phi::TransToFluidOpName("lamb_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
    auto new_moment1 = transformer->TransInTensor("moment1", moment1);
    auto new_moment2 = transformer->TransInTensor("moment2", moment2);
    auto new_beta1_pow = transformer->TransInTensor("beta1_pow", beta1_pow);
    auto new_beta2_pow = transformer->TransInTensor("beta2_pow", beta2_pow);
auto new_master_param = transformer->TransInTensor("master_param", master_param);
    auto new_skip_update = transformer->TransInTensor("skip_update", skip_update);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = lamb__ad_func(new_param, new_grad, new_learning_rate, new_moment1, new_moment2, new_beta1_pow, new_beta2_pow, new_master_param, new_skip_update, weight_decay, beta1, beta2, epsilon, multi_precision);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment1_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment1_out);
        auto& moment2_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&moment2_out);
        auto& beta1_pow_out = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&beta1_pow_out);
        auto& beta2_pow_out = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&beta2_pow_out);
        auto& master_param_outs = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&master_param_outs);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
  }

  VLOG(5) << "Running C++ API: " << "lamb_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::lamb_(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update, weight_decay, beta1, beta2, epsilon, multi_precision);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment1_out = std::get<1>(api_result);
  auto& moment2_out = std::get<2>(api_result);
  auto& beta1_pow_out = std::get<3>(api_result);
  auto& beta2_pow_out = std::get<4>(api_result);
  auto& master_param_outs = std::get<5>(api_result);

  VLOG(4) << "Finish AD API: lamb_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_SKIP_UPDATE_TEMPLATE = " \n( skip_update , [%s]), ";
    std::string input_skip_update_str = paddle::string::Sprintf(TENSOR_SKIP_UPDATE_TEMPLATE, egr::EagerUtils::TensorStr(skip_update));
    input_str += input_skip_update_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT1_OUT_TEMPLATE = " \n( moment1_out , [%s]), ";
    std::string output_moment1_out_str = paddle::string::Sprintf(TENSOR_MOMENT1_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment1_out));
    output_str += output_moment1_out_str; 
    const char* TENSOR_MOMENT2_OUT_TEMPLATE = " \n( moment2_out , [%s]), ";
    std::string output_moment2_out_str = paddle::string::Sprintf(TENSOR_MOMENT2_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment2_out));
    output_str += output_moment2_out_str; 
    const char* TENSOR_BETA1_POW_OUT_TEMPLATE = " \n( beta1_pow_out , [%s]), ";
    std::string output_beta1_pow_out_str = paddle::string::Sprintf(TENSOR_BETA1_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow_out));
    output_str += output_beta1_pow_out_str; 
    const char* TENSOR_BETA2_POW_OUT_TEMPLATE = " \n( beta2_pow_out , [%s]), ";
    std::string output_beta2_pow_out_str = paddle::string::Sprintf(TENSOR_BETA2_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow_out));
    output_str += output_beta2_pow_out_str; 
    const char* TENSOR_MASTER_PARAM_OUTS_TEMPLATE = " \n( master_param_outs , [%s]), ";
    std::string output_master_param_outs_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUTS_TEMPLATE, egr::EagerUtils::TensorStr(master_param_outs));
    output_str += output_master_param_outs_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_outs};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> layer_norm_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& scale, const paddle::optional<paddle::experimental::Tensor>& bias, float epsilon, int begin_norm_axis) {
  VLOG(3) << "Running AD API: " << "layer_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("layer_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("layer_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (scale) amp_tensors_vector.push_back({ *scale });
    if (bias) amp_tensors_vector.push_back({ *bias });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_scale = egr::EagerAmpAutoCast("scale", scale, amp_dst_dtype, op_name);
    auto new_bias = egr::EagerAmpAutoCast("bias", bias, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return layer_norm_ad_func(new_x, new_scale, new_bias, epsilon, begin_norm_axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (scale) tensors_vector.push_back({ *scale });
    if (bias) tensors_vector.push_back({ *bias });

    auto op_name = phi::TransToFluidOpName("layer_norm");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &begin_norm_axis);
    auto new_x = transformer->TransInTensor("x", x);
auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = layer_norm_ad_func(new_x, new_scale, new_bias, epsilon, begin_norm_axis);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mean = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mean);
        auto& variance = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&variance);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean, variance};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "layer_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::layer_norm(x, scale, bias, epsilon, begin_norm_axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("layer_norm", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mean = std::get<1>(api_result);
  auto& variance = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mean_autograd_meta = egr::EagerUtils::autograd_meta(&mean);
  egr::AutogradMeta* variance_autograd_meta = egr::EagerUtils::autograd_meta(&variance);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("layer_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mean_autograd_meta,variance_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LayerNormGradNode>(new LayerNormGradNode(3, 3));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributebegin_norm_axis(begin_norm_axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(scale) grad_node->SetTensorWrapperscale(*scale);
    if(bias) grad_node->SetTensorWrapperbias(*bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    if(scale.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(scale.get_ptr()), 1);
    if(bias.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(bias.get_ptr()), 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_autograd_meta, 1);
    }
    if (variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mean_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_autograd_meta, grad_node);
    }
    if (variance_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mean, 1);
    grad_node->SetGradInMeta(variance, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mean);
    egr::EagerUtils::CheckAndRetainGrad(variance);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermean(mean);
    grad_node->SetTensorWrappervariance(variance);
  }


  VLOG(4) << "Finish AD API: layer_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string output_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    output_str += output_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string output_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    output_str += output_variance_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean, variance};
}


paddle::experimental::Tensor less_equal_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "less_equal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("less_equal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("less_equal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return less_equal_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("less_equal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = less_equal_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "less_equal";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::less_equal(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: less_equal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor less_than_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "less_than";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("less_than dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("less_than");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return less_than_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("less_than");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = less_than_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "less_than";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::less_than(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: less_than";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor linear_interp_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& out_size, const paddle::optional<std::vector<paddle::experimental::Tensor>>& size_tensor, const paddle::optional<paddle::experimental::Tensor>& scale_tensor, std::string data_layout, int out_d, int out_h, int out_w, std::vector<float> scale, std::string interp_method, bool align_corners, int align_mode) {
  VLOG(3) << "Running AD API: " << "linear_interp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("linear_interp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("linear_interp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (out_size) amp_tensors_vector.push_back({ *out_size });
    if (size_tensor) amp_tensors_vector.push_back( *size_tensor );
    if (scale_tensor) amp_tensors_vector.push_back({ *scale_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_out_size = egr::EagerAmpAutoCast("out_size", out_size, amp_dst_dtype, op_name);
    auto new_size_tensor = egr::EagerAmpAutoCasts("size_tensor", size_tensor, amp_dst_dtype, op_name);
    auto new_scale_tensor = egr::EagerAmpAutoCast("scale_tensor", scale_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return linear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (out_size) tensors_vector.push_back({ *out_size });
    if (scale_tensor) tensors_vector.push_back({ *scale_tensor });

    auto op_name = phi::TransToFluidOpName("linear_interp");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_out_size = transformer->TransInTensor("out_size", out_size);
    auto new_size_tensor = transformer->TransInTensors("size_tensor", size_tensor);
    auto new_scale_tensor = transformer->TransInTensor("scale_tensor", scale_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = linear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "linear_interp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::linear_interp(x, out_size, size_tensor, scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("linear_interp", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("linear_interp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LinearInterpGradNode>(new LinearInterpGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeout_d(out_d);
    grad_node->SetAttributeout_h(out_h);
    grad_node->SetAttributeout_w(out_w);
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributeinterp_method(interp_method);
    grad_node->SetAttributealign_corners(align_corners);
    grad_node->SetAttributealign_mode(align_mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(out_size) grad_node->SetTensorWrapperout_size(*out_size);
    if(size_tensor) grad_node->SetTensorWrappersize_tensor(*size_tensor);
    if(scale_tensor) grad_node->SetTensorWrapperscale_tensor(*scale_tensor);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: linear_interp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor linspace_ad_func(const paddle::experimental::Tensor& start, const paddle::experimental::Tensor& stop, const paddle::experimental::Tensor& number, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "linspace";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("linspace dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("linspace");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {start},{stop},{number} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_start = egr::EagerAmpAutoCast("start", start, amp_dst_dtype, op_name);
    auto new_stop = egr::EagerAmpAutoCast("stop", stop, amp_dst_dtype, op_name);
    auto new_number = egr::EagerAmpAutoCast("number", number, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return linspace_ad_func(new_start, new_stop, new_number, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {start},{stop},{number} };
    
    auto op_name = phi::TransToFluidOpName("linspace");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_start = transformer->TransInTensor("start", start);
    auto new_stop = transformer->TransInTensor("stop", stop);
    auto new_number = transformer->TransInTensor("number", number);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = linspace_ad_func(new_start, new_stop, new_number, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "linspace";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_START_TEMPLATE = " \n( start , [%s]), ";
    std::string input_start_str = paddle::string::Sprintf(TENSOR_START_TEMPLATE, egr::EagerUtils::TensorStr(start));
    input_str += input_start_str; 
    const char* TENSOR_STOP_TEMPLATE = " \n( stop , [%s]), ";
    std::string input_stop_str = paddle::string::Sprintf(TENSOR_STOP_TEMPLATE, egr::EagerUtils::TensorStr(stop));
    input_str += input_stop_str; 
    const char* TENSOR_NUMBER_TEMPLATE = " \n( number , [%s]), ";
    std::string input_number_str = paddle::string::Sprintf(TENSOR_NUMBER_TEMPLATE, egr::EagerUtils::TensorStr(number));
    input_str += input_number_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::linspace(start, stop, number, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: linspace";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_START_TEMPLATE = " \n( start , [%s]), ";
    std::string input_start_str = paddle::string::Sprintf(TENSOR_START_TEMPLATE, egr::EagerUtils::TensorStr(start));
    input_str += input_start_str; 
    const char* TENSOR_STOP_TEMPLATE = " \n( stop , [%s]), ";
    std::string input_stop_str = paddle::string::Sprintf(TENSOR_STOP_TEMPLATE, egr::EagerUtils::TensorStr(stop));
    input_str += input_stop_str; 
    const char* TENSOR_NUMBER_TEMPLATE = " \n( number , [%s]), ";
    std::string input_number_str = paddle::string::Sprintf(TENSOR_NUMBER_TEMPLATE, egr::EagerUtils::TensorStr(number));
    input_str += input_number_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log_softmax_ad_func(const paddle::experimental::Tensor& x, int axis) {
  VLOG(3) << "Running AD API: " << "log_softmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log_softmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log_softmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log_softmax_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log_softmax");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log_softmax_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log_softmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::log_softmax(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_softmax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log_softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogSoftmaxGradNode>(new LogSoftmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: log_softmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logcumsumexp_ad_func(const paddle::experimental::Tensor& x, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(3) << "Running AD API: " << "logcumsumexp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logcumsumexp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logcumsumexp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logcumsumexp_ad_func(new_x, axis, flatten, exclusive, reverse);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("logcumsumexp");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logcumsumexp_ad_func(new_x, axis, flatten, exclusive, reverse);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "logcumsumexp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::logcumsumexp(x, axis, flatten, exclusive, reverse);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logcumsumexp", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("logcumsumexp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogcumsumexpGradNode>(new LogcumsumexpGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributeflatten(flatten);
    grad_node->SetAttributeexclusive(exclusive);
    grad_node->SetAttributereverse(reverse);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: logcumsumexp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logical_and_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "logical_and";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logical_and dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logical_and");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logical_and_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("logical_and");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logical_and_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "logical_and";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::logical_and(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: logical_and";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logical_not_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "logical_not";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logical_not dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logical_not");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logical_not_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("logical_not");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logical_not_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "logical_not";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::logical_not(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: logical_not";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logical_or_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "logical_or";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logical_or dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logical_or");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logical_or_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("logical_or");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logical_or_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "logical_or";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::logical_or(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: logical_or";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logical_xor_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "logical_xor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logical_xor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logical_xor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logical_xor_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("logical_xor");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logical_xor_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "logical_xor";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::logical_xor(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: logical_xor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor logsumexp_ad_func(const paddle::experimental::Tensor& x, std::vector<int64_t> axis, bool keepdim, bool reduce_all) {
  VLOG(3) << "Running AD API: " << "logsumexp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("logsumexp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("logsumexp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return logsumexp_ad_func(new_x, axis, keepdim, reduce_all);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("logsumexp");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = logsumexp_ad_func(new_x, axis, keepdim, reduce_all);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "logsumexp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::logsumexp(x, axis, keepdim, reduce_all);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logsumexp", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("logsumexp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LogsumexpGradNode>(new LogsumexpGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(reduce_all);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: logsumexp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> lstsq_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, paddle::experimental::Scalar rcond, std::string driver) {
  VLOG(3) << "Running AD API: " << "lstsq";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lstsq dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lstsq");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lstsq_ad_func(new_x, new_y, rcond, driver);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("lstsq");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = lstsq_ad_func(new_x, new_y, rcond, driver);

        auto& solution = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&solution);
        auto& residuals = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&residuals);
        auto& rank = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&rank);
        auto& singular_values = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&singular_values);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{solution, residuals, rank, singular_values};
  }

  VLOG(5) << "Running C++ API: " << "lstsq";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::lstsq(x, y, rcond, driver);
  // Get Outputs
  auto& solution = std::get<0>(api_result);
  auto& residuals = std::get<1>(api_result);
  auto& rank = std::get<2>(api_result);
  auto& singular_values = std::get<3>(api_result);

  VLOG(4) << "Finish AD API: lstsq";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SOLUTION_TEMPLATE = " \n( solution , [%s]), ";
    std::string output_solution_str = paddle::string::Sprintf(TENSOR_SOLUTION_TEMPLATE, egr::EagerUtils::TensorStr(solution));
    output_str += output_solution_str; 
    const char* TENSOR_RESIDUALS_TEMPLATE = " \n( residuals , [%s]), ";
    std::string output_residuals_str = paddle::string::Sprintf(TENSOR_RESIDUALS_TEMPLATE, egr::EagerUtils::TensorStr(residuals));
    output_str += output_residuals_str; 
    const char* TENSOR_RANK_TEMPLATE = " \n( rank , [%s]), ";
    std::string output_rank_str = paddle::string::Sprintf(TENSOR_RANK_TEMPLATE, egr::EagerUtils::TensorStr(rank));
    output_str += output_rank_str; 
    const char* TENSOR_SINGULAR_VALUES_TEMPLATE = " \n( singular_values , [%s]), ";
    std::string output_singular_values_str = paddle::string::Sprintf(TENSOR_SINGULAR_VALUES_TEMPLATE, egr::EagerUtils::TensorStr(singular_values));
    output_str += output_singular_values_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{solution, residuals, rank, singular_values};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> lu_ad_func(const paddle::experimental::Tensor& x, bool pivot) {
  VLOG(3) << "Running AD API: " << "lu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lu_ad_func(new_x, pivot);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("lu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = lu_ad_func(new_x, pivot);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& pivots = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&pivots);
        auto& infos = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&infos);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, pivots, infos};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "lu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::lu(x, pivot);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lu", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& pivots = std::get<1>(api_result);
  auto& infos = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* pivots_autograd_meta = egr::EagerUtils::autograd_meta(&pivots);
  egr::AutogradMeta* infos_autograd_meta = egr::EagerUtils::autograd_meta(&infos);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("lu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,pivots_autograd_meta,infos_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LuGradNode>(new LuGradNode(3, 1));
    // SetAttributes if needed
    grad_node->SetAttributepivot(pivot);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (pivots_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(pivots_autograd_meta, 1);
    }
    if (infos_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(infos_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (pivots_autograd_meta) {
      egr::EagerUtils::SetHistory(pivots_autograd_meta, grad_node);
    }
    if (infos_autograd_meta) {
      egr::EagerUtils::SetHistory(infos_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(pivots, 1);
    grad_node->SetGradInMeta(infos, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(pivots);
    egr::EagerUtils::CheckAndRetainGrad(infos);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperpivots(pivots);
  }


  VLOG(4) << "Finish AD API: lu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_PIVOTS_TEMPLATE = " \n( pivots , [%s]), ";
    std::string output_pivots_str = paddle::string::Sprintf(TENSOR_PIVOTS_TEMPLATE, egr::EagerUtils::TensorStr(pivots));
    output_str += output_pivots_str; 
    const char* TENSOR_INFOS_TEMPLATE = " \n( infos , [%s]), ";
    std::string output_infos_str = paddle::string::Sprintf(TENSOR_INFOS_TEMPLATE, egr::EagerUtils::TensorStr(infos));
    output_str += output_infos_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, pivots, infos};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> margin_cross_entropy_ad_func(const paddle::experimental::Tensor& logits, const paddle::experimental::Tensor& label, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale) {
  VLOG(3) << "Running AD API: " << "margin_cross_entropy";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("margin_cross_entropy dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("margin_cross_entropy");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {logits},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_logits = egr::EagerAmpAutoCast("logits", logits, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return margin_cross_entropy_ad_func(new_logits, new_label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {logits},{label} };
    
    auto op_name = phi::TransToFluidOpName("margin_cross_entropy");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_logits = transformer->TransInTensor("logits", logits);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = margin_cross_entropy_ad_func(new_logits, new_label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale);

        auto& softmax = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&softmax);
        auto& loss = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&loss);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{softmax, loss};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* logits_autograd_meta = egr::EagerUtils::nullable_autograd_meta(logits);

  VLOG(5) << "Running C++ API: " << "margin_cross_entropy";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::margin_cross_entropy(logits, label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("margin_cross_entropy", api_result); }

  // Get Outputs
  auto& softmax = std::get<0>(api_result);
  auto& loss = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* softmax_autograd_meta = egr::EagerUtils::autograd_meta(&softmax);
  egr::AutogradMeta* loss_autograd_meta = egr::EagerUtils::autograd_meta(&loss);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,logits_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("margin_cross_entropy node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,softmax_autograd_meta,loss_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MarginCrossEntropyGradNode>(new MarginCrossEntropyGradNode(2, 2));
    // SetAttributes if needed
    grad_node->SetAttributereturn_softmax(return_softmax);
    grad_node->SetAttributering_id(ring_id);
    grad_node->SetAttributerank(rank);
    grad_node->SetAttributenranks(nranks);
    grad_node->SetAttributemargin1(margin1);
    grad_node->SetAttributemargin2(margin2);
    grad_node->SetAttributemargin3(margin3);
    grad_node->SetAttributescale(scale);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperlogits(logits);
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(logits, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(softmax_autograd_meta, 0);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(loss_autograd_meta, 1);
    }
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetHistory(softmax_autograd_meta, grad_node);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetHistory(loss_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(softmax, 0);
    grad_node->SetGradInMeta(loss, 1);
    egr::EagerUtils::CheckAndRetainGrad(softmax);
    egr::EagerUtils::CheckAndRetainGrad(loss);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersoftmax(softmax);
  }


  VLOG(4) << "Finish AD API: margin_cross_entropy";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string output_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    output_str += output_softmax_str; 
    const char* TENSOR_LOSS_TEMPLATE = " \n( loss , [%s]), ";
    std::string output_loss_str = paddle::string::Sprintf(TENSOR_LOSS_TEMPLATE, egr::EagerUtils::TensorStr(loss));
    output_str += output_loss_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{softmax, loss};
}


paddle::experimental::Tensor matmul_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, bool transpose_x, bool transpose_y) {
  VLOG(3) << "Running AD API: " << "matmul";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matmul dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matmul");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matmul_ad_func(new_x, new_y, transpose_x, transpose_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("matmul");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = matmul_ad_func(new_x, new_y, transpose_x, transpose_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "matmul";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::matmul(x, y, transpose_x, transpose_y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("matmul node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MatmulGradNode>(new MatmulGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributetranspose_x(transpose_x);
    grad_node->SetAttributetranspose_y(transpose_y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: matmul";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> matrix_nms_ad_func(const paddle::experimental::Tensor& bboxes, const paddle::experimental::Tensor& scores, float score_threshold, int nms_top_k, int keep_top_k, float post_threshold, bool use_gaussian, float gaussian_sigma, int background_label, bool normalized) {
  VLOG(3) << "Running AD API: " << "matrix_nms";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matrix_nms dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matrix_nms");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {bboxes},{scores} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_bboxes = egr::EagerAmpAutoCast("bboxes", bboxes, amp_dst_dtype, op_name);
    auto new_scores = egr::EagerAmpAutoCast("scores", scores, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matrix_nms_ad_func(new_bboxes, new_scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {bboxes},{scores} };
    
    auto op_name = phi::TransToFluidOpName("matrix_nms");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_bboxes = transformer->TransInTensor("bboxes", bboxes);
    auto new_scores = transformer->TransInTensor("scores", scores);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = matrix_nms_ad_func(new_bboxes, new_scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& index = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&index);
        auto& roisnum = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&roisnum);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, roisnum};
  }

  VLOG(5) << "Running C++ API: " << "matrix_nms";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_BBOXES_TEMPLATE = " \n( bboxes , [%s]), ";
    std::string input_bboxes_str = paddle::string::Sprintf(TENSOR_BBOXES_TEMPLATE, egr::EagerUtils::TensorStr(bboxes));
    input_str += input_bboxes_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::matrix_nms(bboxes, scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& index = std::get<1>(api_result);
  auto& roisnum = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: matrix_nms";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_BBOXES_TEMPLATE = " \n( bboxes , [%s]), ";
    std::string input_bboxes_str = paddle::string::Sprintf(TENSOR_BBOXES_TEMPLATE, egr::EagerUtils::TensorStr(bboxes));
    input_str += input_bboxes_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string output_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    output_str += output_index_str; 
    const char* TENSOR_ROISNUM_TEMPLATE = " \n( roisnum , [%s]), ";
    std::string output_roisnum_str = paddle::string::Sprintf(TENSOR_ROISNUM_TEMPLATE, egr::EagerUtils::TensorStr(roisnum));
    output_str += output_roisnum_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, roisnum};
}


paddle::experimental::Tensor matrix_rank_ad_func(const paddle::experimental::Tensor& x, float tol, bool hermitian, bool use_default_tol) {
  VLOG(3) << "Running AD API: " << "matrix_rank";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matrix_rank dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matrix_rank");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matrix_rank_ad_func(new_x, tol, hermitian, use_default_tol);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("matrix_rank");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = matrix_rank_ad_func(new_x, tol, hermitian, use_default_tol);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "matrix_rank";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::matrix_rank(x, tol, hermitian, use_default_tol);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: matrix_rank";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor matrix_rank_tol_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& atol_tensor, bool use_default_tol, bool hermitian) {
  VLOG(3) << "Running AD API: " << "matrix_rank_tol";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matrix_rank_tol dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matrix_rank_tol");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{atol_tensor} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_atol_tensor = egr::EagerAmpAutoCast("atol_tensor", atol_tensor, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matrix_rank_tol_ad_func(new_x, new_atol_tensor, use_default_tol, hermitian);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{atol_tensor} };
    
    auto op_name = phi::TransToFluidOpName("matrix_rank_tol");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_atol_tensor = transformer->TransInTensor("atol_tensor", atol_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = matrix_rank_tol_ad_func(new_x, new_atol_tensor, use_default_tol, hermitian);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "matrix_rank_tol";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ATOL_TENSOR_TEMPLATE = " \n( atol_tensor , [%s]), ";
    std::string input_atol_tensor_str = paddle::string::Sprintf(TENSOR_ATOL_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(atol_tensor));
    input_str += input_atol_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::matrix_rank_tol(x, atol_tensor, use_default_tol, hermitian);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: matrix_rank_tol";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ATOL_TENSOR_TEMPLATE = " \n( atol_tensor , [%s]), ";
    std::string input_atol_tensor_str = paddle::string::Sprintf(TENSOR_ATOL_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(atol_tensor));
    input_str += input_atol_tensor_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor max_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "max";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("max dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("max");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return max_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("max");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = max_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "max";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::max(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("max node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaxGradNode>(new MaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: max";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> max_pool2d_with_index_ad_func(const paddle::experimental::Tensor& x, std::vector<int> kernel_size, std::vector<int> strides, std::vector<int> paddings, bool global_pooling, bool adaptive) {
  VLOG(3) << "Running AD API: " << "max_pool2d_with_index";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("max_pool2d_with_index dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("max_pool2d_with_index");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return max_pool2d_with_index_ad_func(new_x, kernel_size, strides, paddings, global_pooling, adaptive);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("max_pool2d_with_index");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = max_pool2d_with_index_ad_func(new_x, kernel_size, strides, paddings, global_pooling, adaptive);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mask = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mask);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "max_pool2d_with_index";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::max_pool2d_with_index(x, kernel_size, strides, paddings, global_pooling, adaptive);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max_pool2d_with_index", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mask = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mask_autograd_meta = egr::EagerUtils::autograd_meta(&mask);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("max_pool2d_with_index node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mask_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaxPool2dWithIndexGradNode>(new MaxPool2dWithIndexGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_size(kernel_size);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeglobal_pooling(global_pooling);
    grad_node->SetAttributeadaptive(adaptive);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mask_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetHistory(mask_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mask, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mask);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermask(mask);
  }


  VLOG(4) << "Finish AD API: max_pool2d_with_index";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string output_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    output_str += output_mask_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> max_pool3d_with_index_ad_func(const paddle::experimental::Tensor& x, std::vector<int> kernel_size, std::vector<int> strides, std::vector<int> paddings, bool global_pooling, bool adaptive) {
  VLOG(3) << "Running AD API: " << "max_pool3d_with_index";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("max_pool3d_with_index dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("max_pool3d_with_index");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return max_pool3d_with_index_ad_func(new_x, kernel_size, strides, paddings, global_pooling, adaptive);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("max_pool3d_with_index");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = max_pool3d_with_index_ad_func(new_x, kernel_size, strides, paddings, global_pooling, adaptive);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mask = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mask);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "max_pool3d_with_index";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::max_pool3d_with_index(x, kernel_size, strides, paddings, global_pooling, adaptive);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max_pool3d_with_index", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mask = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mask_autograd_meta = egr::EagerUtils::autograd_meta(&mask);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("max_pool3d_with_index node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mask_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaxPool3dWithIndexGradNode>(new MaxPool3dWithIndexGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_size(kernel_size);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeglobal_pooling(global_pooling);
    grad_node->SetAttributeadaptive(adaptive);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mask_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mask_autograd_meta) {
      egr::EagerUtils::SetHistory(mask_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mask, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mask);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermask(mask);
  }


  VLOG(4) << "Finish AD API: max_pool3d_with_index";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string output_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    output_str += output_mask_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mask};
}


paddle::experimental::Tensor maximum_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "maximum";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("maximum dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("maximum");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return maximum_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("maximum");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = maximum_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "maximum";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::maximum(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maximum", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("maximum node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaximumGradNode>(new MaximumGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: maximum";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor mean_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "mean";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mean dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mean");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mean_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("mean");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = mean_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "mean";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::mean(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mean", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mean node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MeanGradNode>(new MeanGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: mean";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor mean_all_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "mean_all";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mean_all dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mean_all");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mean_all_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("mean_all");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = mean_all_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "mean_all";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::mean_all(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mean_all", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mean_all node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MeanAllGradNode>(new MeanAllGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: mean_all";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor merge_selected_rows_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "merge_selected_rows";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("merge_selected_rows dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("merge_selected_rows");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return merge_selected_rows_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("merge_selected_rows");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = merge_selected_rows_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "merge_selected_rows";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::merge_selected_rows(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: merge_selected_rows";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&> merged_adam__ad_func(std::vector<paddle::experimental::Tensor>& param, const std::vector<paddle::experimental::Tensor>& grad, const std::vector<paddle::experimental::Tensor>& learning_rate, std::vector<paddle::experimental::Tensor>& moment1, std::vector<paddle::experimental::Tensor>& moment2, std::vector<paddle::experimental::Tensor>& beta1_pow, std::vector<paddle::experimental::Tensor>& beta2_pow, paddle::optional<std::vector<paddle::experimental::Tensor>>& master_param, paddle::experimental::Scalar beta1, paddle::experimental::Scalar beta2, paddle::experimental::Scalar epsilon, bool multi_precision, bool use_global_beta_pow) {
  VLOG(3) << "Running AD API: " << "merged_adam_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("merged_adam_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for merged_adam__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { param,grad,learning_rate,moment1,moment2,beta1_pow,beta2_pow };
    
    auto op_name = phi::TransToFluidOpName("merged_adam_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensors("param", param);
    auto new_grad = transformer->TransInTensors("grad", grad);
    auto new_learning_rate = transformer->TransInTensors("learning_rate", learning_rate);
    auto new_moment1 = transformer->TransInTensors("moment1", moment1);
    auto new_moment2 = transformer->TransInTensors("moment2", moment2);
    auto new_beta1_pow = transformer->TransInTensors("beta1_pow", beta1_pow);
    auto new_beta2_pow = transformer->TransInTensors("beta2_pow", beta2_pow);
auto new_master_param = transformer->TransInTensors("master_param", master_param);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&> api_result = merged_adam__ad_func(new_param, new_grad, new_learning_rate, new_moment1, new_moment2, new_beta1_pow, new_beta2_pow, new_master_param, beta1, beta2, epsilon, multi_precision, use_global_beta_pow);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment1_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment1_out);
        auto& moment2_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&moment2_out);
        auto& beta1_pow_out = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&beta1_pow_out);
        auto& beta2_pow_out = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&beta2_pow_out);
        auto& master_param_out = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&master_param_out);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_out};
  }

  VLOG(5) << "Running C++ API: " << "merged_adam_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::merged_adam_(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, beta1, beta2, epsilon, multi_precision, use_global_beta_pow);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment1_out = std::get<1>(api_result);
  auto& moment2_out = std::get<2>(api_result);
  auto& beta1_pow_out = std::get<3>(api_result);
  auto& beta2_pow_out = std::get<4>(api_result);
  auto& master_param_out = std::get<5>(api_result);

  VLOG(4) << "Finish AD API: merged_adam_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MOMENT1_TEMPLATE = " \n( moment1 , [%s]), ";
    std::string input_moment1_str = paddle::string::Sprintf(TENSOR_MOMENT1_TEMPLATE, egr::EagerUtils::TensorStr(moment1));
    input_str += input_moment1_str; 
    const char* TENSOR_MOMENT2_TEMPLATE = " \n( moment2 , [%s]), ";
    std::string input_moment2_str = paddle::string::Sprintf(TENSOR_MOMENT2_TEMPLATE, egr::EagerUtils::TensorStr(moment2));
    input_str += input_moment2_str; 
    const char* TENSOR_BETA1_POW_TEMPLATE = " \n( beta1_pow , [%s]), ";
    std::string input_beta1_pow_str = paddle::string::Sprintf(TENSOR_BETA1_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow));
    input_str += input_beta1_pow_str; 
    const char* TENSOR_BETA2_POW_TEMPLATE = " \n( beta2_pow , [%s]), ";
    std::string input_beta2_pow_str = paddle::string::Sprintf(TENSOR_BETA2_POW_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow));
    input_str += input_beta2_pow_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT1_OUT_TEMPLATE = " \n( moment1_out , [%s]), ";
    std::string output_moment1_out_str = paddle::string::Sprintf(TENSOR_MOMENT1_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment1_out));
    output_str += output_moment1_out_str; 
    const char* TENSOR_MOMENT2_OUT_TEMPLATE = " \n( moment2_out , [%s]), ";
    std::string output_moment2_out_str = paddle::string::Sprintf(TENSOR_MOMENT2_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment2_out));
    output_str += output_moment2_out_str; 
    const char* TENSOR_BETA1_POW_OUT_TEMPLATE = " \n( beta1_pow_out , [%s]), ";
    std::string output_beta1_pow_out_str = paddle::string::Sprintf(TENSOR_BETA1_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta1_pow_out));
    output_str += output_beta1_pow_out_str; 
    const char* TENSOR_BETA2_POW_OUT_TEMPLATE = " \n( beta2_pow_out , [%s]), ";
    std::string output_beta2_pow_out_str = paddle::string::Sprintf(TENSOR_BETA2_POW_OUT_TEMPLATE, egr::EagerUtils::TensorStr(beta2_pow_out));
    output_str += output_beta2_pow_out_str; 
    const char* TENSOR_MASTER_PARAM_OUT_TEMPLATE = " \n( master_param_out , [%s]), ";
    std::string output_master_param_out_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(master_param_out));
    output_str += output_master_param_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&>{param_out, moment1_out, moment2_out, beta1_pow_out, beta2_pow_out, master_param_out};
}


std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&> merged_momentum__ad_func(std::vector<paddle::experimental::Tensor>& param, const std::vector<paddle::experimental::Tensor>& grad, std::vector<paddle::experimental::Tensor>& velocity, const std::vector<paddle::experimental::Tensor>& learning_rate, paddle::optional<std::vector<paddle::experimental::Tensor>>& master_param, float mu, bool use_nesterov, std::vector<std::string> regularization_method, std::vector<float> regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(3) << "Running AD API: " << "merged_momentum_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("merged_momentum_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for merged_momentum__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { param,grad,velocity,learning_rate };
    
    auto op_name = phi::TransToFluidOpName("merged_momentum_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensors("param", param);
    auto new_grad = transformer->TransInTensors("grad", grad);
    auto new_velocity = transformer->TransInTensors("velocity", velocity);
    auto new_learning_rate = transformer->TransInTensors("learning_rate", learning_rate);
auto new_master_param = transformer->TransInTensors("master_param", master_param);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&> api_result = merged_momentum__ad_func(new_param, new_grad, new_velocity, new_learning_rate, new_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& velocity_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&velocity_out);
        auto& master_param_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&master_param_out);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&>{param_out, velocity_out, master_param_out};
  }

  VLOG(5) << "Running C++ API: " << "merged_momentum_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_VELOCITY_TEMPLATE = " \n( velocity , [%s]), ";
    std::string input_velocity_str = paddle::string::Sprintf(TENSOR_VELOCITY_TEMPLATE, egr::EagerUtils::TensorStr(velocity));
    input_str += input_velocity_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::merged_momentum_(param, grad, velocity, learning_rate, master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& velocity_out = std::get<1>(api_result);
  auto& master_param_out = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: merged_momentum_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_VELOCITY_TEMPLATE = " \n( velocity , [%s]), ";
    std::string input_velocity_str = paddle::string::Sprintf(TENSOR_VELOCITY_TEMPLATE, egr::EagerUtils::TensorStr(velocity));
    input_str += input_velocity_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_VELOCITY_OUT_TEMPLATE = " \n( velocity_out , [%s]), ";
    std::string output_velocity_out_str = paddle::string::Sprintf(TENSOR_VELOCITY_OUT_TEMPLATE, egr::EagerUtils::TensorStr(velocity_out));
    output_str += output_velocity_out_str; 
    const char* TENSOR_MASTER_PARAM_OUT_TEMPLATE = " \n( master_param_out , [%s]), ";
    std::string output_master_param_out_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(master_param_out));
    output_str += output_master_param_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>&, std::vector<paddle::experimental::Tensor>&, paddle::optional<std::vector<paddle::experimental::Tensor>>&>{param_out, velocity_out, master_param_out};
}


std::vector<paddle::experimental::Tensor> meshgrid_ad_func(const std::vector<paddle::experimental::Tensor>& inputs) {
  VLOG(3) << "Running AD API: " << "meshgrid";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("meshgrid dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("meshgrid");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { inputs };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_inputs = egr::EagerAmpAutoCasts("inputs", inputs, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return meshgrid_ad_func(new_inputs);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { inputs };
    
    auto op_name = phi::TransToFluidOpName("meshgrid");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_inputs = transformer->TransInTensors("inputs", inputs);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> outputs = meshgrid_ad_func(new_inputs);

    transformer -> SetOutTensorLayout(&outputs);

    // Returns
    return outputs;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> inputs_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(inputs);
  std::vector<egr::AutogradMeta*>* inputs_autograd_meta = &inputs_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "meshgrid";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::meshgrid(inputs);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("meshgrid", api_result); }

  // Get Outputs
  auto& outputs = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> outputs_autograd_meta_vec = egr::EagerUtils::autograd_meta(&outputs);
  std::vector<egr::AutogradMeta*>* outputs_autograd_meta = &outputs_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,inputs_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("meshgrid node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,outputs_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MeshgridGradNode>(new MeshgridGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinputs(inputs);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(inputs, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (outputs_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(outputs_autograd_meta, 0);
    }
    if (outputs_autograd_meta) {
      egr::EagerUtils::SetHistory(outputs_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(outputs, 0);
    egr::EagerUtils::CheckAndRetainGrad(outputs);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: meshgrid";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_OUTPUTS_TEMPLATE = " \n( outputs , [%s]), ";
    std::string output_outputs_str = paddle::string::Sprintf(TENSOR_OUTPUTS_TEMPLATE, egr::EagerUtils::TensorStr(outputs));
    output_str += output_outputs_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return outputs;
}


paddle::experimental::Tensor min_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis, bool keepdim) {
  VLOG(3) << "Running AD API: " << "min";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("min dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("min");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return min_ad_func(new_x, axis, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("min");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = min_ad_func(new_x, axis, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "min";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::min(x, axis, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("min", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("min node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MinGradNode>(new MinGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: min";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor minimum_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "minimum";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("minimum dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("minimum");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return minimum_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("minimum");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = minimum_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "minimum";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::minimum(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("minimum", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("minimum node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MinimumGradNode>(new MinimumGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: minimum";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor mish_ad_func(const paddle::experimental::Tensor& x, float threshold) {
  VLOG(3) << "Running AD API: " << "mish";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mish dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mish");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mish_ad_func(new_x, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("mish");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = mish_ad_func(new_x, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "mish";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::mish(x, threshold);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mish", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mish node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MishGradNode>(new MishGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(threshold);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: mish";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> momentum__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& grad, paddle::experimental::Tensor& velocity, const paddle::experimental::Tensor& learning_rate, paddle::optional<paddle::experimental::Tensor>& master_param, float mu, bool use_nesterov, std::string regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(3) << "Running AD API: " << "momentum_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("momentum_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for momentum__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{grad},{velocity},{learning_rate} };
    if (master_param) tensors_vector.push_back({ *master_param });

    auto op_name = phi::TransToFluidOpName("momentum_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_velocity = transformer->TransInTensor("velocity", velocity);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
auto new_master_param = transformer->TransInTensor("master_param", master_param);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = momentum__ad_func(new_param, new_grad, new_velocity, new_learning_rate, new_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& velocity_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&velocity_out);
        auto& master_param_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&master_param_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, velocity_out, master_param_out};
  }

  VLOG(5) << "Running C++ API: " << "momentum_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_VELOCITY_TEMPLATE = " \n( velocity , [%s]), ";
    std::string input_velocity_str = paddle::string::Sprintf(TENSOR_VELOCITY_TEMPLATE, egr::EagerUtils::TensorStr(velocity));
    input_str += input_velocity_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::momentum_(param, grad, velocity, learning_rate, master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& velocity_out = std::get<1>(api_result);
  auto& master_param_out = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: momentum_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_VELOCITY_TEMPLATE = " \n( velocity , [%s]), ";
    std::string input_velocity_str = paddle::string::Sprintf(TENSOR_VELOCITY_TEMPLATE, egr::EagerUtils::TensorStr(velocity));
    input_str += input_velocity_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_VELOCITY_OUT_TEMPLATE = " \n( velocity_out , [%s]), ";
    std::string output_velocity_out_str = paddle::string::Sprintf(TENSOR_VELOCITY_OUT_TEMPLATE, egr::EagerUtils::TensorStr(velocity_out));
    output_str += output_velocity_out_str; 
    const char* TENSOR_MASTER_PARAM_OUT_TEMPLATE = " \n( master_param_out , [%s]), ";
    std::string output_master_param_out_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(master_param_out));
    output_str += output_master_param_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, velocity_out, master_param_out};
}


paddle::experimental::Tensor multi_dot_ad_func(const std::vector<paddle::experimental::Tensor>& x) {
  VLOG(3) << "Running AD API: " << "multi_dot";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multi_dot dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multi_dot");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { x };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCasts("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multi_dot_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x };
    
    auto op_name = phi::TransToFluidOpName("multi_dot");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensors("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = multi_dot_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> x_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(x);
  std::vector<egr::AutogradMeta*>* x_autograd_meta = &x_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "multi_dot";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::multi_dot(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multi_dot", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("multi_dot node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiDotGradNode>(new MultiDotGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: multi_dot";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> multiclass_nms3_ad_func(const paddle::experimental::Tensor& bboxes, const paddle::experimental::Tensor& scores, const paddle::optional<paddle::experimental::Tensor>& rois_num, float score_threshold, int nms_top_k, int keep_top_k, float nms_threshold, bool normalized, float nms_eta, int background_label) {
  VLOG(3) << "Running AD API: " << "multiclass_nms3";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multiclass_nms3 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multiclass_nms3");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {bboxes},{scores} };
    if (rois_num) amp_tensors_vector.push_back({ *rois_num });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_bboxes = egr::EagerAmpAutoCast("bboxes", bboxes, amp_dst_dtype, op_name);
    auto new_scores = egr::EagerAmpAutoCast("scores", scores, amp_dst_dtype, op_name);
    auto new_rois_num = egr::EagerAmpAutoCast("rois_num", rois_num, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multiclass_nms3_ad_func(new_bboxes, new_scores, new_rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {bboxes},{scores} };
    if (rois_num) tensors_vector.push_back({ *rois_num });

    auto op_name = phi::TransToFluidOpName("multiclass_nms3");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_bboxes = transformer->TransInTensor("bboxes", bboxes);
    auto new_scores = transformer->TransInTensor("scores", scores);
auto new_rois_num = transformer->TransInTensor("rois_num", rois_num);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = multiclass_nms3_ad_func(new_bboxes, new_scores, new_rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& index = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&index);
        auto& nms_rois_num = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&nms_rois_num);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, nms_rois_num};
  }

  VLOG(5) << "Running C++ API: " << "multiclass_nms3";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_BBOXES_TEMPLATE = " \n( bboxes , [%s]), ";
    std::string input_bboxes_str = paddle::string::Sprintf(TENSOR_BBOXES_TEMPLATE, egr::EagerUtils::TensorStr(bboxes));
    input_str += input_bboxes_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
    const char* TENSOR_ROIS_NUM_TEMPLATE = " \n( rois_num , [%s]), ";
    std::string input_rois_num_str = paddle::string::Sprintf(TENSOR_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(rois_num));
    input_str += input_rois_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::multiclass_nms3(bboxes, scores, rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& index = std::get<1>(api_result);
  auto& nms_rois_num = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: multiclass_nms3";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_BBOXES_TEMPLATE = " \n( bboxes , [%s]), ";
    std::string input_bboxes_str = paddle::string::Sprintf(TENSOR_BBOXES_TEMPLATE, egr::EagerUtils::TensorStr(bboxes));
    input_str += input_bboxes_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string input_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    input_str += input_scores_str; 
    const char* TENSOR_ROIS_NUM_TEMPLATE = " \n( rois_num , [%s]), ";
    std::string input_rois_num_str = paddle::string::Sprintf(TENSOR_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(rois_num));
    input_str += input_rois_num_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string output_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    output_str += output_index_str; 
    const char* TENSOR_NMS_ROIS_NUM_TEMPLATE = " \n( nms_rois_num , [%s]), ";
    std::string output_nms_rois_num_str = paddle::string::Sprintf(TENSOR_NMS_ROIS_NUM_TEMPLATE, egr::EagerUtils::TensorStr(nms_rois_num));
    output_str += output_nms_rois_num_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, nms_rois_num};
}


paddle::experimental::Tensor multiplex_ad_func(const std::vector<paddle::experimental::Tensor>& inputs, const paddle::experimental::Tensor& index) {
  VLOG(3) << "Running AD API: " << "multiplex";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multiplex dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multiplex");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { inputs,{index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_inputs = egr::EagerAmpAutoCasts("inputs", inputs, amp_dst_dtype, op_name);
    auto new_index = egr::EagerAmpAutoCast("index", index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multiplex_ad_func(new_inputs, new_index);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { inputs,{index} };
    
    auto op_name = phi::TransToFluidOpName("multiplex");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_inputs = transformer->TransInTensors("inputs", inputs);
    auto new_index = transformer->TransInTensor("index", index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = multiplex_ad_func(new_inputs, new_index);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> inputs_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(inputs);
  std::vector<egr::AutogradMeta*>* inputs_autograd_meta = &inputs_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "multiplex";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::multiplex(inputs, index);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiplex", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,inputs_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("multiplex node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiplexGradNode>(new MultiplexGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinputs(inputs);
    grad_node->SetTensorWrapperindex(index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(inputs, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: multiplex";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor multiply_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "multiply";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multiply dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multiply");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multiply_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("multiply");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = multiply_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "multiply";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::multiply(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("multiply node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiplyGradNode>(new MultiplyGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: multiply";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor nearest_interp_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& out_size, const paddle::optional<std::vector<paddle::experimental::Tensor>>& size_tensor, const paddle::optional<paddle::experimental::Tensor>& scale_tensor, std::string data_layout, int out_d, int out_h, int out_w, std::vector<float> scale, std::string interp_method, bool align_corners, int align_mode) {
  VLOG(3) << "Running AD API: " << "nearest_interp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("nearest_interp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("nearest_interp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (out_size) amp_tensors_vector.push_back({ *out_size });
    if (size_tensor) amp_tensors_vector.push_back( *size_tensor );
    if (scale_tensor) amp_tensors_vector.push_back({ *scale_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_out_size = egr::EagerAmpAutoCast("out_size", out_size, amp_dst_dtype, op_name);
    auto new_size_tensor = egr::EagerAmpAutoCasts("size_tensor", size_tensor, amp_dst_dtype, op_name);
    auto new_scale_tensor = egr::EagerAmpAutoCast("scale_tensor", scale_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return nearest_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (out_size) tensors_vector.push_back({ *out_size });
    if (scale_tensor) tensors_vector.push_back({ *scale_tensor });

    auto op_name = phi::TransToFluidOpName("nearest_interp");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_out_size = transformer->TransInTensor("out_size", out_size);
    auto new_size_tensor = transformer->TransInTensors("size_tensor", size_tensor);
    auto new_scale_tensor = transformer->TransInTensor("scale_tensor", scale_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = nearest_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "nearest_interp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::nearest_interp(x, out_size, size_tensor, scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("nearest_interp", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("nearest_interp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<NearestInterpGradNode>(new NearestInterpGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeout_d(out_d);
    grad_node->SetAttributeout_h(out_h);
    grad_node->SetAttributeout_w(out_w);
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributeinterp_method(interp_method);
    grad_node->SetAttributealign_corners(align_corners);
    grad_node->SetAttributealign_mode(align_mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(out_size) grad_node->SetTensorWrapperout_size(*out_size);
    if(size_tensor) grad_node->SetTensorWrappersize_tensor(*size_tensor);
    if(scale_tensor) grad_node->SetTensorWrapperscale_tensor(*scale_tensor);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: nearest_interp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor nms_ad_func(const paddle::experimental::Tensor& x, float threshold) {
  VLOG(3) << "Running AD API: " << "nms";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("nms dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("nms");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return nms_ad_func(new_x, threshold);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("nms");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = nms_ad_func(new_x, threshold);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "nms";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::nms(x, threshold);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: nms";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor nonzero_ad_func(const paddle::experimental::Tensor& condition) {
  VLOG(3) << "Running AD API: " << "nonzero";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("nonzero dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("nonzero");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {condition} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_condition = egr::EagerAmpAutoCast("condition", condition, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return nonzero_ad_func(new_condition);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {condition} };
    
    auto op_name = phi::TransToFluidOpName("nonzero");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_condition = transformer->TransInTensor("condition", condition);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = nonzero_ad_func(new_condition);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "nonzero";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::nonzero(condition);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: nonzero";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> norm_ad_func(const paddle::experimental::Tensor& x, int axis, float epsilon, bool is_test) {
  VLOG(3) << "Running AD API: " << "norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return norm_ad_func(new_x, axis, epsilon, is_test);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("norm");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = norm_ad_func(new_x, axis, epsilon, is_test);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& norm = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&norm);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, norm};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::norm(x, axis, epsilon, is_test);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("norm", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& norm = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* norm_autograd_meta = egr::EagerUtils::autograd_meta(&norm);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,norm_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<NormGradNode>(new NormGradNode(2, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributeis_test(is_test);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (norm_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(norm_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (norm_autograd_meta) {
      egr::EagerUtils::SetHistory(norm_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(norm, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(norm);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappernorm(norm);
  }


  VLOG(4) << "Finish AD API: norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_NORM_TEMPLATE = " \n( norm , [%s]), ";
    std::string output_norm_str = paddle::string::Sprintf(TENSOR_NORM_TEMPLATE, egr::EagerUtils::TensorStr(norm));
    output_str += output_norm_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, norm};
}


paddle::experimental::Tensor not_equal_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "not_equal";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("not_equal dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("not_equal");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return not_equal_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("not_equal");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = not_equal_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "not_equal";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::not_equal(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: not_equal";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor numel_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "numel";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("numel dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("numel");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return numel_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("numel");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor size = numel_ad_func(new_x);

    transformer -> SetOutTensorLayout(&size);

    // Returns
    return size;
  }

  VLOG(5) << "Running C++ API: " << "numel";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::numel(x);
  // Get Outputs
  auto& size = api_result;

  VLOG(4) << "Finish AD API: numel";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SIZE_TEMPLATE = " \n( size , [%s]), ";
    std::string output_size_str = paddle::string::Sprintf(TENSOR_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(size));
    output_str += output_size_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return size;
}


paddle::experimental::Tensor one_hot_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar num_classes) {
  VLOG(3) << "Running AD API: " << "one_hot";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("one_hot dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("one_hot");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return one_hot_ad_func(new_x, num_classes);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("one_hot");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = one_hot_ad_func(new_x, num_classes);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "one_hot";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::one_hot(x, num_classes);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: one_hot";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor ones_ad_func(paddle::experimental::IntArray shape, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "ones";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("ones dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for ones_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "ones";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::ones(shape, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: ones";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor ones_like_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "ones_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("ones_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("ones_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return ones_like_ad_func(new_x, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("ones_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = ones_like_ad_func(new_x, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "ones_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::ones_like(x, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: ones_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor p_norm_ad_func(const paddle::experimental::Tensor& x, float porder, int axis, float epsilon, bool keepdim, bool asvector) {
  VLOG(3) << "Running AD API: " << "p_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("p_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("p_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return p_norm_ad_func(new_x, porder, axis, epsilon, keepdim, asvector);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("p_norm");
    auto transformer = egr::EagerLayoutAutotune<int, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = p_norm_ad_func(new_x, porder, axis, epsilon, keepdim, asvector);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "p_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::p_norm(x, porder, axis, epsilon, keepdim, asvector);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("p_norm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("p_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PNormGradNode>(new PNormGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeporder(porder);
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributeasvector(asvector);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: p_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pad_ad_func(const paddle::experimental::Tensor& x, std::vector<int> paddings, paddle::experimental::Scalar pad_value) {
  VLOG(3) << "Running AD API: " << "pad";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pad dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pad");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pad_ad_func(new_x, paddings, pad_value);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pad");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pad_ad_func(new_x, paddings, pad_value);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pad";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pad(x, paddings, pad_value);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PadGradNode>(new PadGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepad_value(pad_value);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: pad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pad3d_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray paddings, std::string mode, float pad_value, std::string data_format) {
  VLOG(3) << "Running AD API: " << "pad3d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pad3d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pad3d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pad3d_ad_func(new_x, paddings, mode, pad_value, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pad3d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pad3d_ad_func(new_x, paddings, mode, pad_value, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pad3d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pad3d(x, paddings, mode, pad_value, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad3d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pad3d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Pad3dGradNode>(new Pad3dGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributemode(mode);
    grad_node->SetAttributepad_value(pad_value);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: pad3d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pool2d_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray kernel_size, std::vector<int> strides, std::vector<int> paddings, bool ceil_mode, bool exclusive, std::string data_format, std::string pooling_type, bool global_pooling, bool adaptive, std::string padding_algorithm) {
  VLOG(3) << "Running AD API: " << "pool2d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pool2d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pool2d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pool2d_ad_func(new_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pool2d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pool2d_ad_func(new_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pool2d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pool2d(x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pool2d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pool2d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Pool2dGradNode>(new Pool2dGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_size(kernel_size);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeceil_mode(ceil_mode);
    grad_node->SetAttributeexclusive(exclusive);
    grad_node->SetAttributedata_format(data_format);
    grad_node->SetAttributepooling_type(pooling_type);
    grad_node->SetAttributeglobal_pooling(global_pooling);
    grad_node->SetAttributeadaptive(adaptive);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: pool2d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pool3d_ad_func(const paddle::experimental::Tensor& x, std::vector<int> kernel_size, std::vector<int> strides, std::vector<int> paddings, bool ceil_mode, bool exclusive, std::string data_format, std::string pooling_type, bool global_pooling, bool adaptive, std::string padding_algorithm) {
  VLOG(3) << "Running AD API: " << "pool3d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pool3d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pool3d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pool3d_ad_func(new_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pool3d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pool3d_ad_func(new_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pool3d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pool3d(x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pool3d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pool3d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Pool3dGradNode>(new Pool3dGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_size(kernel_size);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeceil_mode(ceil_mode);
    grad_node->SetAttributeexclusive(exclusive);
    grad_node->SetAttributedata_format(data_format);
    grad_node->SetAttributepooling_type(pooling_type);
    grad_node->SetAttributeglobal_pooling(global_pooling);
    grad_node->SetAttributeadaptive(adaptive);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: pool3d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pow_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar y) {
  VLOG(3) << "Running AD API: " << "pow";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pow dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pow");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pow_ad_func(new_x, y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pow");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pow_ad_func(new_x, y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pow";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::pow(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pow node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PowGradNode>(new PowGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributey(y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: pow";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor prelu_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& alpha, std::string data_format, std::string mode) {
  VLOG(3) << "Running AD API: " << "prelu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("prelu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("prelu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{alpha} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_alpha = egr::EagerAmpAutoCast("alpha", alpha, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return prelu_ad_func(new_x, new_alpha, data_format, mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{alpha} };
    
    auto op_name = phi::TransToFluidOpName("prelu");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_alpha = transformer->TransInTensor("alpha", alpha);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = prelu_ad_func(new_x, new_alpha, data_format, mode);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* alpha_autograd_meta = egr::EagerUtils::nullable_autograd_meta(alpha);

  VLOG(5) << "Running C++ API: " << "prelu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::prelu(x, alpha, data_format, mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("prelu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,alpha_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("prelu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PreluGradNode>(new PreluGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributedata_format(data_format);
    grad_node->SetAttributemode(mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperalpha(alpha);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(alpha, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: prelu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> prior_box_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& image, std::vector<float> min_sizes, std::vector<float> aspect_ratios, std::vector<float> variances, std::vector<float> max_sizes, bool flip, bool clip, float step_w, float step_h, float offset, bool min_max_aspect_ratios_order) {
  VLOG(3) << "Running AD API: " << "prior_box";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("prior_box dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("prior_box");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{image} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_image = egr::EagerAmpAutoCast("image", image, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return prior_box_ad_func(new_input, new_image, min_sizes, aspect_ratios, variances, max_sizes, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{image} };
    
    auto op_name = phi::TransToFluidOpName("prior_box");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_image = transformer->TransInTensor("image", image);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = prior_box_ad_func(new_input, new_image, min_sizes, aspect_ratios, variances, max_sizes, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& var = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&var);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, var};
  }

  VLOG(5) << "Running C++ API: " << "prior_box";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_IMAGE_TEMPLATE = " \n( image , [%s]), ";
    std::string input_image_str = paddle::string::Sprintf(TENSOR_IMAGE_TEMPLATE, egr::EagerUtils::TensorStr(image));
    input_str += input_image_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::prior_box(input, image, min_sizes, aspect_ratios, variances, max_sizes, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& var = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: prior_box";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_IMAGE_TEMPLATE = " \n( image , [%s]), ";
    std::string input_image_str = paddle::string::Sprintf(TENSOR_IMAGE_TEMPLATE, egr::EagerUtils::TensorStr(image));
    input_str += input_image_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_VAR_TEMPLATE = " \n( var , [%s]), ";
    std::string output_var_str = paddle::string::Sprintf(TENSOR_VAR_TEMPLATE, egr::EagerUtils::TensorStr(var));
    output_str += output_var_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, var};
}


paddle::experimental::Tensor prod_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray dims, bool keep_dim, bool reduce_all) {
  VLOG(3) << "Running AD API: " << "prod";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("prod dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("prod");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return prod_ad_func(new_x, dims, keep_dim, reduce_all);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("prod");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray, bool>(op_name, tensors_vector, &dims, &keep_dim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = prod_ad_func(new_x, dims, keep_dim, reduce_all);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "prod";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::prod(x, dims, keep_dim, reduce_all);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("prod", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("prod node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ProdGradNode>(new ProdGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributedims(dims);
    grad_node->SetAttributekeep_dim(keep_dim);
    grad_node->SetAttributereduce_all(reduce_all);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: prod";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor psroi_pool_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& boxes, const paddle::optional<paddle::experimental::Tensor>& boxes_num, int pooled_height, int pooled_width, int output_channels, float spatial_scale) {
  VLOG(3) << "Running AD API: " << "psroi_pool";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("psroi_pool dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("psroi_pool");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{boxes} };
    if (boxes_num) amp_tensors_vector.push_back({ *boxes_num });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_boxes = egr::EagerAmpAutoCast("boxes", boxes, amp_dst_dtype, op_name);
    auto new_boxes_num = egr::EagerAmpAutoCast("boxes_num", boxes_num, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return psroi_pool_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, output_channels, spatial_scale);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{boxes} };
    if (boxes_num) tensors_vector.push_back({ *boxes_num });

    auto op_name = phi::TransToFluidOpName("psroi_pool");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_boxes = transformer->TransInTensor("boxes", boxes);
auto new_boxes_num = transformer->TransInTensor("boxes_num", boxes_num);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = psroi_pool_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, output_channels, spatial_scale);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "psroi_pool";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::psroi_pool(x, boxes, boxes_num, pooled_height, pooled_width, output_channels, spatial_scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("psroi_pool", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("psroi_pool node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PsroiPoolGradNode>(new PsroiPoolGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributepooled_height(pooled_height);
    grad_node->SetAttributepooled_width(pooled_width);
    grad_node->SetAttributeoutput_channels(output_channels);
    grad_node->SetAttributespatial_scale(spatial_scale);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperboxes(boxes);
    if(boxes_num) grad_node->SetTensorWrapperboxes_num(*boxes_num);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: psroi_pool";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor randint_ad_func(int low, int high, paddle::experimental::IntArray shape, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "randint";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("randint dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for randint_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "randint";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::randint(low, high, shape, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: randint";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor randperm_ad_func(int n, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "randperm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("randperm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for randperm_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "randperm";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::randperm(n, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: randperm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor real_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "real";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("real dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("real");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return real_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("real");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = real_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "real";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::real(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("real", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("real node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RealGradNode>(new RealGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: real";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor relu6_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "relu6";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("relu6 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("relu6");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return relu6_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("relu6");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = relu6_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "relu6";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::relu6(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu6", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("relu6 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Relu6GradNode>(new Relu6GradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(6);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: relu6";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor remainder_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "remainder";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("remainder dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("remainder");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return remainder_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("remainder");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = remainder_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "remainder";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::remainder(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: remainder";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& remainder__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "remainder_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("remainder_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for remainder__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("remainder_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = remainder__ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "remainder_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto& api_result = paddle::experimental::remainder_(x, y);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: remainder_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor repeat_interleave_ad_func(const paddle::experimental::Tensor& x, int repeats, int axis) {
  VLOG(3) << "Running AD API: " << "repeat_interleave";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("repeat_interleave dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("repeat_interleave");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return repeat_interleave_ad_func(new_x, repeats, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("repeat_interleave");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = repeat_interleave_ad_func(new_x, repeats, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "repeat_interleave";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::repeat_interleave(x, repeats, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("repeat_interleave", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("repeat_interleave node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RepeatInterleaveGradNode>(new RepeatInterleaveGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributerepeats(repeats);
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: repeat_interleave";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor repeat_interleave_with_tensor_index_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& repeats, int axis) {
  VLOG(3) << "Running AD API: " << "repeat_interleave_with_tensor_index";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("repeat_interleave_with_tensor_index dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("repeat_interleave_with_tensor_index");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{repeats} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_repeats = egr::EagerAmpAutoCast("repeats", repeats, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return repeat_interleave_with_tensor_index_ad_func(new_x, new_repeats, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{repeats} };
    
    auto op_name = phi::TransToFluidOpName("repeat_interleave_with_tensor_index");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_repeats = transformer->TransInTensor("repeats", repeats);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = repeat_interleave_with_tensor_index_ad_func(new_x, new_repeats, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "repeat_interleave_with_tensor_index";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_REPEATS_TEMPLATE = " \n( repeats , [%s]), ";
    std::string input_repeats_str = paddle::string::Sprintf(TENSOR_REPEATS_TEMPLATE, egr::EagerUtils::TensorStr(repeats));
    input_str += input_repeats_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::repeat_interleave_with_tensor_index(x, repeats, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("repeat_interleave_with_tensor_index", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("repeat_interleave_with_tensor_index node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RepeatInterleaveWithTensorIndexGradNode>(new RepeatInterleaveWithTensorIndexGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperrepeats(repeats);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: repeat_interleave_with_tensor_index";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_REPEATS_TEMPLATE = " \n( repeats , [%s]), ";
    std::string input_repeats_str = paddle::string::Sprintf(TENSOR_REPEATS_TEMPLATE, egr::EagerUtils::TensorStr(repeats));
    input_str += input_repeats_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor reshape_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray shape) {
  VLOG(3) << "Running AD API: " << "reshape";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reshape dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("reshape");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return reshape_ad_func(new_x, shape);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reshape");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = reshape_ad_func(new_x, shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reshape";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::reshape_intermediate(x, shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reshape node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReshapeGradNode>(new ReshapeGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: reshape";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& reshape__ad_func(paddle::experimental::Tensor& x, paddle::experimental::IntArray shape) {
  VLOG(3) << "Running AD API: " << "reshape_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reshape_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for reshape__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reshape_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = reshape__ad_func(new_x, shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reshape_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::reshape_intermediate_(x, shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape_intermediate_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& xshape = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* xshape_autograd_meta = egr::EagerUtils::autograd_meta(&xshape);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reshape node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,xshape_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReshapeGradNode>(new ReshapeGradNode(2, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(xshape_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (xshape_autograd_meta) {
      egr::EagerUtils::SetHistory(xshape_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(xshape, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(xshape);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperxshape(xshape);
  }


  VLOG(4) << "Finish AD API: reshape_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string output_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    output_str += output_xshape_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor reverse_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis) {
  VLOG(3) << "Running AD API: " << "reverse";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reverse dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("reverse");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return reverse_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reverse");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = reverse_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reverse";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::reverse(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reverse", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reverse node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReverseGradNode>(new ReverseGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: reverse";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> rmsprop__ad_func(paddle::experimental::Tensor& param, paddle::experimental::Tensor& mean_square, const paddle::experimental::Tensor& grad, paddle::experimental::Tensor& moment, const paddle::experimental::Tensor& learning_rate, paddle::optional<paddle::experimental::Tensor>& mean_grad, float epsilon, float decay, float momentum, bool centered) {
  VLOG(3) << "Running AD API: " << "rmsprop_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("rmsprop_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for rmsprop__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{mean_square},{grad},{moment},{learning_rate} };
    if (mean_grad) tensors_vector.push_back({ *mean_grad });

    auto op_name = phi::TransToFluidOpName("rmsprop_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_mean_square = transformer->TransInTensor("mean_square", mean_square);
    auto new_grad = transformer->TransInTensor("grad", grad);
    auto new_moment = transformer->TransInTensor("moment", moment);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
auto new_mean_grad = transformer->TransInTensor("mean_grad", mean_grad);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = rmsprop__ad_func(new_param, new_mean_square, new_grad, new_moment, new_learning_rate, new_mean_grad, epsilon, decay, momentum, centered);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& moment_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&moment_out);
        auto& mean_square_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&mean_square_out);
        auto& mean_grad_out = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&mean_grad_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment_out, mean_square_out, mean_grad_out};
  }

  VLOG(5) << "Running C++ API: " << "rmsprop_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_MEAN_SQUARE_TEMPLATE = " \n( mean_square , [%s]), ";
    std::string input_mean_square_str = paddle::string::Sprintf(TENSOR_MEAN_SQUARE_TEMPLATE, egr::EagerUtils::TensorStr(mean_square));
    input_str += input_mean_square_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MEAN_GRAD_TEMPLATE = " \n( mean_grad , [%s]), ";
    std::string input_mean_grad_str = paddle::string::Sprintf(TENSOR_MEAN_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(mean_grad));
    input_str += input_mean_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::rmsprop_(param, mean_square, grad, moment, learning_rate, mean_grad, epsilon, decay, momentum, centered);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& moment_out = std::get<1>(api_result);
  auto& mean_square_out = std::get<2>(api_result);
  auto& mean_grad_out = std::get<3>(api_result);

  VLOG(4) << "Finish AD API: rmsprop_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_MEAN_SQUARE_TEMPLATE = " \n( mean_square , [%s]), ";
    std::string input_mean_square_str = paddle::string::Sprintf(TENSOR_MEAN_SQUARE_TEMPLATE, egr::EagerUtils::TensorStr(mean_square));
    input_str += input_mean_square_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MOMENT_TEMPLATE = " \n( moment , [%s]), ";
    std::string input_moment_str = paddle::string::Sprintf(TENSOR_MOMENT_TEMPLATE, egr::EagerUtils::TensorStr(moment));
    input_str += input_moment_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_MEAN_GRAD_TEMPLATE = " \n( mean_grad , [%s]), ";
    std::string input_mean_grad_str = paddle::string::Sprintf(TENSOR_MEAN_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(mean_grad));
    input_str += input_mean_grad_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MOMENT_OUT_TEMPLATE = " \n( moment_out , [%s]), ";
    std::string output_moment_out_str = paddle::string::Sprintf(TENSOR_MOMENT_OUT_TEMPLATE, egr::EagerUtils::TensorStr(moment_out));
    output_str += output_moment_out_str; 
    const char* TENSOR_MEAN_SQUARE_OUT_TEMPLATE = " \n( mean_square_out , [%s]), ";
    std::string output_mean_square_out_str = paddle::string::Sprintf(TENSOR_MEAN_SQUARE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_square_out));
    output_str += output_mean_square_out_str; 
    const char* TENSOR_MEAN_GRAD_OUT_TEMPLATE = " \n( mean_grad_out , [%s]), ";
    std::string output_mean_grad_out_str = paddle::string::Sprintf(TENSOR_MEAN_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_grad_out));
    output_str += output_mean_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, moment_out, mean_square_out, mean_grad_out};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>> rnn_ad_func(const paddle::experimental::Tensor& x, const std::vector<paddle::experimental::Tensor>& pre_state, const std::vector<paddle::experimental::Tensor>& weight_list, const paddle::optional<paddle::experimental::Tensor>& sequence_length, const paddle::experimental::Tensor& dropout_state_in, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, std::string mode, int seed, bool is_test) {
  VLOG(3) << "Running AD API: " << "rnn";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("rnn dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("rnn");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},pre_state,weight_list,{dropout_state_in} };
    if (sequence_length) amp_tensors_vector.push_back({ *sequence_length });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_pre_state = egr::EagerAmpAutoCasts("pre_state", pre_state, amp_dst_dtype, op_name);
    auto new_weight_list = egr::EagerAmpAutoCasts("weight_list", weight_list, amp_dst_dtype, op_name);
    auto new_dropout_state_in = egr::EagerAmpAutoCast("dropout_state_in", dropout_state_in, amp_dst_dtype, op_name);
    auto new_sequence_length = egr::EagerAmpAutoCast("sequence_length", sequence_length, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return rnn_ad_func(new_x, new_pre_state, new_weight_list, new_sequence_length, new_dropout_state_in, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},pre_state,weight_list,{dropout_state_in} };
    if (sequence_length) tensors_vector.push_back({ *sequence_length });

    auto op_name = phi::TransToFluidOpName("rnn");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_pre_state = transformer->TransInTensors("pre_state", pre_state);
    auto new_weight_list = transformer->TransInTensors("weight_list", weight_list);
    auto new_dropout_state_in = transformer->TransInTensor("dropout_state_in", dropout_state_in);
auto new_sequence_length = transformer->TransInTensor("sequence_length", sequence_length);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>> api_result = rnn_ad_func(new_x, new_pre_state, new_weight_list, new_sequence_length, new_dropout_state_in, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& dropout_state_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&dropout_state_out);
        auto& state = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&state);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>>{out, dropout_state_out, state};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  std::vector<egr::AutogradMeta*> pre_state_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(pre_state);
  std::vector<egr::AutogradMeta*>* pre_state_autograd_meta = &pre_state_autograd_meta_vec;
  std::vector<egr::AutogradMeta*> weight_list_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(weight_list);
  std::vector<egr::AutogradMeta*>* weight_list_autograd_meta = &weight_list_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "rnn";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_PRE_STATE_TEMPLATE = " \n( pre_state , [%s]), ";
    std::string input_pre_state_str = paddle::string::Sprintf(TENSOR_PRE_STATE_TEMPLATE, egr::EagerUtils::TensorStr(pre_state));
    input_str += input_pre_state_str; 
    const char* TENSOR_WEIGHT_LIST_TEMPLATE = " \n( weight_list , [%s]), ";
    std::string input_weight_list_str = paddle::string::Sprintf(TENSOR_WEIGHT_LIST_TEMPLATE, egr::EagerUtils::TensorStr(weight_list));
    input_str += input_weight_list_str; 
    const char* TENSOR_SEQUENCE_LENGTH_TEMPLATE = " \n( sequence_length , [%s]), ";
    std::string input_sequence_length_str = paddle::string::Sprintf(TENSOR_SEQUENCE_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(sequence_length));
    input_str += input_sequence_length_str; 
    const char* TENSOR_DROPOUT_STATE_IN_TEMPLATE = " \n( dropout_state_in , [%s]), ";
    std::string input_dropout_state_in_str = paddle::string::Sprintf(TENSOR_DROPOUT_STATE_IN_TEMPLATE, egr::EagerUtils::TensorStr(dropout_state_in));
    input_str += input_dropout_state_in_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::rnn_intermediate(x, pre_state, weight_list, sequence_length, dropout_state_in, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rnn_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& dropout_state_out = std::get<1>(api_result);
  auto& state = std::get<2>(api_result);
  auto& reserve = std::get<3>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* dropout_state_out_autograd_meta = egr::EagerUtils::autograd_meta(&dropout_state_out);
  std::vector<egr::AutogradMeta*> state_autograd_meta_vec = egr::EagerUtils::autograd_meta(&state);
  std::vector<egr::AutogradMeta*>* state_autograd_meta = &state_autograd_meta_vec;
  egr::AutogradMeta* reserve_autograd_meta = egr::EagerUtils::autograd_meta(&reserve);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,pre_state_autograd_meta,weight_list_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("rnn node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,dropout_state_out_autograd_meta,state_autograd_meta,reserve_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RnnGradNode>(new RnnGradNode(4, 5));
    // SetAttributes if needed
    grad_node->SetAttributedropout_prob(dropout_prob);
    grad_node->SetAttributeis_bidirec(is_bidirec);
    grad_node->SetAttributeinput_size(input_size);
    grad_node->SetAttributehidden_size(hidden_size);
    grad_node->SetAttributenum_layers(num_layers);
    grad_node->SetAttributemode(mode);
    grad_node->SetAttributeseed(seed);
    grad_node->SetAttributeis_test(is_test);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperpre_state(pre_state);
    grad_node->SetTensorWrapperweight_list(weight_list);
    if(sequence_length) grad_node->SetTensorWrappersequence_length(*sequence_length);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(pre_state, 1);
    grad_node->SetGradOutMeta(weight_list, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (dropout_state_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(dropout_state_out_autograd_meta, 1);
    }
    if (state_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(state_autograd_meta, 2);
    }
    if (reserve_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(reserve_autograd_meta, 3);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (dropout_state_out_autograd_meta) {
      egr::EagerUtils::SetHistory(dropout_state_out_autograd_meta, grad_node);
    }
    if (state_autograd_meta) {
      egr::EagerUtils::SetHistory(state_autograd_meta, grad_node);
    }
    if (reserve_autograd_meta) {
      egr::EagerUtils::SetHistory(reserve_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(dropout_state_out, 1);
    grad_node->SetGradInMeta(state, 2);
    grad_node->SetGradInMeta(reserve, 3);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(dropout_state_out);
    egr::EagerUtils::CheckAndRetainGrad(state);
    egr::EagerUtils::CheckAndRetainGrad(reserve);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperdropout_state_out(dropout_state_out);
    grad_node->SetTensorWrapperreserve(reserve);
  }


  VLOG(4) << "Finish AD API: rnn";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_PRE_STATE_TEMPLATE = " \n( pre_state , [%s]), ";
    std::string input_pre_state_str = paddle::string::Sprintf(TENSOR_PRE_STATE_TEMPLATE, egr::EagerUtils::TensorStr(pre_state));
    input_str += input_pre_state_str; 
    const char* TENSOR_WEIGHT_LIST_TEMPLATE = " \n( weight_list , [%s]), ";
    std::string input_weight_list_str = paddle::string::Sprintf(TENSOR_WEIGHT_LIST_TEMPLATE, egr::EagerUtils::TensorStr(weight_list));
    input_str += input_weight_list_str; 
    const char* TENSOR_SEQUENCE_LENGTH_TEMPLATE = " \n( sequence_length , [%s]), ";
    std::string input_sequence_length_str = paddle::string::Sprintf(TENSOR_SEQUENCE_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(sequence_length));
    input_str += input_sequence_length_str; 
    const char* TENSOR_DROPOUT_STATE_IN_TEMPLATE = " \n( dropout_state_in , [%s]), ";
    std::string input_dropout_state_in_str = paddle::string::Sprintf(TENSOR_DROPOUT_STATE_IN_TEMPLATE, egr::EagerUtils::TensorStr(dropout_state_in));
    input_str += input_dropout_state_in_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_DROPOUT_STATE_OUT_TEMPLATE = " \n( dropout_state_out , [%s]), ";
    std::string output_dropout_state_out_str = paddle::string::Sprintf(TENSOR_DROPOUT_STATE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(dropout_state_out));
    output_str += output_dropout_state_out_str; 
    const char* TENSOR_STATE_TEMPLATE = " \n( state , [%s]), ";
    std::string output_state_str = paddle::string::Sprintf(TENSOR_STATE_TEMPLATE, egr::EagerUtils::TensorStr(state));
    output_str += output_state_str; 
    const char* TENSOR_RESERVE_TEMPLATE = " \n( reserve , [%s]), ";
    std::string output_reserve_str = paddle::string::Sprintf(TENSOR_RESERVE_TEMPLATE, egr::EagerUtils::TensorStr(reserve));
    output_str += output_reserve_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, std::vector<paddle::experimental::Tensor>>{out, dropout_state_out, state};
}


paddle::experimental::Tensor roi_align_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& boxes, const paddle::optional<paddle::experimental::Tensor>& boxes_num, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned) {
  VLOG(3) << "Running AD API: " << "roi_align";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("roi_align dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("roi_align");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{boxes} };
    if (boxes_num) amp_tensors_vector.push_back({ *boxes_num });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_boxes = egr::EagerAmpAutoCast("boxes", boxes, amp_dst_dtype, op_name);
    auto new_boxes_num = egr::EagerAmpAutoCast("boxes_num", boxes_num, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return roi_align_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{boxes} };
    if (boxes_num) tensors_vector.push_back({ *boxes_num });

    auto op_name = phi::TransToFluidOpName("roi_align");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_boxes = transformer->TransInTensor("boxes", boxes);
auto new_boxes_num = transformer->TransInTensor("boxes_num", boxes_num);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = roi_align_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "roi_align";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::roi_align(x, boxes, boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roi_align", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("roi_align node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RoiAlignGradNode>(new RoiAlignGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributepooled_height(pooled_height);
    grad_node->SetAttributepooled_width(pooled_width);
    grad_node->SetAttributespatial_scale(spatial_scale);
    grad_node->SetAttributesampling_ratio(sampling_ratio);
    grad_node->SetAttributealigned(aligned);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperboxes(boxes);
    if(boxes_num) grad_node->SetTensorWrapperboxes_num(*boxes_num);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: roi_align";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor roi_pool_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& boxes, const paddle::optional<paddle::experimental::Tensor>& boxes_num, int pooled_height, int pooled_width, float spatial_scale) {
  VLOG(3) << "Running AD API: " << "roi_pool";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("roi_pool dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("roi_pool");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{boxes} };
    if (boxes_num) amp_tensors_vector.push_back({ *boxes_num });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_boxes = egr::EagerAmpAutoCast("boxes", boxes, amp_dst_dtype, op_name);
    auto new_boxes_num = egr::EagerAmpAutoCast("boxes_num", boxes_num, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return roi_pool_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, spatial_scale);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{boxes} };
    if (boxes_num) tensors_vector.push_back({ *boxes_num });

    auto op_name = phi::TransToFluidOpName("roi_pool");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_boxes = transformer->TransInTensor("boxes", boxes);
auto new_boxes_num = transformer->TransInTensor("boxes_num", boxes_num);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = roi_pool_ad_func(new_x, new_boxes, new_boxes_num, pooled_height, pooled_width, spatial_scale);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "roi_pool";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::roi_pool_intermediate(x, boxes, boxes_num, pooled_height, pooled_width, spatial_scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roi_pool_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& arg_max = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* arg_max_autograd_meta = egr::EagerUtils::autograd_meta(&arg_max);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("roi_pool node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,arg_max_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<RoiPoolGradNode>(new RoiPoolGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributepooled_height(pooled_height);
    grad_node->SetAttributepooled_width(pooled_width);
    grad_node->SetAttributespatial_scale(spatial_scale);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperboxes(boxes);
    if(boxes_num) grad_node->SetTensorWrapperboxes_num(*boxes_num);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (arg_max_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(arg_max_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (arg_max_autograd_meta) {
      egr::EagerUtils::SetHistory(arg_max_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(arg_max, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(arg_max);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperarg_max(arg_max);
  }


  VLOG(4) << "Finish AD API: roi_pool";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_ARG_MAX_TEMPLATE = " \n( arg_max , [%s]), ";
    std::string output_arg_max_str = paddle::string::Sprintf(TENSOR_ARG_MAX_TEMPLATE, egr::EagerUtils::TensorStr(arg_max));
    output_str += output_arg_max_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor scale_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar scale, float bias, bool bias_after_scale) {
  VLOG(3) << "Running AD API: " << "scale";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scale dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("scale");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return scale_ad_func(new_x, scale, bias, bias_after_scale);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("scale");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = scale_ad_func(new_x, scale, bias, bias_after_scale);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "scale";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::scale(x, scale, bias, bias_after_scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scale", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scale node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScaleGradNode>(new ScaleGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributebias_after_scale(bias_after_scale);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scale";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& scale__ad_func(paddle::experimental::Tensor& x, paddle::experimental::Scalar scale, float bias, bool bias_after_scale) {
  VLOG(3) << "Running AD API: " << "scale_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scale_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for scale__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("scale_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = scale__ad_func(new_x, scale, bias, bias_after_scale);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "scale_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::scale_(x, scale, bias, bias_after_scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scale_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scale node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScaleGradNode>(new ScaleGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributebias_after_scale(bias_after_scale);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scale_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> segment_pool_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& segment_ids, std::string pooltype) {
  VLOG(3) << "Running AD API: " << "segment_pool";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("segment_pool dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("segment_pool");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{segment_ids} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_segment_ids = egr::EagerAmpAutoCast("segment_ids", segment_ids, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return segment_pool_ad_func(new_x, new_segment_ids, pooltype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{segment_ids} };
    
    auto op_name = phi::TransToFluidOpName("segment_pool");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_segment_ids = transformer->TransInTensor("segment_ids", segment_ids);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = segment_pool_ad_func(new_x, new_segment_ids, pooltype);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& summed_ids = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&summed_ids);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, summed_ids};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "segment_pool";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEGMENT_IDS_TEMPLATE = " \n( segment_ids , [%s]), ";
    std::string input_segment_ids_str = paddle::string::Sprintf(TENSOR_SEGMENT_IDS_TEMPLATE, egr::EagerUtils::TensorStr(segment_ids));
    input_str += input_segment_ids_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::segment_pool(x, segment_ids, pooltype);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("segment_pool", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& summed_ids = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* summed_ids_autograd_meta = egr::EagerUtils::autograd_meta(&summed_ids);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("segment_pool node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,summed_ids_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SegmentPoolGradNode>(new SegmentPoolGradNode(2, 2));
    // SetAttributes if needed
    grad_node->SetAttributepooltype(pooltype);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappersegment_ids(segment_ids);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (summed_ids_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(summed_ids_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (summed_ids_autograd_meta) {
      egr::EagerUtils::SetHistory(summed_ids_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(summed_ids, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(summed_ids);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrappersummed_ids(summed_ids);
  }


  VLOG(4) << "Finish AD API: segment_pool";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEGMENT_IDS_TEMPLATE = " \n( segment_ids , [%s]), ";
    std::string input_segment_ids_str = paddle::string::Sprintf(TENSOR_SEGMENT_IDS_TEMPLATE, egr::EagerUtils::TensorStr(segment_ids));
    input_str += input_segment_ids_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_SUMMED_IDS_TEMPLATE = " \n( summed_ids , [%s]), ";
    std::string output_summed_ids_str = paddle::string::Sprintf(TENSOR_SUMMED_IDS_TEMPLATE, egr::EagerUtils::TensorStr(summed_ids));
    output_str += output_summed_ids_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{out, summed_ids};
}


paddle::experimental::Tensor send_u_recv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& src_index, const paddle::experimental::Tensor& dst_index, std::string reduce_op, paddle::experimental::IntArray out_size) {
  VLOG(3) << "Running AD API: " << "send_u_recv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("send_u_recv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("send_u_recv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{src_index},{dst_index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_src_index = egr::EagerAmpAutoCast("src_index", src_index, amp_dst_dtype, op_name);
    auto new_dst_index = egr::EagerAmpAutoCast("dst_index", dst_index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return send_u_recv_ad_func(new_x, new_src_index, new_dst_index, reduce_op, out_size);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{src_index},{dst_index} };
    
    auto op_name = phi::TransToFluidOpName("send_u_recv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_src_index = transformer->TransInTensor("src_index", src_index);
    auto new_dst_index = transformer->TransInTensor("dst_index", dst_index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = send_u_recv_ad_func(new_x, new_src_index, new_dst_index, reduce_op, out_size);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "send_u_recv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::send_u_recv_intermediate(x, src_index, dst_index, reduce_op, out_size);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_u_recv_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& dst_count = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* dst_count_autograd_meta = egr::EagerUtils::autograd_meta(&dst_count);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("send_u_recv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,dst_count_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SendURecvGradNode>(new SendURecvGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributereduce_op(reduce_op);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappersrc_index(src_index);
    grad_node->SetTensorWrapperdst_index(dst_index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (dst_count_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(dst_count_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (dst_count_autograd_meta) {
      egr::EagerUtils::SetHistory(dst_count_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(dst_count, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(dst_count);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperdst_count(dst_count);
  }


  VLOG(4) << "Finish AD API: send_u_recv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string output_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    output_str += output_dst_count_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor send_ue_recv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& src_index, const paddle::experimental::Tensor& dst_index, std::string message_op, std::string reduce_op, paddle::experimental::IntArray out_size) {
  VLOG(3) << "Running AD API: " << "send_ue_recv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("send_ue_recv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("send_ue_recv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y},{src_index},{dst_index} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    auto new_src_index = egr::EagerAmpAutoCast("src_index", src_index, amp_dst_dtype, op_name);
    auto new_dst_index = egr::EagerAmpAutoCast("dst_index", dst_index, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return send_ue_recv_ad_func(new_x, new_y, new_src_index, new_dst_index, message_op, reduce_op, out_size);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{src_index},{dst_index} };
    
    auto op_name = phi::TransToFluidOpName("send_ue_recv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_src_index = transformer->TransInTensor("src_index", src_index);
    auto new_dst_index = transformer->TransInTensor("dst_index", dst_index);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = send_ue_recv_ad_func(new_x, new_y, new_src_index, new_dst_index, message_op, reduce_op, out_size);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "send_ue_recv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::send_ue_recv_intermediate(x, y, src_index, dst_index, message_op, reduce_op, out_size);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_ue_recv_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& dst_count = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* dst_count_autograd_meta = egr::EagerUtils::autograd_meta(&dst_count);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("send_ue_recv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,dst_count_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SendUeRecvGradNode>(new SendUeRecvGradNode(2, 4));
    // SetAttributes if needed
    grad_node->SetAttributemessage_op(message_op);
    grad_node->SetAttributereduce_op(reduce_op);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappersrc_index(src_index);
    grad_node->SetTensorWrapperdst_index(dst_index);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (dst_count_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(dst_count_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (dst_count_autograd_meta) {
      egr::EagerUtils::SetHistory(dst_count_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(dst_count, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(dst_count);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperdst_count(dst_count);
  }


  VLOG(4) << "Finish AD API: send_ue_recv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string output_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    output_str += output_dst_count_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> sgd__ad_func(paddle::experimental::Tensor& param, const paddle::experimental::Tensor& learning_rate, const paddle::experimental::Tensor& grad, paddle::optional<paddle::experimental::Tensor>& master_param, bool multi_precision) {
  VLOG(3) << "Running AD API: " << "sgd_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sgd_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for sgd__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {param},{learning_rate},{grad} };
    if (master_param) tensors_vector.push_back({ *master_param });

    auto op_name = phi::TransToFluidOpName("sgd_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_param = transformer->TransInTensor("param", param);
    auto new_learning_rate = transformer->TransInTensor("learning_rate", learning_rate);
    auto new_grad = transformer->TransInTensor("grad", grad);
auto new_master_param = transformer->TransInTensor("master_param", master_param);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&> api_result = sgd__ad_func(new_param, new_learning_rate, new_grad, new_master_param, multi_precision);

        auto& param_out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&param_out);
        auto& master_param_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&master_param_out);

    // Returns
    return std::tuple<paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, master_param_out};
  }

  VLOG(5) << "Running C++ API: " << "sgd_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::sgd_(param, learning_rate, grad, master_param, multi_precision);
  // Get Outputs
  auto& param_out = std::get<0>(api_result);
  auto& master_param_out = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: sgd_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_PARAM_TEMPLATE = " \n( param , [%s]), ";
    std::string input_param_str = paddle::string::Sprintf(TENSOR_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(param));
    input_str += input_param_str; 
    const char* TENSOR_LEARNING_RATE_TEMPLATE = " \n( learning_rate , [%s]), ";
    std::string input_learning_rate_str = paddle::string::Sprintf(TENSOR_LEARNING_RATE_TEMPLATE, egr::EagerUtils::TensorStr(learning_rate));
    input_str += input_learning_rate_str; 
    const char* TENSOR_GRAD_TEMPLATE = " \n( grad , [%s]), ";
    std::string input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad));
    input_str += input_grad_str; 
    const char* TENSOR_MASTER_PARAM_TEMPLATE = " \n( master_param , [%s]), ";
    std::string input_master_param_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_TEMPLATE, egr::EagerUtils::TensorStr(master_param));
    input_str += input_master_param_str; 
    const char* TENSOR_PARAM_OUT_TEMPLATE = " \n( param_out , [%s]), ";
    std::string output_param_out_str = paddle::string::Sprintf(TENSOR_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(param_out));
    output_str += output_param_out_str; 
    const char* TENSOR_MASTER_PARAM_OUT_TEMPLATE = " \n( master_param_out , [%s]), ";
    std::string output_master_param_out_str = paddle::string::Sprintf(TENSOR_MASTER_PARAM_OUT_TEMPLATE, egr::EagerUtils::TensorStr(master_param_out));
    output_str += output_master_param_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor&, paddle::optional<paddle::experimental::Tensor>&>{param_out, master_param_out};
}


paddle::experimental::Tensor shape_ad_func(const paddle::experimental::Tensor& input) {
  VLOG(3) << "Running AD API: " << "shape";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("shape dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("shape");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return shape_ad_func(new_input);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("shape");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = shape_ad_func(new_input);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "shape";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::shape(input);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: shape";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sigmoid_cross_entropy_with_logits_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& label, bool normalize, int ignore_index) {
  VLOG(3) << "Running AD API: " << "sigmoid_cross_entropy_with_logits";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sigmoid_cross_entropy_with_logits dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sigmoid_cross_entropy_with_logits");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{label} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sigmoid_cross_entropy_with_logits_ad_func(new_x, new_label, normalize, ignore_index);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{label} };
    
    auto op_name = phi::TransToFluidOpName("sigmoid_cross_entropy_with_logits");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_label = transformer->TransInTensor("label", label);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sigmoid_cross_entropy_with_logits_ad_func(new_x, new_label, normalize, ignore_index);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sigmoid_cross_entropy_with_logits";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sigmoid_cross_entropy_with_logits(x, label, normalize, ignore_index);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid_cross_entropy_with_logits", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sigmoid_cross_entropy_with_logits node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SigmoidCrossEntropyWithLogitsGradNode>(new SigmoidCrossEntropyWithLogitsGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributenormalize(normalize);
    grad_node->SetAttributeignore_index(ignore_index);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperlabel(label);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sigmoid_cross_entropy_with_logits";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sign_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sign";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sign dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sign");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sign_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sign");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sign_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sign";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sign(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sign", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sign node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SignGradNode>(new SignGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sign";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor slice_ad_func(const paddle::experimental::Tensor& input, std::vector<int64_t> axes, paddle::experimental::IntArray starts, paddle::experimental::IntArray ends, std::vector<int64_t> infer_flags, std::vector<int64_t> decrease_axis) {
  VLOG(3) << "Running AD API: " << "slice";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("slice dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("slice");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return slice_ad_func(new_input, axes, starts, ends, infer_flags, decrease_axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("slice");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int64_t>>(op_name, tensors_vector,&axes);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = slice_ad_func(new_input, axes, starts, ends, infer_flags, decrease_axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "slice";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::slice(input, axes, starts, ends, infer_flags, decrease_axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("slice", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("slice node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SliceGradNode>(new SliceGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributestarts(starts);
    grad_node->SetAttributeends(ends);
    grad_node->SetAttributeinfer_flags(infer_flags);
    grad_node->SetAttributedecrease_axis(decrease_axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: slice";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor slogdet_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "slogdet";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("slogdet dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("slogdet");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return slogdet_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("slogdet");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = slogdet_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "slogdet";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::slogdet(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("slogdet", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("slogdet node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SlogdetGradNode>(new SlogdetGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: slogdet";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor softmax_ad_func(const paddle::experimental::Tensor& x, int axis) {
  VLOG(3) << "Running AD API: " << "softmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("softmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return softmax_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softmax");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = softmax_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::softmax(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softmax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftmaxGradNode>(new SoftmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: softmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& softmax__ad_func(paddle::experimental::Tensor& x, int axis) {
  VLOG(3) << "Running AD API: " << "softmax_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softmax_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for softmax__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softmax_");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = softmax__ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softmax_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::softmax_(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softmax_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftmaxGradNode>(new SoftmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: softmax_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor spectral_norm_ad_func(const paddle::experimental::Tensor& weight, const paddle::experimental::Tensor& u, const paddle::experimental::Tensor& v, int dim, int power_iters, float eps) {
  VLOG(3) << "Running AD API: " << "spectral_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("spectral_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("spectral_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {weight},{u},{v} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_weight = egr::EagerAmpAutoCast("weight", weight, amp_dst_dtype, op_name);
    auto new_u = egr::EagerAmpAutoCast("u", u, amp_dst_dtype, op_name);
    auto new_v = egr::EagerAmpAutoCast("v", v, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return spectral_norm_ad_func(new_weight, new_u, new_v, dim, power_iters, eps);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {weight},{u},{v} };
    
    auto op_name = phi::TransToFluidOpName("spectral_norm");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &dim);
    auto new_weight = transformer->TransInTensor("weight", weight);
    auto new_u = transformer->TransInTensor("u", u);
    auto new_v = transformer->TransInTensor("v", v);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = spectral_norm_ad_func(new_weight, new_u, new_v, dim, power_iters, eps);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* weight_autograd_meta = egr::EagerUtils::nullable_autograd_meta(weight);

  VLOG(5) << "Running C++ API: " << "spectral_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_V_TEMPLATE = " \n( v , [%s]), ";
    std::string input_v_str = paddle::string::Sprintf(TENSOR_V_TEMPLATE, egr::EagerUtils::TensorStr(v));
    input_str += input_v_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::spectral_norm(weight, u, v, dim, power_iters, eps);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("spectral_norm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,weight_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("spectral_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SpectralNormGradNode>(new SpectralNormGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributedim(dim);
    grad_node->SetAttributepower_iters(power_iters);
    grad_node->SetAttributeeps(eps);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperweight(weight);
    grad_node->SetTensorWrapperu(u);
    grad_node->SetTensorWrapperv(v);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(weight, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: spectral_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_V_TEMPLATE = " \n( v , [%s]), ";
    std::string input_v_str = paddle::string::Sprintf(TENSOR_V_TEMPLATE, egr::EagerUtils::TensorStr(v));
    input_str += input_v_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::vector<paddle::experimental::Tensor> split_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray num_or_sections, paddle::experimental::Scalar axis) {
  VLOG(3) << "Running AD API: " << "split";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("split dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("split");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return split_ad_func(new_x, num_or_sections, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("split");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> out = split_ad_func(new_x, num_or_sections, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "split";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::split(x, num_or_sections, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("split", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> out_autograd_meta_vec = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*>* out_autograd_meta = &out_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("split node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SplitGradNode>(new SplitGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: split";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::vector<paddle::experimental::Tensor> split_with_num_ad_func(const paddle::experimental::Tensor& x, int num, paddle::experimental::Scalar axis) {
  VLOG(3) << "Running AD API: " << "split_with_num";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("split_with_num dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("split_with_num");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return split_with_num_ad_func(new_x, num, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("split_with_num");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> out = split_with_num_ad_func(new_x, num, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "split_with_num";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::split_with_num(x, num, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("split_with_num", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> out_autograd_meta_vec = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*>* out_autograd_meta = &out_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("split_with_num node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SplitWithNumGradNode>(new SplitWithNumGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: split_with_num";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor squared_l2_norm_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "squared_l2_norm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("squared_l2_norm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("squared_l2_norm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return squared_l2_norm_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("squared_l2_norm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = squared_l2_norm_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "squared_l2_norm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::squared_l2_norm(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squared_l2_norm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("squared_l2_norm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SquaredL2NormGradNode>(new SquaredL2NormGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: squared_l2_norm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor stack_ad_func(const std::vector<paddle::experimental::Tensor>& x, int axis) {
  VLOG(3) << "Running AD API: " << "stack";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("stack dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("stack");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { x };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCasts("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return stack_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x };
    
    auto op_name = phi::TransToFluidOpName("stack");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensors("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = stack_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  std::vector<egr::AutogradMeta*> x_autograd_meta_vec = egr::EagerUtils::nullable_autograd_meta(x);
  std::vector<egr::AutogradMeta*>* x_autograd_meta = &x_autograd_meta_vec;

  VLOG(5) << "Running C++ API: " << "stack";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::stack(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("stack", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("stack node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<StackGradNode>(new StackGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: stack";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor strided_slice_ad_func(const paddle::experimental::Tensor& x, std::vector<int> axes, paddle::experimental::IntArray starts, paddle::experimental::IntArray ends, paddle::experimental::IntArray strides) {
  VLOG(3) << "Running AD API: " << "strided_slice";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("strided_slice dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("strided_slice");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return strided_slice_ad_func(new_x, axes, starts, ends, strides);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("strided_slice");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector,&axes);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = strided_slice_ad_func(new_x, axes, starts, ends, strides);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "strided_slice";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::strided_slice(x, axes, starts, ends, strides);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("strided_slice", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("strided_slice node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<StridedSliceGradNode>(new StridedSliceGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributestarts(starts);
    grad_node->SetAttributeends(ends);
    grad_node->SetAttributestrides(strides);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: strided_slice";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor subtract_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "subtract";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("subtract dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("subtract");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return subtract_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("subtract");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = subtract_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "subtract";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::subtract(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("subtract node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SubtractGradNode>(new SubtractGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: subtract";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& subtract__ad_func(paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "subtract_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("subtract_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for subtract__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("subtract_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = subtract__ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "subtract_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::subtract_(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("subtract node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SubtractGradNode>(new SubtractGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: subtract_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sum_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray axis, paddle::experimental::DataType dtype, bool keepdim) {
  VLOG(3) << "Running AD API: " << "sum";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sum dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sum");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sum_ad_func(new_x, axis, dtype, keepdim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sum");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::IntArray, bool>(op_name, tensors_vector, &axis, &keepdim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sum_ad_func(new_x, axis, dtype, keepdim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sum";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sum(x, axis, dtype, keepdim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sum", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sum node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SumGradNode>(new SumGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    grad_node->SetAttributereduce_all(false);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sum";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor swish_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "swish";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("swish dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("swish");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return swish_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("swish");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = swish_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "swish";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::swish(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("swish", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("swish node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SwishGradNode>(new SwishGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributebete(1.0);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: swish";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> sync_batch_norm__ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Tensor& mean, paddle::experimental::Tensor& variance, const paddle::experimental::Tensor& scale, const paddle::experimental::Tensor& bias, bool is_test, float momentum, float epsilon, std::string data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(3) << "Running AD API: " << "sync_batch_norm_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sync_batch_norm_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for sync_batch_norm__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{mean},{variance},{scale},{bias} };
    
    auto op_name = phi::TransToFluidOpName("sync_batch_norm_");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_mean = transformer->TransInTensor("mean", mean);
    auto new_variance = transformer->TransInTensor("variance", variance);
    auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = sync_batch_norm__ad_func(new_x, new_mean, new_variance, new_scale, new_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mean_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mean_out);
        auto& variance_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&variance_out);
        auto& saved_mean = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&saved_mean);
        auto& saved_variance = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&saved_variance);
        auto& reserve_space = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&reserve_space);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* mean_autograd_meta = egr::EagerUtils::nullable_autograd_meta(mean);
  egr::AutogradMeta* variance_autograd_meta = egr::EagerUtils::nullable_autograd_meta(variance);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "sync_batch_norm_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sync_batch_norm_(x, mean, variance, scale, bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sync_batch_norm_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mean_out = std::get<1>(api_result);
  auto& variance_out = std::get<2>(api_result);
  auto& saved_mean = std::get<3>(api_result);
  auto& saved_variance = std::get<4>(api_result);
  auto& reserve_space = std::get<5>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mean_out_autograd_meta = egr::EagerUtils::autograd_meta(&mean_out);
  egr::AutogradMeta* variance_out_autograd_meta = egr::EagerUtils::autograd_meta(&variance_out);
  egr::AutogradMeta* saved_mean_autograd_meta = egr::EagerUtils::autograd_meta(&saved_mean);
  egr::AutogradMeta* saved_variance_autograd_meta = egr::EagerUtils::autograd_meta(&saved_variance);
  egr::AutogradMeta* reserve_space_autograd_meta = egr::EagerUtils::autograd_meta(&reserve_space);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,mean_autograd_meta,variance_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(mean, mean_autograd_meta, require_any_grad);

  egr::EagerUtils::CheckInplace(variance, variance_autograd_meta, require_any_grad);

  // Bump Inplace Version
  mean.bump_inplace_version();
  VLOG(3) << "Tensor(" << mean.name() << ") uses Inplace Strategy.";

  // Bump Inplace Version
  variance.bump_inplace_version();
  VLOG(3) << "Tensor(" << variance.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sync_batch_norm_ node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mean_out_autograd_meta,variance_out_autograd_meta,saved_mean_autograd_meta,saved_variance_autograd_meta,reserve_space_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SyncBatchNormGradNode>(new SyncBatchNormGradNode(6, 5));
    // SetAttributes if needed
    grad_node->SetAttributemomentum(momentum);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributeuse_global_stats(use_global_stats);
    grad_node->SetAttributetrainable_statistics(trainable_statistics);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperscale(scale);
    grad_node->SetTensorWrapperbias(bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(scale, 3);
    grad_node->SetGradOutMeta(bias, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_out_autograd_meta, 1);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_out_autograd_meta, 2);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_mean_autograd_meta, 3);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_variance_autograd_meta, 4);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(reserve_space_autograd_meta, 5);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_out_autograd_meta, grad_node);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_out_autograd_meta, grad_node);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_mean_autograd_meta, grad_node);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_variance_autograd_meta, grad_node);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetHistory(reserve_space_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mean_out, 1);
    grad_node->SetGradInMeta(variance_out, 2);
    grad_node->SetGradInMeta(saved_mean, 3);
    grad_node->SetGradInMeta(saved_variance, 4);
    grad_node->SetGradInMeta(reserve_space, 5);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mean_out);
    egr::EagerUtils::CheckAndRetainGrad(variance_out);
    egr::EagerUtils::CheckAndRetainGrad(saved_mean);
    egr::EagerUtils::CheckAndRetainGrad(saved_variance);
    egr::EagerUtils::CheckAndRetainGrad(reserve_space);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrapperreserve_space(reserve_space);
  }


  VLOG(4) << "Finish AD API: sync_batch_norm_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string output_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    output_str += output_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string output_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    output_str += output_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string output_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    output_str += output_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string output_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    output_str += output_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string output_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    output_str += output_reserve_space_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
}


paddle::experimental::Tensor temporal_shift_ad_func(const paddle::experimental::Tensor& x, int seg_num, float shift_ratio, std::string data_format_str) {
  VLOG(3) << "Running AD API: " << "temporal_shift";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("temporal_shift dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("temporal_shift");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return temporal_shift_ad_func(new_x, seg_num, shift_ratio, data_format_str);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("temporal_shift");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format_str);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = temporal_shift_ad_func(new_x, seg_num, shift_ratio, data_format_str);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "temporal_shift";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::temporal_shift(x, seg_num, shift_ratio, data_format_str);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("temporal_shift", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("temporal_shift node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TemporalShiftGradNode>(new TemporalShiftGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeseg_num(seg_num);
    grad_node->SetAttributeshift_ratio(shift_ratio);
    grad_node->SetAttributedata_format_str(data_format_str);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: temporal_shift";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tile_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray repeat_times) {
  VLOG(3) << "Running AD API: " << "tile";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tile dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tile");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tile_ad_func(new_x, repeat_times);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tile");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tile_ad_func(new_x, repeat_times);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tile";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::tile(x, repeat_times);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tile", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tile node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TileGradNode>(new TileGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributerepeat_times(repeat_times);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: tile";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor transpose_ad_func(const paddle::experimental::Tensor& x, std::vector<int> perm) {
  VLOG(3) << "Running AD API: " << "transpose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("transpose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("transpose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return transpose_ad_func(new_x, perm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("transpose");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector, &perm);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = transpose_ad_func(new_x, perm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "transpose";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::transpose(x, perm);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("transpose", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("transpose node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TransposeGradNode>(new TransposeGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeperm(perm);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: transpose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor triangular_solve_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, bool upper, bool tranpose, bool unitriangular) {
  VLOG(3) << "Running AD API: " << "triangular_solve";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("triangular_solve dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("triangular_solve");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return triangular_solve_ad_func(new_x, new_y, upper, tranpose, unitriangular);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("triangular_solve");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = triangular_solve_ad_func(new_x, new_y, upper, tranpose, unitriangular);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "triangular_solve";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::triangular_solve(x, y, upper, tranpose, unitriangular);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("triangular_solve", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("triangular_solve node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TriangularSolveGradNode>(new TriangularSolveGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeupper(upper);
    grad_node->SetAttributetranpose(tranpose);
    grad_node->SetAttributeunitriangular(unitriangular);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: triangular_solve";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tril_ad_func(const paddle::experimental::Tensor& x, int diagonal) {
  VLOG(3) << "Running AD API: " << "tril";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tril dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tril");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tril_ad_func(new_x, diagonal);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tril");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tril_ad_func(new_x, diagonal);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tril";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::tril(x, diagonal);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tril", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tril node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TrilGradNode>(new TrilGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributediagonal(diagonal);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: tril";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tril_indices_ad_func(int rows, int cols, int offset, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "tril_indices";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tril_indices dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for tril_indices_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "tril_indices";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::tril_indices(rows, cols, offset, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: tril_indices";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor trilinear_interp_ad_func(const paddle::experimental::Tensor& x, const paddle::optional<paddle::experimental::Tensor>& out_size, const paddle::optional<std::vector<paddle::experimental::Tensor>>& size_tensor, const paddle::optional<paddle::experimental::Tensor>& scale_tensor, std::string data_layout, int out_d, int out_h, int out_w, std::vector<float> scale, std::string interp_method, bool align_corners, int align_mode) {
  VLOG(3) << "Running AD API: " << "trilinear_interp";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("trilinear_interp dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("trilinear_interp");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    if (out_size) amp_tensors_vector.push_back({ *out_size });
    if (size_tensor) amp_tensors_vector.push_back( *size_tensor );
    if (scale_tensor) amp_tensors_vector.push_back({ *scale_tensor });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_out_size = egr::EagerAmpAutoCast("out_size", out_size, amp_dst_dtype, op_name);
    auto new_size_tensor = egr::EagerAmpAutoCasts("size_tensor", size_tensor, amp_dst_dtype, op_name);
    auto new_scale_tensor = egr::EagerAmpAutoCast("scale_tensor", scale_tensor, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return trilinear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    if (out_size) tensors_vector.push_back({ *out_size });
    if (scale_tensor) tensors_vector.push_back({ *scale_tensor });

    auto op_name = phi::TransToFluidOpName("trilinear_interp");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
auto new_out_size = transformer->TransInTensor("out_size", out_size);
    auto new_size_tensor = transformer->TransInTensors("size_tensor", size_tensor);
    auto new_scale_tensor = transformer->TransInTensor("scale_tensor", scale_tensor);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor output = trilinear_interp_ad_func(new_x, new_out_size, new_size_tensor, new_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);

    transformer -> SetOutTensorLayout(&output);

    // Returns
    return output;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "trilinear_interp";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::trilinear_interp(x, out_size, size_tensor, scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trilinear_interp", api_result); }

  // Get Outputs
  auto& output = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* output_autograd_meta = egr::EagerUtils::autograd_meta(&output);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("trilinear_interp node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,output_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TrilinearInterpGradNode>(new TrilinearInterpGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeout_d(out_d);
    grad_node->SetAttributeout_h(out_h);
    grad_node->SetAttributeout_w(out_w);
    grad_node->SetAttributescale(scale);
    grad_node->SetAttributeinterp_method(interp_method);
    grad_node->SetAttributealign_corners(align_corners);
    grad_node->SetAttributealign_mode(align_mode);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    if(out_size) grad_node->SetTensorWrapperout_size(*out_size);
    if(size_tensor) grad_node->SetTensorWrappersize_tensor(*size_tensor);
    if(scale_tensor) grad_node->SetTensorWrapperscale_tensor(*scale_tensor);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (output_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(output_autograd_meta, 0);
    }
    if (output_autograd_meta) {
      egr::EagerUtils::SetHistory(output_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(output, 0);
    egr::EagerUtils::CheckAndRetainGrad(output);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: trilinear_interp";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_OUTPUT_TEMPLATE = " \n( output , [%s]), ";
    std::string output_output_str = paddle::string::Sprintf(TENSOR_OUTPUT_TEMPLATE, egr::EagerUtils::TensorStr(output));
    output_str += output_output_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return output;
}


paddle::experimental::Tensor triu_ad_func(const paddle::experimental::Tensor& x, int diagonal) {
  VLOG(3) << "Running AD API: " << "triu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("triu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("triu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return triu_ad_func(new_x, diagonal);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("triu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = triu_ad_func(new_x, diagonal);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "triu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::triu(x, diagonal);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("triu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("triu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TriuGradNode>(new TriuGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributediagonal(diagonal);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: triu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor triu_indices_ad_func(int row, int col, int offset, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "triu_indices";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("triu_indices dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for triu_indices_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "triu_indices";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::triu_indices(row, col, offset, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: triu_indices";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor truncated_gaussian_random_ad_func(std::vector<int> shape, float mean, float std, int seed, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "truncated_gaussian_random";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("truncated_gaussian_random dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for truncated_gaussian_random_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "truncated_gaussian_random";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::truncated_gaussian_random(shape, mean, std, seed, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: truncated_gaussian_random";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::vector<paddle::experimental::Tensor> unbind_ad_func(const paddle::experimental::Tensor& input, int axis) {
  VLOG(3) << "Running AD API: " << "unbind";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unbind dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unbind");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unbind_ad_func(new_input, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input} };
    
    auto op_name = phi::TransToFluidOpName("unbind");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_input = transformer->TransInTensor("input", input);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::vector<paddle::experimental::Tensor> out = unbind_ad_func(new_input, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);

  VLOG(5) << "Running C++ API: " << "unbind";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unbind(input, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unbind", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  std::vector<egr::AutogradMeta*> out_autograd_meta_vec = egr::EagerUtils::autograd_meta(&out);
  std::vector<egr::AutogradMeta*>* out_autograd_meta = &out_autograd_meta_vec;
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unbind node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnbindGradNode>(new UnbindGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: unbind";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor uniform_ad_func(paddle::experimental::IntArray shape, paddle::experimental::DataType dtype, paddle::experimental::Scalar min, paddle::experimental::Scalar max, int seed, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "uniform";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("uniform dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for uniform_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "uniform";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::uniform(shape, dtype, min, max, seed, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: uniform";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor uniform_inplace_ad_func(const paddle::experimental::Tensor& x, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(3) << "Running AD API: " << "uniform_inplace";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("uniform_inplace dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("uniform_inplace");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return uniform_inplace_ad_func(new_x, min, max, seed, diag_num, diag_step, diag_val);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("uniform_inplace");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = uniform_inplace_ad_func(new_x, min, max, seed, diag_num, diag_step, diag_val);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "uniform_inplace";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::uniform_inplace(x, min, max, seed, diag_num, diag_step, diag_val);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("uniform_inplace", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("uniform_inplace node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UniformInplaceGradNode>(new UniformInplaceGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributemin(min);
    grad_node->SetAttributemax(max);
    grad_node->SetAttributeseed(seed);
    grad_node->SetAttributediag_num(diag_num);
    grad_node->SetAttributediag_step(diag_step);
    grad_node->SetAttributediag_val(diag_val);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: uniform_inplace";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}

paddle::experimental::Tensor& uniform_inplace__ad_func(paddle::experimental::Tensor& x, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(3) << "Running AD API: " << "uniform_inplace_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("uniform_inplace_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for uniform_inplace__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("uniform_inplace_");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor& out = uniform_inplace__ad_func(new_x, min, max, seed, diag_num, diag_step, diag_val);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "uniform_inplace_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto& api_result = paddle::experimental::uniform_inplace_(x, min, max, seed, diag_num, diag_step, diag_val);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("uniform_inplace_", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(x, x_autograd_meta, require_any_grad);

  // Bump Inplace Version
  x.bump_inplace_version();
  VLOG(3) << "Tensor(" << x.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("uniform_inplace node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UniformInplaceGradNode>(new UniformInplaceGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributemin(min);
    grad_node->SetAttributemax(max);
    grad_node->SetAttributeseed(seed);
    grad_node->SetAttributediag_num(diag_num);
    grad_node->SetAttributediag_step(diag_step);
    grad_node->SetAttributediag_val(diag_val);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: uniform_inplace_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> unique_ad_func(const paddle::experimental::Tensor& x, bool return_index, bool return_inverse, bool return_counts, std::vector<int> axis, paddle::experimental::DataType dtype) {
  VLOG(3) << "Running AD API: " << "unique";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unique dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unique");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unique_ad_func(new_x, return_index, return_inverse, return_counts, axis, dtype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unique");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = unique_ad_func(new_x, return_index, return_inverse, return_counts, axis, dtype);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& indices = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&indices);
        auto& inverse = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&inverse);
        auto& counts = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&counts);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices, inverse, counts};
  }

  VLOG(5) << "Running C++ API: " << "unique";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::unique(x, return_index, return_inverse, return_counts, axis, dtype);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& indices = std::get<1>(api_result);
  auto& inverse = std::get<2>(api_result);
  auto& counts = std::get<3>(api_result);

  VLOG(4) << "Finish AD API: unique";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string output_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    output_str += output_indices_str; 
    const char* TENSOR_INVERSE_TEMPLATE = " \n( inverse , [%s]), ";
    std::string output_inverse_str = paddle::string::Sprintf(TENSOR_INVERSE_TEMPLATE, egr::EagerUtils::TensorStr(inverse));
    output_str += output_inverse_str; 
    const char* TENSOR_COUNTS_TEMPLATE = " \n( counts , [%s]), ";
    std::string output_counts_str = paddle::string::Sprintf(TENSOR_COUNTS_TEMPLATE, egr::EagerUtils::TensorStr(counts));
    output_str += output_counts_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, indices, inverse, counts};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> unique_consecutive_ad_func(const paddle::experimental::Tensor& x, bool return_inverse, bool return_counts, std::vector<int> axis, int dtype) {
  VLOG(3) << "Running AD API: " << "unique_consecutive";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unique_consecutive dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unique_consecutive");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unique_consecutive_ad_func(new_x, return_inverse, return_counts, axis, dtype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("unique_consecutive");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = unique_consecutive_ad_func(new_x, return_inverse, return_counts, axis, dtype);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& index = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&index);
        auto& counts = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&counts);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, counts};
  }

  VLOG(5) << "Running C++ API: " << "unique_consecutive";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::unique_consecutive(x, return_inverse, return_counts, axis, dtype);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& index = std::get<1>(api_result);
  auto& counts = std::get<2>(api_result);

  VLOG(4) << "Finish AD API: unique_consecutive";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string output_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    output_str += output_index_str; 
    const char* TENSOR_COUNTS_TEMPLATE = " \n( counts , [%s]), ";
    std::string output_counts_str = paddle::string::Sprintf(TENSOR_COUNTS_TEMPLATE, egr::EagerUtils::TensorStr(counts));
    output_str += output_counts_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, index, counts};
}


paddle::experimental::Tensor unpool_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& indices, std::vector<int> ksize, std::vector<int> strides, std::vector<int> padding, paddle::experimental::IntArray output_size, std::string data_format) {
  VLOG(3) << "Running AD API: " << "unpool";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unpool dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unpool");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{indices} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unpool_ad_func(new_x, new_indices, ksize, strides, padding, output_size, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{indices} };
    
    auto op_name = phi::TransToFluidOpName("unpool");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_indices = transformer->TransInTensor("indices", indices);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = unpool_ad_func(new_x, new_indices, ksize, strides, padding, output_size, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unpool";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unpool(x, indices, ksize, strides, padding, output_size, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unpool", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unpool node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<UnpoolGradNode>(new UnpoolGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeksize(ksize);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepadding(padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: unpool";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor unpool3d_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& indices, std::vector<int> ksize, std::vector<int> strides, std::vector<int> padding, std::vector<int> output_size, std::string data_format) {
  VLOG(3) << "Running AD API: " << "unpool3d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("unpool3d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("unpool3d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{indices} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return unpool3d_ad_func(new_x, new_indices, ksize, strides, padding, output_size, data_format);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{indices} };
    
    auto op_name = phi::TransToFluidOpName("unpool3d");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_format);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_indices = transformer->TransInTensor("indices", indices);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = unpool3d_ad_func(new_x, new_indices, ksize, strides, padding, output_size, data_format);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "unpool3d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::unpool3d(x, indices, ksize, strides, padding, output_size, data_format);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unpool3d", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("unpool3d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Unpool3dGradNode>(new Unpool3dGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeksize(ksize);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepadding(padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: unpool3d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> update_loss_scaling__ad_func(std::vector<paddle::experimental::Tensor>& x, const paddle::experimental::Tensor& found_infinite, paddle::experimental::Tensor& prev_loss_scaling, paddle::experimental::Tensor& in_good_steps, paddle::experimental::Tensor& in_bad_steps, int incr_every_n_steps, int decr_every_n_nan_or_inf, float incr_ratio, float decr_ratio, paddle::experimental::Scalar stop_update) {
  VLOG(3) << "Running AD API: " << "update_loss_scaling_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("update_loss_scaling_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for update_loss_scaling__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { x,{found_infinite},{prev_loss_scaling},{in_good_steps},{in_bad_steps} };
    
    auto op_name = phi::TransToFluidOpName("update_loss_scaling_");
    auto transformer = egr::EagerLayoutAutotune<paddle::experimental::Scalar>(op_name, tensors_vector, &stop_update);
    auto new_x = transformer->TransInTensors("x", x);
    auto new_found_infinite = transformer->TransInTensor("found_infinite", found_infinite);
    auto new_prev_loss_scaling = transformer->TransInTensor("prev_loss_scaling", prev_loss_scaling);
    auto new_in_good_steps = transformer->TransInTensor("in_good_steps", in_good_steps);
    auto new_in_bad_steps = transformer->TransInTensor("in_bad_steps", in_bad_steps);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&> api_result = update_loss_scaling__ad_func(new_x, new_found_infinite, new_prev_loss_scaling, new_in_good_steps, new_in_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& loss_scaling = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&loss_scaling);
        auto& out_good_steps = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&out_good_steps);
        auto& out_bad_steps = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&out_bad_steps);

    // Returns
    return std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{out, loss_scaling, out_good_steps, out_bad_steps};
  }

  VLOG(5) << "Running C++ API: " << "update_loss_scaling_";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FOUND_INFINITE_TEMPLATE = " \n( found_infinite , [%s]), ";
    std::string input_found_infinite_str = paddle::string::Sprintf(TENSOR_FOUND_INFINITE_TEMPLATE, egr::EagerUtils::TensorStr(found_infinite));
    input_str += input_found_infinite_str; 
    const char* TENSOR_PREV_LOSS_SCALING_TEMPLATE = " \n( prev_loss_scaling , [%s]), ";
    std::string input_prev_loss_scaling_str = paddle::string::Sprintf(TENSOR_PREV_LOSS_SCALING_TEMPLATE, egr::EagerUtils::TensorStr(prev_loss_scaling));
    input_str += input_prev_loss_scaling_str; 
    const char* TENSOR_IN_GOOD_STEPS_TEMPLATE = " \n( in_good_steps , [%s]), ";
    std::string input_in_good_steps_str = paddle::string::Sprintf(TENSOR_IN_GOOD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(in_good_steps));
    input_str += input_in_good_steps_str; 
    const char* TENSOR_IN_BAD_STEPS_TEMPLATE = " \n( in_bad_steps , [%s]), ";
    std::string input_in_bad_steps_str = paddle::string::Sprintf(TENSOR_IN_BAD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(in_bad_steps));
    input_str += input_in_bad_steps_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::update_loss_scaling_(x, found_infinite, prev_loss_scaling, in_good_steps, in_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update);
  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& loss_scaling = std::get<1>(api_result);
  auto& out_good_steps = std::get<2>(api_result);
  auto& out_bad_steps = std::get<3>(api_result);

  VLOG(4) << "Finish AD API: update_loss_scaling_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FOUND_INFINITE_TEMPLATE = " \n( found_infinite , [%s]), ";
    std::string input_found_infinite_str = paddle::string::Sprintf(TENSOR_FOUND_INFINITE_TEMPLATE, egr::EagerUtils::TensorStr(found_infinite));
    input_str += input_found_infinite_str; 
    const char* TENSOR_PREV_LOSS_SCALING_TEMPLATE = " \n( prev_loss_scaling , [%s]), ";
    std::string input_prev_loss_scaling_str = paddle::string::Sprintf(TENSOR_PREV_LOSS_SCALING_TEMPLATE, egr::EagerUtils::TensorStr(prev_loss_scaling));
    input_str += input_prev_loss_scaling_str; 
    const char* TENSOR_IN_GOOD_STEPS_TEMPLATE = " \n( in_good_steps , [%s]), ";
    std::string input_in_good_steps_str = paddle::string::Sprintf(TENSOR_IN_GOOD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(in_good_steps));
    input_str += input_in_good_steps_str; 
    const char* TENSOR_IN_BAD_STEPS_TEMPLATE = " \n( in_bad_steps , [%s]), ";
    std::string input_in_bad_steps_str = paddle::string::Sprintf(TENSOR_IN_BAD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(in_bad_steps));
    input_str += input_in_bad_steps_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_LOSS_SCALING_TEMPLATE = " \n( loss_scaling , [%s]), ";
    std::string output_loss_scaling_str = paddle::string::Sprintf(TENSOR_LOSS_SCALING_TEMPLATE, egr::EagerUtils::TensorStr(loss_scaling));
    output_str += output_loss_scaling_str; 
    const char* TENSOR_OUT_GOOD_STEPS_TEMPLATE = " \n( out_good_steps , [%s]), ";
    std::string output_out_good_steps_str = paddle::string::Sprintf(TENSOR_OUT_GOOD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(out_good_steps));
    output_str += output_out_good_steps_str; 
    const char* TENSOR_OUT_BAD_STEPS_TEMPLATE = " \n( out_bad_steps , [%s]), ";
    std::string output_out_bad_steps_str = paddle::string::Sprintf(TENSOR_OUT_BAD_STEPS_TEMPLATE, egr::EagerUtils::TensorStr(out_bad_steps));
    output_str += output_out_bad_steps_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<std::vector<paddle::experimental::Tensor>&, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor&>{out, loss_scaling, out_good_steps, out_bad_steps};
}


paddle::experimental::Tensor warpctc_ad_func(const paddle::experimental::Tensor& logits, const paddle::experimental::Tensor& label, const paddle::optional<paddle::experimental::Tensor>& logits_length, const paddle::optional<paddle::experimental::Tensor>& labels_length, int blank, bool norm_by_times) {
  VLOG(3) << "Running AD API: " << "warpctc";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("warpctc dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("warpctc");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {logits},{label} };
    if (logits_length) amp_tensors_vector.push_back({ *logits_length });
    if (labels_length) amp_tensors_vector.push_back({ *labels_length });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_logits = egr::EagerAmpAutoCast("logits", logits, amp_dst_dtype, op_name);
    auto new_label = egr::EagerAmpAutoCast("label", label, amp_dst_dtype, op_name);
    auto new_logits_length = egr::EagerAmpAutoCast("logits_length", logits_length, amp_dst_dtype, op_name);
    auto new_labels_length = egr::EagerAmpAutoCast("labels_length", labels_length, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return warpctc_ad_func(new_logits, new_label, new_logits_length, new_labels_length, blank, norm_by_times);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {logits},{label} };
    if (logits_length) tensors_vector.push_back({ *logits_length });
    if (labels_length) tensors_vector.push_back({ *labels_length });

    auto op_name = phi::TransToFluidOpName("warpctc");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_logits = transformer->TransInTensor("logits", logits);
    auto new_label = transformer->TransInTensor("label", label);
auto new_logits_length = transformer->TransInTensor("logits_length", logits_length);
    auto new_labels_length = transformer->TransInTensor("labels_length", labels_length);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor loss = warpctc_ad_func(new_logits, new_label, new_logits_length, new_labels_length, blank, norm_by_times);

    transformer -> SetOutTensorLayout(&loss);

    // Returns
    return loss;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* logits_autograd_meta = egr::EagerUtils::nullable_autograd_meta(logits);

  VLOG(5) << "Running C++ API: " << "warpctc";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_LOGITS_LENGTH_TEMPLATE = " \n( logits_length , [%s]), ";
    std::string input_logits_length_str = paddle::string::Sprintf(TENSOR_LOGITS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(logits_length));
    input_str += input_logits_length_str; 
    const char* TENSOR_LABELS_LENGTH_TEMPLATE = " \n( labels_length , [%s]), ";
    std::string input_labels_length_str = paddle::string::Sprintf(TENSOR_LABELS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(labels_length));
    input_str += input_labels_length_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::warpctc_intermediate(logits, label, logits_length, labels_length, blank, norm_by_times);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("warpctc_intermediate", api_result); }

  // Get Outputs
  auto& loss = std::get<0>(api_result);
  auto& warpctcgrad = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* loss_autograd_meta = egr::EagerUtils::autograd_meta(&loss);
  egr::AutogradMeta* warpctcgrad_autograd_meta = egr::EagerUtils::autograd_meta(&warpctcgrad);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,logits_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("warpctc node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,loss_autograd_meta,warpctcgrad_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<WarpctcGradNode>(new WarpctcGradNode(2, 4));
    // SetAttributes if needed
    grad_node->SetAttributeblank(blank);
    grad_node->SetAttributenorm_by_times(norm_by_times);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperlogits(logits);
    if(logits_length) grad_node->SetTensorWrapperlogits_length(*logits_length);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(logits, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (loss_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(loss_autograd_meta, 0);
    }
    if (warpctcgrad_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(warpctcgrad_autograd_meta, 1);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetHistory(loss_autograd_meta, grad_node);
    }
    if (warpctcgrad_autograd_meta) {
      egr::EagerUtils::SetHistory(warpctcgrad_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(loss, 0);
    grad_node->SetGradInMeta(warpctcgrad, 1);
    egr::EagerUtils::CheckAndRetainGrad(loss);
    egr::EagerUtils::CheckAndRetainGrad(warpctcgrad);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperwarpctcgrad(warpctcgrad);
  }


  VLOG(4) << "Finish AD API: warpctc";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_LOGITS_LENGTH_TEMPLATE = " \n( logits_length , [%s]), ";
    std::string input_logits_length_str = paddle::string::Sprintf(TENSOR_LOGITS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(logits_length));
    input_str += input_logits_length_str; 
    const char* TENSOR_LABELS_LENGTH_TEMPLATE = " \n( labels_length , [%s]), ";
    std::string input_labels_length_str = paddle::string::Sprintf(TENSOR_LABELS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(labels_length));
    input_str += input_labels_length_str; 
    const char* TENSOR_LOSS_TEMPLATE = " \n( loss , [%s]), ";
    std::string output_loss_str = paddle::string::Sprintf(TENSOR_LOSS_TEMPLATE, egr::EagerUtils::TensorStr(loss));
    output_str += output_loss_str; 
    const char* TENSOR_WARPCTCGRAD_TEMPLATE = " \n( warpctcgrad , [%s]), ";
    std::string output_warpctcgrad_str = paddle::string::Sprintf(TENSOR_WARPCTCGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warpctcgrad));
    output_str += output_warpctcgrad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return loss;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> yolo_box_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& img_size, std::vector<int> anchors, int class_num, float conf_thresh, int downsample_ratio, bool clip_bbox, float scale_x_y, bool iou_aware, float iou_aware_factor) {
  VLOG(3) << "Running AD API: " << "yolo_box";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("yolo_box dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("yolo_box");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{img_size} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_img_size = egr::EagerAmpAutoCast("img_size", img_size, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return yolo_box_ad_func(new_x, new_img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{img_size} };
    
    auto op_name = phi::TransToFluidOpName("yolo_box");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_img_size = transformer->TransInTensor("img_size", img_size);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = yolo_box_ad_func(new_x, new_img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor);

        auto& boxes = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&boxes);
        auto& scores = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&scores);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{boxes, scores};
  }

  VLOG(5) << "Running C++ API: " << "yolo_box";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_IMG_SIZE_TEMPLATE = " \n( img_size , [%s]), ";
    std::string input_img_size_str = paddle::string::Sprintf(TENSOR_IMG_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(img_size));
    input_str += input_img_size_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::yolo_box(x, img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor);
  // Get Outputs
  auto& boxes = std::get<0>(api_result);
  auto& scores = std::get<1>(api_result);

  VLOG(4) << "Finish AD API: yolo_box";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_IMG_SIZE_TEMPLATE = " \n( img_size , [%s]), ";
    std::string input_img_size_str = paddle::string::Sprintf(TENSOR_IMG_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(img_size));
    input_str += input_img_size_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string output_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    output_str += output_boxes_str; 
    const char* TENSOR_SCORES_TEMPLATE = " \n( scores , [%s]), ";
    std::string output_scores_str = paddle::string::Sprintf(TENSOR_SCORES_TEMPLATE, egr::EagerUtils::TensorStr(scores));
    output_str += output_scores_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor>{boxes, scores};
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> yolo_loss_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& gt_box, const paddle::experimental::Tensor& gt_label, const paddle::optional<paddle::experimental::Tensor>& gt_score, std::vector<int> anchors, std::vector<int> anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y) {
  VLOG(3) << "Running AD API: " << "yolo_loss";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("yolo_loss dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("yolo_loss");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{gt_box},{gt_label} };
    if (gt_score) amp_tensors_vector.push_back({ *gt_score });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_gt_box = egr::EagerAmpAutoCast("gt_box", gt_box, amp_dst_dtype, op_name);
    auto new_gt_label = egr::EagerAmpAutoCast("gt_label", gt_label, amp_dst_dtype, op_name);
    auto new_gt_score = egr::EagerAmpAutoCast("gt_score", gt_score, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return yolo_loss_ad_func(new_x, new_gt_box, new_gt_label, new_gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{gt_box},{gt_label} };
    if (gt_score) tensors_vector.push_back({ *gt_score });

    auto op_name = phi::TransToFluidOpName("yolo_loss");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_gt_box = transformer->TransInTensor("gt_box", gt_box);
    auto new_gt_label = transformer->TransInTensor("gt_label", gt_label);
auto new_gt_score = transformer->TransInTensor("gt_score", gt_score);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = yolo_loss_ad_func(new_x, new_gt_box, new_gt_label, new_gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y);

        auto& loss = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&loss);
        auto& objectness_mask = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&objectness_mask);
        auto& gt_match_mask = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&gt_match_mask);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{loss, objectness_mask, gt_match_mask};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* gt_box_autograd_meta = egr::EagerUtils::nullable_autograd_meta(gt_box);
  egr::AutogradMeta* gt_label_autograd_meta = egr::EagerUtils::nullable_autograd_meta(gt_label);
  egr::AutogradMeta* gt_score_autograd_meta = egr::EagerUtils::nullable_autograd_meta(gt_score);

  VLOG(5) << "Running C++ API: " << "yolo_loss";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GT_BOX_TEMPLATE = " \n( gt_box , [%s]), ";
    std::string input_gt_box_str = paddle::string::Sprintf(TENSOR_GT_BOX_TEMPLATE, egr::EagerUtils::TensorStr(gt_box));
    input_str += input_gt_box_str; 
    const char* TENSOR_GT_LABEL_TEMPLATE = " \n( gt_label , [%s]), ";
    std::string input_gt_label_str = paddle::string::Sprintf(TENSOR_GT_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(gt_label));
    input_str += input_gt_label_str; 
    const char* TENSOR_GT_SCORE_TEMPLATE = " \n( gt_score , [%s]), ";
    std::string input_gt_score_str = paddle::string::Sprintf(TENSOR_GT_SCORE_TEMPLATE, egr::EagerUtils::TensorStr(gt_score));
    input_str += input_gt_score_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::yolo_loss(x, gt_box, gt_label, gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("yolo_loss", api_result); }

  // Get Outputs
  auto& loss = std::get<0>(api_result);
  auto& objectness_mask = std::get<1>(api_result);
  auto& gt_match_mask = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* loss_autograd_meta = egr::EagerUtils::autograd_meta(&loss);
  egr::AutogradMeta* objectness_mask_autograd_meta = egr::EagerUtils::autograd_meta(&objectness_mask);
  egr::AutogradMeta* gt_match_mask_autograd_meta = egr::EagerUtils::autograd_meta(&gt_match_mask);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,gt_box_autograd_meta,gt_label_autograd_meta,gt_score_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("yolo_loss node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,loss_autograd_meta,objectness_mask_autograd_meta,gt_match_mask_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<YoloLossGradNode>(new YoloLossGradNode(3, 4));
    // SetAttributes if needed
    grad_node->SetAttributeanchors(anchors);
    grad_node->SetAttributeanchor_mask(anchor_mask);
    grad_node->SetAttributeclass_num(class_num);
    grad_node->SetAttributeignore_thresh(ignore_thresh);
    grad_node->SetAttributedownsample_ratio(downsample_ratio);
    grad_node->SetAttributeuse_label_smooth(use_label_smooth);
    grad_node->SetAttributescale_x_y(scale_x_y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergt_box(gt_box);
    grad_node->SetTensorWrappergt_label(gt_label);
    if(gt_score) grad_node->SetTensorWrappergt_score(*gt_score);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(gt_box, 1);
    grad_node->SetGradOutMeta(gt_label, 2);
    if(gt_score.get_ptr() != nullptr) grad_node->SetGradOutMeta(*(gt_score.get_ptr()), 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (loss_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(loss_autograd_meta, 0);
    }
    if (objectness_mask_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(objectness_mask_autograd_meta, 1);
    }
    if (gt_match_mask_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(gt_match_mask_autograd_meta, 2);
    }
    if (loss_autograd_meta) {
      egr::EagerUtils::SetHistory(loss_autograd_meta, grad_node);
    }
    if (objectness_mask_autograd_meta) {
      egr::EagerUtils::SetHistory(objectness_mask_autograd_meta, grad_node);
    }
    if (gt_match_mask_autograd_meta) {
      egr::EagerUtils::SetHistory(gt_match_mask_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(loss, 0);
    grad_node->SetGradInMeta(objectness_mask, 1);
    grad_node->SetGradInMeta(gt_match_mask, 2);
    egr::EagerUtils::CheckAndRetainGrad(loss);
    egr::EagerUtils::CheckAndRetainGrad(objectness_mask);
    egr::EagerUtils::CheckAndRetainGrad(gt_match_mask);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperobjectness_mask(objectness_mask);
    grad_node->SetTensorWrappergt_match_mask(gt_match_mask);
  }


  VLOG(4) << "Finish AD API: yolo_loss";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GT_BOX_TEMPLATE = " \n( gt_box , [%s]), ";
    std::string input_gt_box_str = paddle::string::Sprintf(TENSOR_GT_BOX_TEMPLATE, egr::EagerUtils::TensorStr(gt_box));
    input_str += input_gt_box_str; 
    const char* TENSOR_GT_LABEL_TEMPLATE = " \n( gt_label , [%s]), ";
    std::string input_gt_label_str = paddle::string::Sprintf(TENSOR_GT_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(gt_label));
    input_str += input_gt_label_str; 
    const char* TENSOR_GT_SCORE_TEMPLATE = " \n( gt_score , [%s]), ";
    std::string input_gt_score_str = paddle::string::Sprintf(TENSOR_GT_SCORE_TEMPLATE, egr::EagerUtils::TensorStr(gt_score));
    input_str += input_gt_score_str; 
    const char* TENSOR_LOSS_TEMPLATE = " \n( loss , [%s]), ";
    std::string output_loss_str = paddle::string::Sprintf(TENSOR_LOSS_TEMPLATE, egr::EagerUtils::TensorStr(loss));
    output_str += output_loss_str; 
    const char* TENSOR_OBJECTNESS_MASK_TEMPLATE = " \n( objectness_mask , [%s]), ";
    std::string output_objectness_mask_str = paddle::string::Sprintf(TENSOR_OBJECTNESS_MASK_TEMPLATE, egr::EagerUtils::TensorStr(objectness_mask));
    output_str += output_objectness_mask_str; 
    const char* TENSOR_GT_MATCH_MASK_TEMPLATE = " \n( gt_match_mask , [%s]), ";
    std::string output_gt_match_mask_str = paddle::string::Sprintf(TENSOR_GT_MATCH_MASK_TEMPLATE, egr::EagerUtils::TensorStr(gt_match_mask));
    output_str += output_gt_match_mask_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{loss, objectness_mask, gt_match_mask};
}


paddle::experimental::Tensor zeros_ad_func(paddle::experimental::IntArray shape, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "zeros";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("zeros dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for zeros_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "zeros";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::zeros(shape, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: zeros";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor zeros_like_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::DataType dtype, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "zeros_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("zeros_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("zeros_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return zeros_like_ad_func(new_x, dtype, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("zeros_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = zeros_like_ad_func(new_x, dtype, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "zeros_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::zeros_like(x, dtype, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: zeros_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}



namespace sparse {
    
paddle::experimental::Tensor abs_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "abs";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("abs dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("abs");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return abs_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("abs");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = abs_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "abs";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::abs(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("abs", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("abs node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AbsGradNode>(new AbsGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: abs";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor acos_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "acos";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("acos dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("acos");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return acos_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("acos");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = acos_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "acos";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::acos(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acos", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("acos node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AcosGradNode>(new AcosGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: acos";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor acosh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "acosh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("acosh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("acosh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return acosh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("acosh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = acosh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "acosh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::acosh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acosh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("acosh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AcoshGradNode>(new AcoshGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: acosh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor add_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "add";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("add dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("add");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return add_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("add");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = add_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "add";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::add(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("add node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AddGradNode>(new AddGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: add";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor asin_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "asin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("asin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("asin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return asin_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("asin");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = asin_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "asin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::asin(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("asin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsinGradNode>(new AsinGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: asin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor asinh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "asinh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("asinh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("asinh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return asinh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("asinh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = asinh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "asinh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::asinh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asinh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("asinh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AsinhGradNode>(new AsinhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: asinh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor atan_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "atan";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("atan dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("atan");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return atan_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("atan");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = atan_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "atan";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::atan(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("atan node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AtanGradNode>(new AtanGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: atan";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor atanh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "atanh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("atanh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("atanh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return atanh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("atanh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = atanh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "atanh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::atanh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atanh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("atanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AtanhGradNode>(new AtanhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: atanh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> batch_norm__ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Tensor& mean, paddle::experimental::Tensor& variance, const paddle::experimental::Tensor& scale, const paddle::experimental::Tensor& bias, bool is_test, float momentum, float epsilon, std::string data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(3) << "Running AD API: " << "batch_norm_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("batch_norm_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for batch_norm__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{mean},{variance},{scale},{bias} };
    
    auto op_name = phi::TransToFluidOpName("batch_norm_");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_mean = transformer->TransInTensor("mean", mean);
    auto new_variance = transformer->TransInTensor("variance", variance);
    auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = batch_norm__ad_func(new_x, new_mean, new_variance, new_scale, new_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mean_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mean_out);
        auto& variance_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&variance_out);
        auto& saved_mean = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&saved_mean);
        auto& saved_variance = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&saved_variance);
        auto& reserve_space = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&reserve_space);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* mean_autograd_meta = egr::EagerUtils::nullable_autograd_meta(mean);
  egr::AutogradMeta* variance_autograd_meta = egr::EagerUtils::nullable_autograd_meta(variance);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "batch_norm_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::batch_norm_(x, mean, variance, scale, bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("batch_norm_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mean_out = std::get<1>(api_result);
  auto& variance_out = std::get<2>(api_result);
  auto& saved_mean = std::get<3>(api_result);
  auto& saved_variance = std::get<4>(api_result);
  auto& reserve_space = std::get<5>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mean_out_autograd_meta = egr::EagerUtils::autograd_meta(&mean_out);
  egr::AutogradMeta* variance_out_autograd_meta = egr::EagerUtils::autograd_meta(&variance_out);
  egr::AutogradMeta* saved_mean_autograd_meta = egr::EagerUtils::autograd_meta(&saved_mean);
  egr::AutogradMeta* saved_variance_autograd_meta = egr::EagerUtils::autograd_meta(&saved_variance);
  egr::AutogradMeta* reserve_space_autograd_meta = egr::EagerUtils::autograd_meta(&reserve_space);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,mean_autograd_meta,variance_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(mean, mean_autograd_meta, require_any_grad);

  egr::EagerUtils::CheckInplace(variance, variance_autograd_meta, require_any_grad);

  // Bump Inplace Version
  mean.bump_inplace_version();
  VLOG(3) << "Tensor(" << mean.name() << ") uses Inplace Strategy.";

  // Bump Inplace Version
  variance.bump_inplace_version();
  VLOG(3) << "Tensor(" << variance.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("batch_norm_ node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mean_out_autograd_meta,variance_out_autograd_meta,saved_mean_autograd_meta,saved_variance_autograd_meta,reserve_space_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<BatchNormGradNode>(new BatchNormGradNode(6, 5));
    // SetAttributes if needed
    grad_node->SetAttributemomentum(momentum);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributeuse_global_stats(use_global_stats);
    grad_node->SetAttributetrainable_statistics(trainable_statistics);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperscale(scale);
    grad_node->SetTensorWrapperbias(bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(scale, 3);
    grad_node->SetGradOutMeta(bias, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_out_autograd_meta, 1);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_out_autograd_meta, 2);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_mean_autograd_meta, 3);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_variance_autograd_meta, 4);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(reserve_space_autograd_meta, 5);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_out_autograd_meta, grad_node);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_out_autograd_meta, grad_node);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_mean_autograd_meta, grad_node);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_variance_autograd_meta, grad_node);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetHistory(reserve_space_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mean_out, 1);
    grad_node->SetGradInMeta(variance_out, 2);
    grad_node->SetGradInMeta(saved_mean, 3);
    grad_node->SetGradInMeta(saved_variance, 4);
    grad_node->SetGradInMeta(reserve_space, 5);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mean_out);
    egr::EagerUtils::CheckAndRetainGrad(variance_out);
    egr::EagerUtils::CheckAndRetainGrad(saved_mean);
    egr::EagerUtils::CheckAndRetainGrad(saved_variance);
    egr::EagerUtils::CheckAndRetainGrad(reserve_space);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappermean_out(mean_out);
    grad_node->SetTensorWrappervariance_out(variance_out);
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrapperreserve_space(reserve_space);
  }


  VLOG(4) << "Finish AD API: batch_norm_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string output_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    output_str += output_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string output_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    output_str += output_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string output_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    output_str += output_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string output_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    output_str += output_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string output_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    output_str += output_reserve_space_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
}


paddle::experimental::Tensor cast_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::DataType index_dtype, paddle::experimental::DataType value_dtype) {
  VLOG(3) << "Running AD API: " << "cast";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("cast dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for cast_ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("cast");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = cast_ad_func(new_x, index_dtype, value_dtype);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "cast";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::cast(x, index_dtype, value_dtype);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cast", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("cast node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<CastGradNode>(new CastGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributevalue_dtype(value_dtype);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: cast";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor conv3d_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& kernel, std::vector<int> paddings, std::vector<int> dilations, std::vector<int> strides, int groups, bool subm, std::string key) {
  VLOG(3) << "Running AD API: " << "conv3d";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("conv3d dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("conv3d");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{kernel} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_kernel = egr::EagerAmpAutoCast("kernel", kernel, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return conv3d_ad_func(new_x, new_kernel, paddings, dilations, strides, groups, subm, key);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{kernel} };
    
    auto op_name = phi::TransToFluidOpName("conv3d");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_kernel = transformer->TransInTensor("kernel", kernel);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = conv3d_ad_func(new_x, new_kernel, paddings, dilations, strides, groups, subm, key);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* kernel_autograd_meta = egr::EagerUtils::nullable_autograd_meta(kernel);

  VLOG(5) << "Running C++ API: " << "conv3d";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_KERNEL_TEMPLATE = " \n( kernel , [%s]), ";
    std::string input_kernel_str = paddle::string::Sprintf(TENSOR_KERNEL_TEMPLATE, egr::EagerUtils::TensorStr(kernel));
    input_str += input_kernel_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::conv3d_intermediate(x, kernel, paddings, dilations, strides, groups, subm, key);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& rulebook = std::get<1>(api_result);
  auto& counter = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* rulebook_autograd_meta = egr::EagerUtils::autograd_meta(&rulebook);
  egr::AutogradMeta* counter_autograd_meta = egr::EagerUtils::autograd_meta(&counter);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,kernel_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("conv3d node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,rulebook_autograd_meta,counter_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv3dGradNode>(new Conv3dGradNode(3, 2));
    // SetAttributes if needed
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributesubm(subm);
    grad_node->SetAttributekey(key);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperkernel(kernel);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(kernel, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (rulebook_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(rulebook_autograd_meta, 1);
    }
    if (counter_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(counter_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (rulebook_autograd_meta) {
      egr::EagerUtils::SetHistory(rulebook_autograd_meta, grad_node);
    }
    if (counter_autograd_meta) {
      egr::EagerUtils::SetHistory(counter_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(rulebook, 1);
    grad_node->SetGradInMeta(counter, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(rulebook);
    egr::EagerUtils::CheckAndRetainGrad(counter);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperrulebook(rulebook);
    grad_node->SetTensorWrappercounter(counter);
  }


  VLOG(4) << "Finish AD API: conv3d";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_KERNEL_TEMPLATE = " \n( kernel , [%s]), ";
    std::string input_kernel_str = paddle::string::Sprintf(TENSOR_KERNEL_TEMPLATE, egr::EagerUtils::TensorStr(kernel));
    input_str += input_kernel_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string output_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    output_str += output_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string output_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    output_str += output_counter_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor divide_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "divide";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("divide dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("divide");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return divide_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("divide");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = divide_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "divide";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::divide(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("divide node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DivideGradNode>(new DivideGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: divide";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor divide_scalar_ad_func(const paddle::experimental::Tensor& x, float scalar) {
  VLOG(3) << "Running AD API: " << "divide_scalar";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("divide_scalar dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("divide_scalar");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return divide_scalar_ad_func(new_x, scalar);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("divide_scalar");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = divide_scalar_ad_func(new_x, scalar);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "divide_scalar";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::divide_scalar(x, scalar);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide_scalar", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("divide_scalar node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<DivideScalarGradNode>(new DivideScalarGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributescalar(scalar);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: divide_scalar";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor expm1_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "expm1";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("expm1 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("expm1");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return expm1_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("expm1");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = expm1_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "expm1";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::expm1(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expm1", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("expm1 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Expm1GradNode>(new Expm1GradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: expm1";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor leaky_relu_ad_func(const paddle::experimental::Tensor& x, float alpha) {
  VLOG(3) << "Running AD API: " << "leaky_relu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("leaky_relu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("leaky_relu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return leaky_relu_ad_func(new_x, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("leaky_relu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = leaky_relu_ad_func(new_x, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "leaky_relu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::leaky_relu(x, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("leaky_relu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("leaky_relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<LeakyReluGradNode>(new LeakyReluGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: leaky_relu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor log1p_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "log1p";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("log1p dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("log1p");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return log1p_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("log1p");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = log1p_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "log1p";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::log1p(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log1p", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("log1p node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Log1pGradNode>(new Log1pGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: log1p";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor multiply_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "multiply";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("multiply dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("multiply");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return multiply_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("multiply");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = multiply_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "multiply";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::multiply(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("multiply node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiplyGradNode>(new MultiplyGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: multiply";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor pow_ad_func(const paddle::experimental::Tensor& x, float factor) {
  VLOG(3) << "Running AD API: " << "pow";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("pow dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("pow");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return pow_ad_func(new_x, factor);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("pow");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = pow_ad_func(new_x, factor);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "pow";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::pow(x, factor);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("pow node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<PowGradNode>(new PowGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributefactor(factor);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: pow";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor relu_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "relu";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("relu dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("relu");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return relu_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("relu");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = relu_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "relu";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::relu(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("relu node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReluGradNode>(new ReluGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: relu";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor relu6_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "relu6";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("relu6 dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("relu6");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return relu6_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("relu6");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = relu6_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "relu6";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::relu6(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu6", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("relu6 node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<Relu6GradNode>(new Relu6GradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributethreshold(6);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: relu6";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor reshape_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::IntArray shape) {
  VLOG(3) << "Running AD API: " << "reshape";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("reshape dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("reshape");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return reshape_ad_func(new_x, shape);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("reshape");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = reshape_ad_func(new_x, shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "reshape";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::reshape(x, shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("reshape node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ReshapeGradNode>(new ReshapeGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: reshape";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor scale_ad_func(const paddle::experimental::Tensor& x, float scale, float bias, bool bias_after_scale) {
  VLOG(3) << "Running AD API: " << "scale";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("scale dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("scale");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return scale_ad_func(new_x, scale, bias, bias_after_scale);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("scale");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = scale_ad_func(new_x, scale, bias, bias_after_scale);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "scale";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::scale(x, scale, bias, bias_after_scale);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scale", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("scale node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ScaleGradNode>(new ScaleGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributescale(scale);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: scale";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sin_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sin";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sin dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sin");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sin_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sin");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sin_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sin";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::sin(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sin node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SinGradNode>(new SinGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sin";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sinh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sinh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sinh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sinh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sinh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sinh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sinh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sinh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::sinh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sinh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sinh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SinhGradNode>(new SinhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sinh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor softmax_ad_func(const paddle::experimental::Tensor& x, int axis) {
  VLOG(3) << "Running AD API: " << "softmax";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("softmax dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("softmax");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return softmax_ad_func(new_x, axis);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("softmax");
    auto transformer = egr::EagerLayoutAutotune<int>(op_name, tensors_vector, &axis);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = softmax_ad_func(new_x, axis);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "softmax";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::softmax(x, axis);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softmax", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("softmax node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SoftmaxGradNode>(new SoftmaxGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: softmax";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sparse_coo_tensor_ad_func(const paddle::experimental::Tensor& values, const paddle::experimental::Tensor& indices, std::vector<int64_t> shape) {
  VLOG(3) << "Running AD API: " << "sparse_coo_tensor";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sparse_coo_tensor dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sparse_coo_tensor");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {values},{indices} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_values = egr::EagerAmpAutoCast("values", values, amp_dst_dtype, op_name);
    auto new_indices = egr::EagerAmpAutoCast("indices", indices, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sparse_coo_tensor_ad_func(new_values, new_indices, shape);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {values},{indices} };
    
    auto op_name = phi::TransToFluidOpName("sparse_coo_tensor");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_values = transformer->TransInTensor("values", values);
    auto new_indices = transformer->TransInTensor("indices", indices);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sparse_coo_tensor_ad_func(new_values, new_indices, shape);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* values_autograd_meta = egr::EagerUtils::nullable_autograd_meta(values);

  VLOG(5) << "Running C++ API: " << "sparse_coo_tensor";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_VALUES_TEMPLATE = " \n( values , [%s]), ";
    std::string input_values_str = paddle::string::Sprintf(TENSOR_VALUES_TEMPLATE, egr::EagerUtils::TensorStr(values));
    input_str += input_values_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::sparse_coo_tensor(values, indices, shape);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sparse_coo_tensor", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,values_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sparse_coo_tensor node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SparseCooTensorGradNode>(new SparseCooTensorGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperindices(indices);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(values, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: sparse_coo_tensor";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_VALUES_TEMPLATE = " \n( values , [%s]), ";
    std::string input_values_str = paddle::string::Sprintf(TENSOR_VALUES_TEMPLATE, egr::EagerUtils::TensorStr(values));
    input_str += input_values_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor sqrt_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "sqrt";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sqrt dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("sqrt");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return sqrt_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("sqrt");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = sqrt_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "sqrt";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::sqrt(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sqrt node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SqrtGradNode>(new SqrtGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: sqrt";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor square_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "square";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("square dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("square");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return square_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("square");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = square_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "square";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::square(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("square", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("square node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SquareGradNode>(new SquareGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: square";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor subtract_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "subtract";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("subtract dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("subtract");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return subtract_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("subtract");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = subtract_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "subtract";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::subtract(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("subtract node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SubtractGradNode>(new SubtractGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: subtract";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> sync_batch_norm__ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Tensor& mean, paddle::experimental::Tensor& variance, const paddle::experimental::Tensor& scale, const paddle::experimental::Tensor& bias, bool is_test, float momentum, float epsilon, std::string data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(3) << "Running AD API: " << "sync_batch_norm_";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("sync_batch_norm_ dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(5) << " No AMP for sync_batch_norm__ad_func because it is a inplace or cast api. "; 
  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{mean},{variance},{scale},{bias} };
    
    auto op_name = phi::TransToFluidOpName("sync_batch_norm_");
    auto transformer = egr::EagerLayoutAutotune<std::string>(op_name, tensors_vector, &data_layout);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_mean = transformer->TransInTensor("mean", mean);
    auto new_variance = transformer->TransInTensor("variance", variance);
    auto new_scale = transformer->TransInTensor("scale", scale);
    auto new_bias = transformer->TransInTensor("bias", bias);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor> api_result = sync_batch_norm__ad_func(new_x, new_mean, new_variance, new_scale, new_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);

        auto& out = std::get<0>(api_result);
        transformer -> SetOutTensorLayout(&out);
        auto& mean_out = std::get<1>(api_result);
        transformer -> SetOutTensorLayout(&mean_out);
        auto& variance_out = std::get<2>(api_result);
        transformer -> SetOutTensorLayout(&variance_out);
        auto& saved_mean = std::get<3>(api_result);
        transformer -> SetOutTensorLayout(&saved_mean);
        auto& saved_variance = std::get<4>(api_result);
        transformer -> SetOutTensorLayout(&saved_variance);
        auto& reserve_space = std::get<5>(api_result);
        transformer -> SetOutTensorLayout(&reserve_space);

    // Returns
    return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* mean_autograd_meta = egr::EagerUtils::nullable_autograd_meta(mean);
  egr::AutogradMeta* variance_autograd_meta = egr::EagerUtils::nullable_autograd_meta(variance);
  egr::AutogradMeta* scale_autograd_meta = egr::EagerUtils::nullable_autograd_meta(scale);
  egr::AutogradMeta* bias_autograd_meta = egr::EagerUtils::nullable_autograd_meta(bias);

  VLOG(5) << "Running C++ API: " << "sync_batch_norm_";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::sync_batch_norm_(x, mean, variance, scale, bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sync_batch_norm_", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& mean_out = std::get<1>(api_result);
  auto& variance_out = std::get<2>(api_result);
  auto& saved_mean = std::get<3>(api_result);
  auto& saved_variance = std::get<4>(api_result);
  auto& reserve_space = std::get<5>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* mean_out_autograd_meta = egr::EagerUtils::autograd_meta(&mean_out);
  egr::AutogradMeta* variance_out_autograd_meta = egr::EagerUtils::autograd_meta(&variance_out);
  egr::AutogradMeta* saved_mean_autograd_meta = egr::EagerUtils::autograd_meta(&saved_mean);
  egr::AutogradMeta* saved_variance_autograd_meta = egr::EagerUtils::autograd_meta(&saved_variance);
  egr::AutogradMeta* reserve_space_autograd_meta = egr::EagerUtils::autograd_meta(&reserve_space);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,mean_autograd_meta,variance_autograd_meta,scale_autograd_meta,bias_autograd_meta);

  // Check Inplace if needed

  egr::EagerUtils::CheckInplace(mean, mean_autograd_meta, require_any_grad);

  egr::EagerUtils::CheckInplace(variance, variance_autograd_meta, require_any_grad);

  // Bump Inplace Version
  mean.bump_inplace_version();
  VLOG(3) << "Tensor(" << mean.name() << ") uses Inplace Strategy.";

  // Bump Inplace Version
  variance.bump_inplace_version();
  VLOG(3) << "Tensor(" << variance.name() << ") uses Inplace Strategy.";

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("sync_batch_norm_ node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,mean_out_autograd_meta,variance_out_autograd_meta,saved_mean_autograd_meta,saved_variance_autograd_meta,reserve_space_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<SyncBatchNormGradNode>(new SyncBatchNormGradNode(6, 5));
    // SetAttributes if needed
    grad_node->SetAttributemomentum(momentum);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributeuse_global_stats(use_global_stats);
    grad_node->SetAttributetrainable_statistics(trainable_statistics);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperscale(scale);
    grad_node->SetTensorWrapperbias(bias);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(scale, 3);
    grad_node->SetGradOutMeta(bias, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(mean_out_autograd_meta, 1);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(variance_out_autograd_meta, 2);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_mean_autograd_meta, 3);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(saved_variance_autograd_meta, 4);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(reserve_space_autograd_meta, 5);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (mean_out_autograd_meta) {
      egr::EagerUtils::SetHistory(mean_out_autograd_meta, grad_node);
    }
    if (variance_out_autograd_meta) {
      egr::EagerUtils::SetHistory(variance_out_autograd_meta, grad_node);
    }
    if (saved_mean_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_mean_autograd_meta, grad_node);
    }
    if (saved_variance_autograd_meta) {
      egr::EagerUtils::SetHistory(saved_variance_autograd_meta, grad_node);
    }
    if (reserve_space_autograd_meta) {
      egr::EagerUtils::SetHistory(reserve_space_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(mean_out, 1);
    grad_node->SetGradInMeta(variance_out, 2);
    grad_node->SetGradInMeta(saved_mean, 3);
    grad_node->SetGradInMeta(saved_variance, 4);
    grad_node->SetGradInMeta(reserve_space, 5);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(mean_out);
    egr::EagerUtils::CheckAndRetainGrad(variance_out);
    egr::EagerUtils::CheckAndRetainGrad(saved_mean);
    egr::EagerUtils::CheckAndRetainGrad(saved_variance);
    egr::EagerUtils::CheckAndRetainGrad(reserve_space);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrapperreserve_space(reserve_space);
  }


  VLOG(4) << "Finish AD API: sync_batch_norm_";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string output_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    output_str += output_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string output_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    output_str += output_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string output_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    output_str += output_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string output_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    output_str += output_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string output_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    output_str += output_reserve_space_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return std::tuple<paddle::experimental::Tensor, paddle::experimental::Tensor&, paddle::experimental::Tensor&, paddle::experimental::Tensor, paddle::experimental::Tensor, paddle::experimental::Tensor>{out, mean_out, variance_out, saved_mean, saved_variance, reserve_space};
}


paddle::experimental::Tensor tan_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tan";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tan dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tan");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tan_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tan");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tan_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tan";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::tan(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tan", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tan node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanGradNode>(new TanGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: tan";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor tanh_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "tanh";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("tanh dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("tanh");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return tanh_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("tanh");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = tanh_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "tanh";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::tanh(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("tanh node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhGradNode>(new TanhGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: tanh";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor to_dense_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "to_dense";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("to_dense dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("to_dense");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return to_dense_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("to_dense");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = to_dense_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "to_dense";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::to_dense(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("to_dense", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("to_dense node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ToDenseGradNode>(new ToDenseGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: to_dense";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor to_sparse_coo_ad_func(const paddle::experimental::Tensor& x, int64_t sparse_dim) {
  VLOG(3) << "Running AD API: " << "to_sparse_coo";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("to_sparse_coo dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("to_sparse_coo");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return to_sparse_coo_ad_func(new_x, sparse_dim);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("to_sparse_coo");
    auto transformer = egr::EagerLayoutAutotune<int64_t>(op_name, tensors_vector, &sparse_dim);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = to_sparse_coo_ad_func(new_x, sparse_dim);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "to_sparse_coo";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::to_sparse_coo(x, sparse_dim);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("to_sparse_coo", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("to_sparse_coo node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ToSparseCooGradNode>(new ToSparseCooGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: to_sparse_coo";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor to_sparse_csr_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "to_sparse_csr";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("to_sparse_csr dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("to_sparse_csr");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return to_sparse_csr_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("to_sparse_csr");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = to_sparse_csr_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "to_sparse_csr";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::sparse::to_sparse_csr(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: to_sparse_csr";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor transpose_ad_func(const paddle::experimental::Tensor& x, std::vector<int> perm) {
  VLOG(3) << "Running AD API: " << "transpose";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("transpose dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("transpose");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return transpose_ad_func(new_x, perm);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("transpose");
    auto transformer = egr::EagerLayoutAutotune<std::vector<int>>(op_name, tensors_vector, &perm);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = transpose_ad_func(new_x, perm);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "transpose";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::transpose(x, perm);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("transpose", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("transpose node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<TransposeGradNode>(new TransposeGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeperm(perm);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: transpose";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor values_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "values";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("values dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("values");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return values_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("values");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = values_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "values";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::values(x);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("values", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("values node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<ValuesGradNode>(new ValuesGradNode(1, 1));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: values";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor addmm_ad_func(const paddle::experimental::Tensor& input, const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, float beta, float alpha) {
  VLOG(3) << "Running AD API: " << "addmm";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("addmm dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("addmm");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {input},{x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_input = egr::EagerAmpAutoCast("input", input, amp_dst_dtype, op_name);
    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return addmm_ad_func(new_input, new_x, new_y, beta, alpha);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {input},{x},{y} };
    
    auto op_name = phi::TransToFluidOpName("addmm");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_input = transformer->TransInTensor("input", input);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = addmm_ad_func(new_input, new_x, new_y, beta, alpha);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* input_autograd_meta = egr::EagerUtils::nullable_autograd_meta(input);
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "addmm";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::addmm(input, x, y, beta, alpha);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("addmm", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,input_autograd_meta,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("addmm node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<AddmmGradNode>(new AddmmGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    grad_node->SetAttributebeta(beta);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(x, 1);
    grad_node->SetGradOutMeta(y, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: addmm";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor coalesce_ad_func(const paddle::experimental::Tensor& x) {
  VLOG(3) << "Running AD API: " << "coalesce";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("coalesce dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("coalesce");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return coalesce_ad_func(new_x);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("coalesce");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = coalesce_ad_func(new_x);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "coalesce";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::sparse::coalesce(x);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: coalesce";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor full_like_ad_func(const paddle::experimental::Tensor& x, paddle::experimental::Scalar value, paddle::experimental::DataType dtype) {
  VLOG(3) << "Running AD API: " << "full_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("full_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("full_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return full_like_ad_func(new_x, value, dtype);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("full_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = full_like_ad_func(new_x, value, dtype);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "full_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::sparse::full_like(x, value, dtype);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: full_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor fused_attention_ad_func(const paddle::experimental::Tensor& query, const paddle::experimental::Tensor& key, const paddle::experimental::Tensor& value, const paddle::experimental::Tensor& sparse_mask, const paddle::optional<paddle::experimental::Tensor>& key_padding_mask, const paddle::optional<paddle::experimental::Tensor>& attn_mask) {
  VLOG(3) << "Running AD API: " << "fused_attention";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("fused_attention dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("fused_attention");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {query},{key},{value},{sparse_mask} };
    if (key_padding_mask) amp_tensors_vector.push_back({ *key_padding_mask });
    if (attn_mask) amp_tensors_vector.push_back({ *attn_mask });

    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_query = egr::EagerAmpAutoCast("query", query, amp_dst_dtype, op_name);
    auto new_key = egr::EagerAmpAutoCast("key", key, amp_dst_dtype, op_name);
    auto new_value = egr::EagerAmpAutoCast("value", value, amp_dst_dtype, op_name);
    auto new_sparse_mask = egr::EagerAmpAutoCast("sparse_mask", sparse_mask, amp_dst_dtype, op_name);
    auto new_key_padding_mask = egr::EagerAmpAutoCast("key_padding_mask", key_padding_mask, amp_dst_dtype, op_name);
    auto new_attn_mask = egr::EagerAmpAutoCast("attn_mask", attn_mask, amp_dst_dtype, op_name);

    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return fused_attention_ad_func(new_query, new_key, new_value, new_sparse_mask, new_key_padding_mask, new_attn_mask);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {query},{key},{value},{sparse_mask} };
    if (key_padding_mask) tensors_vector.push_back({ *key_padding_mask });
    if (attn_mask) tensors_vector.push_back({ *attn_mask });

    auto op_name = phi::TransToFluidOpName("fused_attention");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_query = transformer->TransInTensor("query", query);
    auto new_key = transformer->TransInTensor("key", key);
    auto new_value = transformer->TransInTensor("value", value);
    auto new_sparse_mask = transformer->TransInTensor("sparse_mask", sparse_mask);
auto new_key_padding_mask = transformer->TransInTensor("key_padding_mask", key_padding_mask);
    auto new_attn_mask = transformer->TransInTensor("attn_mask", attn_mask);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = fused_attention_ad_func(new_query, new_key, new_value, new_sparse_mask, new_key_padding_mask, new_attn_mask);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* query_autograd_meta = egr::EagerUtils::nullable_autograd_meta(query);
  egr::AutogradMeta* key_autograd_meta = egr::EagerUtils::nullable_autograd_meta(key);
  egr::AutogradMeta* value_autograd_meta = egr::EagerUtils::nullable_autograd_meta(value);

  VLOG(5) << "Running C++ API: " << "fused_attention";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_QUERY_TEMPLATE = " \n( query , [%s]), ";
    std::string input_query_str = paddle::string::Sprintf(TENSOR_QUERY_TEMPLATE, egr::EagerUtils::TensorStr(query));
    input_str += input_query_str; 
    const char* TENSOR_KEY_TEMPLATE = " \n( key , [%s]), ";
    std::string input_key_str = paddle::string::Sprintf(TENSOR_KEY_TEMPLATE, egr::EagerUtils::TensorStr(key));
    input_str += input_key_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_SPARSE_MASK_TEMPLATE = " \n( sparse_mask , [%s]), ";
    std::string input_sparse_mask_str = paddle::string::Sprintf(TENSOR_SPARSE_MASK_TEMPLATE, egr::EagerUtils::TensorStr(sparse_mask));
    input_str += input_sparse_mask_str; 
    const char* TENSOR_KEY_PADDING_MASK_TEMPLATE = " \n( key_padding_mask , [%s]), ";
    std::string input_key_padding_mask_str = paddle::string::Sprintf(TENSOR_KEY_PADDING_MASK_TEMPLATE, egr::EagerUtils::TensorStr(key_padding_mask));
    input_str += input_key_padding_mask_str; 
    const char* TENSOR_ATTN_MASK_TEMPLATE = " \n( attn_mask , [%s]), ";
    std::string input_attn_mask_str = paddle::string::Sprintf(TENSOR_ATTN_MASK_TEMPLATE, egr::EagerUtils::TensorStr(attn_mask));
    input_str += input_attn_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::fused_attention_intermediate(query, key, value, sparse_mask, key_padding_mask, attn_mask);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fused_attention_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& softmax = std::get<1>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* softmax_autograd_meta = egr::EagerUtils::autograd_meta(&softmax);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,query_autograd_meta,key_autograd_meta,value_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("fused_attention node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,softmax_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<FusedAttentionGradNode>(new FusedAttentionGradNode(2, 6));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperquery(query);
    grad_node->SetTensorWrapperkey(key);
    grad_node->SetTensorWrappervalue(value);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(query, 0);
    grad_node->SetGradOutMeta(key, 1);
    grad_node->SetGradOutMeta(value, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(softmax_autograd_meta, 1);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (softmax_autograd_meta) {
      egr::EagerUtils::SetHistory(softmax_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(softmax, 1);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(softmax);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappersoftmax(softmax);
  }


  VLOG(4) << "Finish AD API: fused_attention";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_QUERY_TEMPLATE = " \n( query , [%s]), ";
    std::string input_query_str = paddle::string::Sprintf(TENSOR_QUERY_TEMPLATE, egr::EagerUtils::TensorStr(query));
    input_str += input_query_str; 
    const char* TENSOR_KEY_TEMPLATE = " \n( key , [%s]), ";
    std::string input_key_str = paddle::string::Sprintf(TENSOR_KEY_TEMPLATE, egr::EagerUtils::TensorStr(key));
    input_str += input_key_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_SPARSE_MASK_TEMPLATE = " \n( sparse_mask , [%s]), ";
    std::string input_sparse_mask_str = paddle::string::Sprintf(TENSOR_SPARSE_MASK_TEMPLATE, egr::EagerUtils::TensorStr(sparse_mask));
    input_str += input_sparse_mask_str; 
    const char* TENSOR_KEY_PADDING_MASK_TEMPLATE = " \n( key_padding_mask , [%s]), ";
    std::string input_key_padding_mask_str = paddle::string::Sprintf(TENSOR_KEY_PADDING_MASK_TEMPLATE, egr::EagerUtils::TensorStr(key_padding_mask));
    input_str += input_key_padding_mask_str; 
    const char* TENSOR_ATTN_MASK_TEMPLATE = " \n( attn_mask , [%s]), ";
    std::string input_attn_mask_str = paddle::string::Sprintf(TENSOR_ATTN_MASK_TEMPLATE, egr::EagerUtils::TensorStr(attn_mask));
    input_str += input_attn_mask_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string output_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    output_str += output_softmax_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor masked_matmul_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y, const paddle::experimental::Tensor& mask) {
  VLOG(3) << "Running AD API: " << "masked_matmul";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("masked_matmul dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("masked_matmul");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y},{mask} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    auto new_mask = egr::EagerAmpAutoCast("mask", mask, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return masked_matmul_ad_func(new_x, new_y, new_mask);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y},{mask} };
    
    auto op_name = phi::TransToFluidOpName("masked_matmul");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);
    auto new_mask = transformer->TransInTensor("mask", mask);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = masked_matmul_ad_func(new_x, new_y, new_mask);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "masked_matmul";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::masked_matmul(x, y, mask);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("masked_matmul", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("masked_matmul node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaskedMatmulGradNode>(new MaskedMatmulGradNode(1, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: masked_matmul";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor matmul_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& y) {
  VLOG(3) << "Running AD API: " << "matmul";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("matmul dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("matmul");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{y} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_y = egr::EagerAmpAutoCast("y", y, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return matmul_ad_func(new_x, new_y);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{y} };
    
    auto op_name = phi::TransToFluidOpName("matmul");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_y = transformer->TransInTensor("y", y);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = matmul_ad_func(new_x, new_y);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* y_autograd_meta = egr::EagerUtils::nullable_autograd_meta(y);

  VLOG(5) << "Running C++ API: " << "matmul";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::matmul(x, y);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,y_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("matmul node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MatmulGradNode>(new MatmulGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: matmul";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor maxpool_ad_func(const paddle::experimental::Tensor& x, std::vector<int> kernel_sizes, std::vector<int> paddings, std::vector<int> dilations, std::vector<int> strides) {
  VLOG(3) << "Running AD API: " << "maxpool";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("maxpool dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("maxpool");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return maxpool_ad_func(new_x, kernel_sizes, paddings, dilations, strides);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("maxpool");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = maxpool_ad_func(new_x, kernel_sizes, paddings, dilations, strides);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);

  VLOG(5) << "Running C++ API: " << "maxpool";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::maxpool_intermediate(x, kernel_sizes, paddings, dilations, strides);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maxpool_intermediate", api_result); }

  // Get Outputs
  auto& out = std::get<0>(api_result);
  auto& rulebook = std::get<1>(api_result);
  auto& counter = std::get<2>(api_result);

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  egr::AutogradMeta* rulebook_autograd_meta = egr::EagerUtils::autograd_meta(&rulebook);
  egr::AutogradMeta* counter_autograd_meta = egr::EagerUtils::autograd_meta(&counter);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("maxpool node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta,rulebook_autograd_meta,counter_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MaxpoolGradNode>(new MaxpoolGradNode(3, 1));
    // SetAttributes if needed
    grad_node->SetAttributekernel_sizes(kernel_sizes);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (rulebook_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(rulebook_autograd_meta, 1);
    }
    if (counter_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(counter_autograd_meta, 2);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    if (rulebook_autograd_meta) {
      egr::EagerUtils::SetHistory(rulebook_autograd_meta, grad_node);
    }
    if (counter_autograd_meta) {
      egr::EagerUtils::SetHistory(counter_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    grad_node->SetGradInMeta(rulebook, 1);
    grad_node->SetGradInMeta(counter, 2);
    egr::EagerUtils::CheckAndRetainGrad(out);
    egr::EagerUtils::CheckAndRetainGrad(rulebook);
    egr::EagerUtils::CheckAndRetainGrad(counter);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrapperrulebook(rulebook);
    grad_node->SetTensorWrappercounter(counter);
    grad_node->SetTensorWrapperout(out);
  }


  VLOG(4) << "Finish AD API: maxpool";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string output_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    output_str += output_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string output_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    output_str += output_counter_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor mv_ad_func(const paddle::experimental::Tensor& x, const paddle::experimental::Tensor& vec) {
  VLOG(3) << "Running AD API: " << "mv";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("mv dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("mv");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x},{vec} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    auto new_vec = egr::EagerAmpAutoCast("vec", vec, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return mv_ad_func(new_x, new_vec);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x},{vec} };
    
    auto op_name = phi::TransToFluidOpName("mv");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);
    auto new_vec = transformer->TransInTensor("vec", vec);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = mv_ad_func(new_x, new_vec);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  // Get Input AutoGradMeta
  egr::AutogradMeta* x_autograd_meta = egr::EagerUtils::nullable_autograd_meta(x);
  egr::AutogradMeta* vec_autograd_meta = egr::EagerUtils::nullable_autograd_meta(vec);

  VLOG(5) << "Running C++ API: " << "mv";
 // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

 // Forward API Call
  auto api_result = paddle::experimental::sparse::mv(x, vec);
  // Check NaN and Inf if needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mv", api_result); }

  // Get Outputs
  auto& out = api_result;

  // Get Output AutoGradMeta
  egr::AutogradMeta* out_autograd_meta = egr::EagerUtils::autograd_meta(&out);
  bool trace_backward = egr::Controller::Instance().HasGrad();
  bool require_any_grad = egr::EagerUtils::ComputeRequireGrad(trace_backward,x_autograd_meta,vec_autograd_meta);

  // Check Inplace if needed

  // Node Creation
  if(require_any_grad) {
    paddle::platform::RecordEvent node_creation_record_event("mv node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    egr::EagerUtils::PassStopGradient(false,out_autograd_meta);

    // Node Construction
    auto grad_node = std::shared_ptr<MvGradNode>(new MvGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappervec(vec);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(vec, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(out_autograd_meta, 0);
    }
    if (out_autograd_meta) {
      egr::EagerUtils::SetHistory(out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(out, 0);
    egr::EagerUtils::CheckAndRetainGrad(out);
    // Set TensorWrappers for Forward Outputs if needed

  }


  VLOG(4) << "Finish AD API: mv";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


}


namespace strings {
    
paddle::experimental::Tensor empty_ad_func(paddle::experimental::IntArray shape, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "empty";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("empty dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic

 VLOG(7) << " No AMP for empty_ad_func because it has no input. "; 
  // Layout autotune

  VLOG(5) << "Running C++ API: " << "empty";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::strings::empty(shape, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: empty";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor empty_like_ad_func(const paddle::experimental::Tensor& x, paddle::Place place) {
  VLOG(3) << "Running AD API: " << "empty_like";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("empty_like dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("empty_like");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return empty_like_ad_func(new_x, place);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("empty_like");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = empty_like_ad_func(new_x, place);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "empty_like";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::strings::empty_like(x, place);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: empty_like";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor lower_ad_func(const paddle::experimental::Tensor& x, bool use_utf8_encoding) {
  VLOG(3) << "Running AD API: " << "lower";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("lower dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("lower");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return lower_ad_func(new_x, use_utf8_encoding);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("lower");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = lower_ad_func(new_x, use_utf8_encoding);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "lower";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::strings::lower(x, use_utf8_encoding);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: lower";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


paddle::experimental::Tensor upper_ad_func(const paddle::experimental::Tensor& x, bool use_utf8_encoding) {
  VLOG(3) << "Running AD API: " << "upper";
  // Dygraph Record Event
  paddle::platform::RecordEvent dygraph_entrance_record_event("upper dygraph", paddle::platform::TracerEventType::Operator, 1);

  // AMP Logic
  if (egr::Controller::Instance().GetAMPLevel() != paddle::imperative::AmpLevel::O0) {
    VLOG(5) << "Check and Prepare For AMP";
    auto op_name = phi::TransToFluidOpName("upper");
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> amp_tensors_vector = { {x} };
    
    auto amp_dst_dtype = egr::GetAmpDestDtype(op_name, amp_tensors_vector);

    auto new_x = egr::EagerAmpAutoCast("x", x, amp_dst_dtype, op_name);
    
    {
      paddle::imperative::AutoCastGuard guard(egr::Controller::Instance().GetCurrentTracer(), paddle::imperative::AmpLevel::O0);
      return upper_ad_func(new_x, use_utf8_encoding);
    }
  }

  // Layout autotune

  if (egr::Controller::Instance().UseLayoutAutoTune()) {
    paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> tensors_vector = { {x} };
    
    auto op_name = phi::TransToFluidOpName("upper");
    auto transformer = egr::EagerLayoutAutotune(op_name, tensors_vector);
    auto new_x = transformer->TransInTensor("x", x);

    VLOG(5) << "Check and Prepare For LAYOUT "<< op_name;
    paddle::imperative::LayoutAutotuneGuard guard(egr::Controller::Instance().GetCurrentTracer(), false);
    paddle::experimental::Tensor out = upper_ad_func(new_x, use_utf8_encoding);

    transformer -> SetOutTensorLayout(&out);

    // Returns
    return out;
  }

  VLOG(5) << "Running C++ API: " << "upper";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Forward API Call
  auto api_result = paddle::experimental::strings::upper(x, use_utf8_encoding);
  // Get Outputs
  auto& out = api_result;

  VLOG(4) << "Finish AD API: upper";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string output_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    output_str += output_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Returns
  return out;
}


}


